@STRING{aaai    = {Proc.~of the Conference on Advancements of Artificial Intelligence (AAAI)} }
@STRING{aaaiold = {Proc.~of the National Conference on Artificial Intelligence (AAAI)} }
@STRING{ac      = {IEEE Trans. on Automatic Control} }
@STRING{acc     = {Proc.~of the IEEE American Control Conference (ACC)} }
@STRING{accv    = {Proc.~of the Asian Conf.~on Computer Vision (ACCV)} }
@STRING{acra    = {Proc.~of the Australasian Conf.~on Robotics and Automation (ACRA)} }
@STRING{acmgraphics = {ACM Transactions on Graphics} }
@STRING{addison = {Addison-Wesley Publishing Inc.} }
@STRING{advancedrobotics={Advanced Robotics} }
@STRING{ai      = {Artificial Intelligence} }
@STRING{ams     = {Proc.~of Autonome Mobile Systeme} }
@STRING{ar      = {Autonomous Robots} }
@STRING{arxiv   = {arXiv preprint} }
@STRING{bmvc    = {Proc.~of British Machine Vision Conference (BMVC)} }
@STRING{cacm    = {Communications of the ACM} }
@STRING{ccvw    = {Proc.~of the Croation Computer Vision Workshop (CCVW)} }
@STRING{cira    = {Proc.~of the IEEE Intl.~Symp. on Computer Intelligence in Robotics and Automation (CIRA)} }
@STRING{cogsys  = {Proc.~of the Intl.~Conf.~on Cognitive Systems (CogSys)} }
@STRING{cviu    = {Journal of Computer Vision and Image Understanding (CVIU)} }
@STRING{cvpr    = {Proc.~of the IEEE Conf.~on Computer Vision and Pattern Recognition (CVPR)} }
@STRING{cvvt    = {Proc.~of the Intl.~Workshop on Computer Vision in Vehicle Technology (CVVT)} }
@STRING{dagm    = {Proc.~of the Symposion of the German Association for Pattern Recognition (DAGM)} }
@STRING{dagstuhl= {Proc.~of the Dagstuhl Seminar} }
@STRING{dgpf    = {Proc.~of the Conf.~of the German Society for Photogrammetry, Remote Sensing and Geoinformation (DGPF)} }
@STRING{eccv    = {Proc.~of the Europ.~Conf.~on Computer Vision (ECCV)} }
@STRING{ecmr    = {Proc.~of the Europ.~Conf.~on Mobile Robotics (ECMR)} }
@STRING{emav    = {Proc.~of the European Micro Aerial Vehicle Conference} }
@STRING{euros   = {Proc.~of the Europ.~Robotics Symp. (EUROS)} }
@STRING{fntr    = {Foundations and Trends in Robotics} }
@STRING{gcpr    = {Proc.~of the German Conf.~on Pattern Recognition (GCPR)} }
@STRING{humanoids={Proc.~of the IEEE Intl.~Conf.~on Humanoid Robots} }
@STRING{icar    = {Proc.~of the Int.~Conf.~on Advanced Robotics (ICAR)} }
@STRING{iccv    = {Proc.~of the IEEE Intl.~Conf.~on Computer Vision (ICCV)} }
@STRING{iciap   = {Proc.~of the Intl.~Conf.~on Image Analysis and Processing (ICIAP)} }
@STRING{icip    = {Proc.~of the IEEE Intl.~Conf.~on Image Processing (ICIP)} }
@STRING{icra    = {Proc.~of the IEEE Intl.~Conf.~on Robotics \& Automation (ICRA)} }
@STRING{icuas   = {Proc.~of the Intl.~Conf.~on Unmanned Aircraft Systems (ICUAS)} }
@STRING{ieeepress={IEEE Computer Society Press} }
@STRING{ijcai   = {Proc.~of the Intl.~Conf.~on Artificial Intelligence (IJCAI)} }
@STRING{ijcv    = {Intl.~Journal~of Computer Vision (IJCV)} }
@STRING{ijgi    = {Intl.~Journal of Geo-Information} }
@STRING{ijhr    = {The Int.~Journal of Humanoid Robotics (IJHR)} }
@STRING{ijrr    = {Intl.~Journal~of Robotics Research (IJRR)} }
@STRING{imvip   = {Proc.~of the Irish Machine Vision and Image Processing Conference (IMVIP)} }
@STRING{iros    = {Proc.~of the IEEE/RSJ Intl.~Conf.~on Intelligent Robots and Systems (IROS)} }
@STRING{iser    = {Proc.~of the Intl.~Sym.~on Experimental Robotics (ISER)} }
@STRING{ismar   = {Proc.~of the Intl.~Symposium~on Mixed and Augmented Reality (ISMAR)} }
@STRING{isprsannals={ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences} }
@STRING{isprsarchives={ISPRS Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences} }
@STRING{isrr    = {Proc.~of the Intl.~Symposium~on Robotic Research (ISRR)} }
@STRING{iv      = {Proc.~of the IEEE Intelligent Vehicles Symposium (IV)} }
@STRING{ivc     = {Journal on Image and Vision Computing (IVC)} }
@STRING{jair    = {Journal of Artificial Intelligence Research (JAIR)} }
@STRING{jbe     = {ASME Journal of Basic Engineering} }
@STRING{jfr     = {Journal of Field Robotics (JFR)} }
@STRING{jirs    = {Journal of Intelligent and Robotic Systems (JIRS)} }
@STRING{jmiv    = {Journal of Mathematical Imaging and Vision} }
@STRING{joe     = {IEEE Journal of Oceanic Engineering} }
@STRING{jprs    = {ISPRS Journal of Photogrammetry and Remote Sensing (JPRS)} }
@STRING{jra     = {IEEE Journal of Robotics and Automation} }
@STRING{jras    = {Journal on Robotics and Autonomous Systems (RAS)} }
@STRING{mcg     = {Proc.~of the Intl.~Conf.~on Machine Control and Guidance (MCG)} }
@STRING{mirage  = {Proc.~of the Intl.~Conf.~on Computer Vision/Computer Graphics Collaboration Techniques and Applications (MIRAGE)} }
@STRING{mitpress= {MIT Press} }
@STRING{ml      = {Machine Learning} }
@STRING{mobicom = {Proc.~of the {ACM} Intl.~Conf.~on Mobile Computing and Networking (MobiCom)} }
@STRING{mva     = {Proc.~of the IAPR Conf.~on Machine Vision Applications (MVA)} }
@STRING{nips    = {Proc.~of the Conf.~on Neural Information Processing Systems (NIPS)} }
@STRING{nipsjournal={Advances in Neural Information Processing Systems} }
@STRING{oceans  = {Proc.~of OCEANS MTS/IEEE Conference and Exhibition} }
@STRING{pami    = {IEEE Trans.~on Pattern Analysis and Machine Intelligence (TPAMI)} }
@STRING{pcv     = {Proc.~of the ISPRS Conference on Photogrammeric Computer Vision (PCV)} }
@STRING{pers    = {Photogrammetric Engineering and Remote Sensing (PE\&RS)} }
@STRING{pfg     = {Photogrammetrie -- Fernerkundung -- Geoinformation (PFG)} }
@STRING{phowo   = {Proc.~of the Photogrammetric Week (PhoWo)} }
@STRING{pia     = {Proc.~of the ISPRS Conference on Photogrammeric Image Analysis (PIA)} }
@STRING{pr      = {Pattern Recognition} }
@STRING{prl     = {Pattern Recognition Letters} }
@STRING{ral     = {IEEE Robotics and Automation Letters (RA-L)} }
@STRING{ram     = {IEEE Robotics and Automation Magazine (RAM)} }
@STRING{ras     = {Journal on Robotics and Autonomous Systems (RAS)} }
@STRING{rasmag  = {IEEE Robotics and Automation Magazine} }
@STRING{rs      = {Remote Sensing} }
@STRING{rss     = {Proc.~of Robotics: Science and Systems (RSS)} }
@STRING{rssbook = {Robotics: Science and Systems} }
@STRING{sensors = {IEEE Sensors Journal} }
@STRING{sice    = {Proc.~of the Annual Conference of the Society of Instrument and Control Engineers (SICE)} }
@STRING{smc     = {Proc.~of the IEEE Intl.~Conf.~on Systems, Man, and Cybernetics (SMC)} }
@STRING{snowbird= {Proc.~of the Learning Workshop (Snowbird)} }
@STRING{soave   = {Proc.~of the Workshop on Self-Organization of AdaptiVE behavior (SOAVE)} }
@STRING{spiesdvrs={Proc.~of SPIE Stereoscopic Displays and Virtual Reality Systems} }
@STRING{spiev   = {Proc.~of SPIE Videometrics} }
@STRING{springer= {Springer Verlag} }
@STRING{springerstaradvanced={STAR Springer Tracts in Advanced Robotics} }
@STRING{tarj    = {The Australian Rangeland Journal} }
@STRING{tits    = {IEEE Trans.~on Intelligent Transportation Systems (ITS)} }
@STRING{titsmag = {IEEE Trans.~on Intelligent Transportation Systems Magazine} }
@STRING{tpami   = {IEEE Trans.~on Pattern Analalysis and Machine Intelligence (TPAMI)} }
@STRING{tra     = {IEEE Trans.~on Robotics and Automation} }
@STRING{tro     = {IEEE Trans.~on Robotics (TRO)} }
@STRING{uai     = {Proc.~of the Conf.~on Uncertainty in Artificial Intelligence (UAI)} }
@STRING{uavg    = {Proc.~of the Intl.~Conf.~on Unmanned Aerial Vehicles in Geomatics} }
@STRING{uust    = {Proc.~of the Intl.~Symp.~on Unmanned Untethered Submersible Technology} }
@STRING{vc      = {The Visual Computer (VC)} }
@STRING{wafr    = {Intl.~Workshop on the Algorithmic Foundations of Robotics (WAFR)} }
@STRING{threedv = {Proc. of the International Conference on 3D Vision~(3DV)}}

@InProceedings{zhang2017icra,
  author        = {J. Zhang and S. Singh},
  title         = {{Enabling Aggressive Motion Estimation at Low-Drift and Accurate Mapping in Real-Time}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Range Sensing, Visual-Based Navigation, Mapping},
  abstract      = {We present a data processing pipeline to online estimate ego-motion and build a map of the traversed environment, leveraging data from a 3D laser, a camera, and an IMU. Different from traditional methods that use a Kalman filter or factor-graph optimization, the proposed method employs a sequential, multi-layer processing pipeline, solving for motion from coarse to fine. The resulting system enables highfrequency, low-latency ego-motion estimation, along with dense, accurate 3D map registration. Further, the system is capable of handling sensor degradation by automatic reconfiguration bypassing failure modules. Therefore, it can operate in the presence of highly dynamic motion as well as in dark, textureless, and structure-less environments. During experiments, the system demonstrates 0.22\% of relative position drift over 9.3km of navigation and robustness w.r.t aggressive motion such as highway speed driving (up to 33m/s).},
  url           = {http://www.frc.ri.cmu.edu/~jizhang03/Publications/ICRA_2017.pdf},
}

@InProceedings{tung2017icra,
  author        = {F. Tung and J.J. Little},
  title         = {{MF3D: Model-Free 3D Semantic Scene Parsing}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Semantic Scene Understanding, Recognition, Object detection, Segmentation, Categorization},
  abstract      = {We present a novel model-free method for online 3D semantic scene parsing from video sequences. MF3D (Model-Free 3D) is different from conventional methods for 3D scene parsing in that voxel labelling is approached via searchbased label transfer instead of discriminative classification. This non-parametric approach makes MF3D easy to scale with an online growth in the database, as no model re-training is required with the addition of new examples or categories. Experimental results on the KITTI benchmark demonstrate that our model-free approach enables accurate online 3D scene parsing while retaining extensibility to new categories. In addition, we show that unsupervised binary encoding (hashing) techniques can be easily incorporated into our framework for scalability to larger databases. Building Vegetation Car Wall/fence Pavement},
  url           = {http://www.sfu.ca/~ftung/papers/mf3d_icra17.pdf},
}

@InProceedings{weerasekera2017icra,
  author        = {C.S. Weerasekera and Y. Latif and R. Garg and I. Reid},
  title         = {{Dense Monocular Reconstruction Using Surface Normals}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Mapping, Semantic Scene Understanding, Visual-Based Navigation, CNN},
  abstract      = {This paper presents an efficient framework for dense 3D scene reconstruction using input from a moving monocular camera. Visual SLAM (Simultaneous Localisation and Mapping) approaches based solely on geometric methods have proven to be quite capable of accurately tracking the pose of a moving camera and simultaneously building a map of the environment in real-time. However, most of them suffer from the 3D map being too sparse for practical use. The missing points in the generated map correspond mainly to areas lacking texture in the input images, and dense mapping systems often rely on hand-crafted priors like piecewise-planarity or piecewise-smooth depth. These priors do not always provide the required level of scene understanding to accurately fill the map. On the other hand, Convolutional Neural Networks (CNNs) have had great success in extracting high-level information from images and regressing pixel-wise surface normals, semantics, and even depth. In this work we leverage this high-level scene context learned by a deep CNN in the form of a surface normal prior. We show, in particular, that using the surface normal prior leads to better reconstructions than the weaker smoothness prior.},
}

@InProceedings{barnes2017icra,
  author        = {D. Barnes and W. Maddern and I. Posner},
  title         = {{Find Your Own Way: Weakly-Supervised Segmentation of Path Proposals for Urban Autonomy}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Autonomous Vehicle Navigation, Computer Vision for Transportation, Motion and Path Planning},
  abstract      = {We present a weakly-supervised approach to segmenting proposed drivable paths in images with the goal of autonomous driving in complex urban environments. Using recorded routes from a data collection vehicle, our proposed method generates vast quantities of labelled images containing proposed paths and obstacles without requiring manual annotation, which we then use to train a deep semantic segmentation network. With the trained network we can segment proposed paths and obstacles at run-time using a vehicle equipped with only a monocular camera without relying on explicit modelling of road or lane markings. We evaluate our method on the largescale KITTI and Oxford RobotCar datasets and demonstrate reliable path proposal and obstacle segmentation in a wide variety of environments under a range of lighting, weather and traffic conditions. We illustrate how the method can generalise to multiple path proposals at intersections and outline plans to incorporate the system into a framework for autonomous urban driving.},
  url           = {https://arxiv.org/pdf/1610.01238.pdf},
}

@InProceedings{jellal2017icra,
  author        = {R.A. Jellal and M. Lange and B. Wassermann and A. Schilling and A. Zell},
  title         = {{LS-ELAS: Line Segment Based Efficient Large Scale Stereo Matching}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Computer Vision for Other Robotic Applications, Mapping, RGB-D Perception},
  abstract      = {We present LS-ELAS, a line segment extension to the ELAS algorithm, which increases the performance and robustness. LS-ELAS is a binocular dense stereo matching algorithm, which computes the disparities in constant time for most of the pixels in the image and in linear time for a small subset of the pixels (support points). Our approach is based on line segments to determine the support points instead of uniformly selecting them over the image range. This way we find very informative support points which preserve the depth discontinuity. The prior of our Bayesian stereo matching method is based on a set of line segments and a set of support points. Both sets are given to a constrained Delaunay triangulation to generate a triangulation mesh which is aware of possible depth discontinuities. We further increased the accuracy by using an adaptive method to sample candidate points along edge segments. We evaluated our algorithm on the Middlebury benchmark.},
  url           = {http://www.cogsys.cs.uni-tuebingen.de/publikationen/2017/JellalICRA17.pdf},
}

@InProceedings{dzitsiuk2017icra,
  author        = {M. Dzitsiuk and J. Sturm and R. Maier and L. Ma and D. Cremers},
  title         = {{De-Noising, Stabilizing and Completing 3D Reconstructions On-The-Go Using Plane Priors}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {RGB-D Perception, Object detection, Segmentation, Categorization, Semantic Scene Understanding},
  abstract      = {Creating 3D maps on robots and other mobile devices has become a reality in recent years. Online 3D reconstruction enables many exciting applications in robotics and AR/VR gaming. However, the reconstructions are noisy and generally incomplete. Moreover, during online reconstruction, the surface changes with every newly integrated depth image which poses a significant challenge for physics engines and path planning algorithms. This paper presents a novel, fast and robust method for obtaining and using information about planar surfaces, such as walls, floors, and ceilings as a stage in 3D reconstruction based on Signed Distance Fields (SDFs). Our algorithm recovers clean and accurate surfaces, reduces the movement of individual mesh vertices caused by noise during online reconstruction and fills in the occluded and unobserved regions. We implemented and evaluated two different strategies to generate plane candidates and two strategies for merging them. Our implementation is optimized to run in real-time on mobile devices such as the Tango tablet. In an extensive set of experiments, we validated that our approach works well in a large number of natural environments despite the presence of significant amount of occlusion, clutter and noise, which occur frequently. We further show that plane fitting enables in many cases a meaningful semantic segmentation of real-world scenes.},
  url           = {https://vision.in.tum.de/_media/spezial/bib/dzitsiuk2017icra.pdf},
}

@InProceedings{rehder2017icra,
  author        = {J. Rehder and J. Nikolic and T. Schneider and R. Siegwart},
  title         = {{A Direct Formulation for Camera Calibration}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Calibration and Identification, Sensor Fusion},
  abstract      = {Conventional camera calibration techniques rely on discrete reference points extracted from a set of input images. While these approaches have been applied successfully for a long time, omitting all image information apart from reference point positions at the initial stage of the calibration pipeline renders correct treatment of uncertainties difficult and gives rise to complications in timestamping measurements in applications where exposure time cannot be neglected. Drawing inspiration from visual state estimation, we employ a direct formulation of the camera measurement model. To this end, we render a view of the target given all calibration parameters, enabling a maximum likelihood estimator formulated on image intensities as measurements. We demonstrate the advantages of avoiding abstraction from image measurements for determining the line delay of a rolling shutter camera and by estimating camera exposure time from motion blur.},
}

@InProceedings{chen2017icra,
  author        = {J. Chen and S. Shen},
  title         = {{Improving Octree-Based Occupancy Maps Using Environment Sparsity with Application to Aerial Robot Navigation}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Mapping, Aerial Robotics, Autonomous Vehicle Navigation},
  abstract      = {In this paper, we present an improved octreebased mapping framework for autonomous navigation of mobile robots. Octree is best known for its memory efficiency for representing large-scale environments. However, existing implementations, including the state-of-the-art OctoMap [1], are computationally too expensive for online applications that require frequent map updates and inquiries. Utilizing the sparse nature of the environment, we propose a ray tracing method with early termination for efficient probabilistic map update. We also propose a divide-and-conquer volume occupancy inquiry method which serves as the core operation for generation of free-space configurations for optimization-based trajectory generation. We experimentally demonstrate that our method maintains the same storage advantage of the original OctoMap, but being computationally more efficient for map update and occupancy inquiry. Finally, by integrating the proposed map structure in a complete navigation pipeline, we show autonomous quadrotor flight through complex environments.},
}

@InProceedings{pfrommer2017icra,
  author        = {B. Pfrommer and N.J. Sanket and K. Daniilidis and J. Cleveland},
  title         = {{PennCOSYVIO: A Challenging Visual Inertial Odometry Benchmark}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {SLAM, Localization, Mapping},
  abstract      = {We present PennCOSYVIO, a new challenging Visual Inertial Odometry (VIO) benchmark with synchronized data from a VI-sensor (stereo camera and IMU), two Project Tango hand-held devices, and three GoPro Hero 4 cameras. Recorded at UPenns Singh center, the 150m long path of the hand-held rig crosses from outdoors to indoors and includes rapid rotations, thereby testing the abilities of VIO and Simultaneous Localization and Mapping (SLAM) algorithms to handle changes in lighting, different textures, repetitive structures, and large glass surfaces. All sensors are synchronized and intrinsically and extrinsically calibrated. We demonstrate the accuracy with which ground-truth poses can be obtained via optic localization off of fiducial markers. The data set can be found at https://daniilidis-group.github.io/penncosyvio/. Fig. 1.},
}

@InProceedings{zhang2017icra-rsmw,
  author        = {S. Zhang and W. Xie and G. Zhang and H. Bao and M. Kaess},
  title         = {{Robust Stereo Matching with Surface Normal Prediction}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Mapping, Range Sensing, Computer Vision for Other Robotic Applications},
  abstract      = {Traditional stereo matching approaches generally have problems in handling textureless regions, strong occlusions and reflective regions that do not satisfy a Lambertian surface assumption. In this paper, we propose to combine the predicted surface normal by deep learning to overcome these inherent difficulties in stereo matching. With the selected reliable disparities from stereo matching method and effective edge fusion strategy, we can faithfully convert the predicted surface normal map to a disparity map by solving a least squares system which maintains discontinuity on object boundaries and continuity on other regions. Then we refine the disparity map iteratively by bilateral filtering-based completion and edge feature refinement. Experimental results on the Middlebury dataset and our own captured stereo sequences demonstrate the effectiveness of the proposed approach.},
  url           = {http://frc.ri.cmu.edu/~kaess/pub/Zhang17icra.pdf},
}

@InProceedings{chan2017icra,
  author        = {S. Chan and X. Zhou and Z. Zhang and S. Chen},
  title         = {{Compressive Tracking with Locality Sensitive Histograms Features}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Visual Tracking, Computer Vision for Automation, Surveillance Systems},
  abstract      = {Currently, Compressive Tracking (CT) method has drawn great attention because of its high efficiency. However, it cannot well deal with some appearance variations due to its limitations of feature expression and it only uses a fixed parameter to update the appearance model. In order to handle such matters, we propose an adaptive CT method that combines the predicted target position with CT based on Locality Sensitive Histograms (LSH) features. Our method significantly improves CT in four aspects. First, the efficient illumination invariant features extracted based on LSH are used to represent an effective appearance model that is robust to illumination changes. Second, the color attributes tracker is adopted to predict the target position for re-building the new weighted discriminant function which brings in the color information to make up for the inadequacy of Haarlike characteristics. Third, a new model update mechanism is proposed to preserve the stable features while avoid the noisy appearance variations during tracking. Fourth, a trajectory rectification method is employed to refine the tracking location when possible inaccurate tracking occurs. Finally, we show that our tracker achieves state-of-the-art performance in a comprehensive evaluation over 47 challenging color sequences.},
}

@InProceedings{hall2017icra,
  author        = {D. Hall and F. Dayoub and J. Kulk and C.S. McCool},
  title         = {{Towards Unsupervised Weed Scouting for Agricultural Robotics}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Object detection, Segmentation, Categorization, Agricultural Automation},
  abstract      = {Weed scouting is an important part of modern integrated weed management but can be time consuming and sparse when performed manually. Automated weed scouting and weed destruction has typically been performed using classification systems able to classify a set group of species known a priori. This greatly limits deployability as classification systems must be retrained for any field with a different set of weed species present within them. In order to overcome this limitation, this paper works towards developing a clustering approach to weed scouting which can be utilized in any field without the need for prior species knowledge. We demonstrate our system using challenging data collected in the field from an agricultural robotics platform. We show that considerable improvements can be made by (i) learning low-dimensional (bottleneck) features using a deep convolutional neural network to represent plants in general and (ii) tying views of the same area (plant) together. Deploying this algorithm on in-field data collected by AgBotII, we are able to successfully cluster cotton plants from grasses without prior knowledge or training for the specific plants in the field.},
  url           = {https://arxiv.org/pdf/1702.01247.pdf},
}

@InProceedings{peretroukhin2017icra,
  author        = {V. Peretroukhin and L. Clement and J. Kelly},
  title         = {{Reducing Drift in Visual Odometry by Inferring Sun Direction Using a Bayesian Convolutional Neural Network}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Visual-Based Navigation, Visual Learning, Localization, CNN},
  abstract      = {We present a method to incorporate global orientation information from the sun into a visual odometry pipeline using only the existing image stream, where the sun is typically not visible. We leverage recent advances in Bayesian Convolutional Neural Networks to train and implement a sun detection model that infers a three-dimensional sun direction vector from a single RGB image. Crucially, our method also computes a principled uncertainty associated with each prediction, using a Monte Carlo dropout scheme. We incorporate this uncertainty into a sliding window stereo visual odometry pipeline where accurate uncertainty estimates are critical for optimal data fusion. Our Bayesian sun detection model achieves a median error of approximately 12 degrees on the KITTI odometry benchmark training set, and yields improvements of up to 42\% in translational ARMSE and 32\% in rotational ARMSE compared to standard VO. An open source implementation of our Bayesian CNN sun estimator (Sun-BCNN) using Caffe is available at https://github. com/utiasSTARS/sun-bcnn-vo.},
}

@InProceedings{sun2017icra,
  author        = {W. Sun and N. Sood and D. Dey and G. Ranade and S. Prakash and A. Kapoor},
  title         = {{No-Regret Replanning under Uncertainty}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Probability and Statistical Methods, AI Reasoning Methods, Reactive and Sensor-Based Planning},
  abstract      = {This paper explores the problem of path planning under uncertainty. Specifically, we consider online receding horizon based planners that need to operate in a latent environment where the latent information can be modelled via Gaussian Processes. Online path planning in latent environments is challenging since the robot needs to explore the environment to get a more accurate model of latent information for better planning later and also achieves the task as quick as possible. We propose UCB style algorithms that are popular in the bandit settings and show how those analyses can be adapted to the online robotic path planning problems. The proposed algorithm trades-off exploration and exploitation in near-optimal manner and has appealing no-regret properties. We demonstrate the efficacy of the framework on the application of aircraft flight path planning when the winds are partially observed.},
}

@InProceedings{kim2017icra,
  author        = {P. Kim and B. Coltin and O. Alexandrov and H.J. Kim},
  title         = {{Robust Visual Localization in Changing Lighting Conditions}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Space Robotics, Localization, Visual-Based Navigation},
  abstract      = {We present an illumination-robust visual localization algorithm for Astrobee, a free-flying robot designed to autonomously navigate on the International Space Station (ISS). Astrobee localizes with a monocular camera and a prebuilt sparse map composed of natural visual features. Astrobee must perform tasks not only during the day, but also at night when the ISS lights are dimmed. However, the localization performance degrades when the observed lighting conditions differ from the conditions when the sparse map was built. We investigate and quantify the effect of lighting variations on visual feature-based localization systems, and discover that maps built in darker conditions can also be effective in bright conditions, but the reverse is not true. We extend Astrobees localization algorithm to make it more robust to changinglight environments on the ISS by automatically recognizing the current illumination level, and selecting an appropriate map and camera exposure time. We extensively evaluate the proposed algorithm through experiments on Astrobee.},
}

@InProceedings{zermas2017icra,
  author        = {D. Zermas and I. Izzat and N. Papanikolopoulos},
  title         = {{Fast Segmentation of 3D Point Clouds: A Paradigm on Lidar Data for Autonomous Vehicle Applications}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Intelligent Transportation Systems, Computer Vision for Transportation},
  abstract      = {The recent activity in the area of autonomous vehicle navigation has initiated a series of reactions that stirred the automobile industry, pushing for the fast commercialization of this technology which, until recently, seemed futuristic. The LiDAR sensor is able to provide a detailed understanding of the environment surrounding the vehicle making it useful in a plethora of autonomous driving scenarios. Segmenting the 3D point cloud that is provided by modern LiDAR sensors, is the first important step towards the situational assessment pipeline that aims for the safety of the passengers. This step needs to provide accurate segmentation of the ground surface and the obstacles in the vehicles path, and to process each point cloud in real time. The proposed pipeline aims to solve the problem of 3D point cloud segmentation for data received from a LiDAR in a fast and low complexity manner that targets real world applications. The two-step algorithm first extracts the ground surface in an iterative fashion using deterministically assigned seed points, and then clusters the remaining non-ground points taking advantage of the structure of the LiDAR point cloud. Our proposed algorithms outperform similar approaches in running time, while producing similar results and support the validity of this pipeline as a segmentation tool for real world applications.},
}

@InProceedings{engelcke2017icra,
  author        = {M. Engelcke and D. Rao and D.Z. Wang and C.H. Tong and I. Posner},
  title         = {{Vote3Deep: Fast Object Detection in 3D Point Clouds Using Efficient Convolutional Neural Networks}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Object detection, Segmentation, Categorization, Learning and Adaptive Systems, CNN, Range Sensing},
  abstract      = {This paper proposes a computationally efficient approach to detecting objects natively in 3D point clouds using convolutional neural networks (CNNs). In particular, this is achieved by leveraging a feature-centric voting scheme to implement novel convolutional layers which explicitly exploit the sparsity encountered in the input. To this end, we examine the trade-off between accuracy and speed for different architectures and additionally propose to use an L1 penalty on the filter activations to further encourage sparsity in the intermediate representations. To the best of our knowledge, this is the first work to propose sparse convolutional layers and L1 regularisation for efficient large-scale processing of 3D data. We demonstrate the efficacy of our approach on the KITTI object detection benchmark and show that Vote3Deep models with as few as three layers outperform the previous state of the art in both laser and laser-vision based approaches by margins of up to 40\% while remaining highly competitive in terms of processing time.},
}

@InProceedings{ma2017icra,
  author        = {W. Ma and S. Wang and M.A. Brubaker and S. Fidler and R. Urtasun},
  title         = {{Find Your Way by Observing the Sun and Other Semantic Cues}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Localization, Autonomous Vehicle Navigation, Visual-Based Navigation},
  abstract      = {In this paper we present a robust, efficient and affordable approach to self-localization which requires neither GPS nor knowledge about the appearance of the world. Towards this goal, we utilize freely available cartographic maps and derive a probabilistic model that exploits semantic cues in the form of sun direction, presence of an intersection, road type, speed limit and ego-car trajectory to produce very reliable localization results. Our experimental evaluation shows that our approach can localize much faster (in terms of driving time) with less computation and more robustly than competing approaches, which ignore semantic information.},
}

@InProceedings{yang2017icra,
  author        = {S. Yang and S. Scherer},
  title         = {{Direct Monocular Odometry Using Points and Lines}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {SLAM, Mapping, Localization},
  abstract      = {Most visual odometry algorithm for a monocular camera focuses on points, either by feature matching, or direct alignment of pixel intensity, while ignoring a common but important geometry entity: edges. In this paper, we propose an odometry algorithm that combines points and edges to benefit from the advantages of both direct and feature based methods. It works better in texture-less environments and is also more robust to lighting changes and fast motion by increasing the convergence basin. We maintain a depth map for the keyframe then in the tracking part, the camera pose is recovered by minimizing both the photometric error and geometric error to the matched edge in a probabilistic framework. In the mapping part, edge is used to speed up and increase stereo matching accuracy. On various public datasets, our algorithm achieves better or comparable performance than state-of-theart monocular odometry methods. In some challenging textureless environments, our algorithm reduces the state estimation error over 50\%.},
}

@InProceedings{fehr2017icra,
  author        = {M. Fehr and F. Furrer and I. Dryanovski and J. Sturm and I. Gilitschenski and R. Siegwart and C.C. Lerma},
  title         = {{TSDF-Based Change Detection for Consistent Long-Term Dense Reconstruction and Dynamic Object Discovery}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Object detection, Segmentation, Categorization, RGB-D Perception, Mapping},
  abstract      = {Robots that are operating for extended periods of time need to be able to deal with changes in their environment and represent them adequately in their maps. In this paper, we present a novel 3D reconstruction algorithm based on an extended Truncated Signed Distance Function (TSDF) that enables to continuously refine the static map while simultaneously obtaining 3D reconstructions of dynamic objects in the scene. This is a challenging problem because map updates happen incrementally and are often incomplete. Previous work typically performs change detection on point clouds, surfels or maps, which are not able to distinguish between unexplored and empty space. In contrast, our TSDF-based representation naturally contains this information and thus allows us to more robustly solve the scene differencing problem. We demonstrate the algorithms performance as part of a system for unsupervised object discovery and class recognition. We evaluated our algorithm on challenging datasets that we recorded over several days with RGB-D enabled tablets. To stimulate further research in this area, all of our datasets are publicly available3 .},
}

@InProceedings{dong2017icra,
  author        = {J. Dong and J.G. Burnham and B. Boots and G. Rains and F. Dellaert},
  title         = {{4D Crop Monitoring: Spatio-Temporal Reconstruction for Agriculture}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {SLAM, Agricultural Automation, Field Robots},
  abstract      = {Autonomous crop monitoring at high spatial and temporal resolution is a critical problem in precision agriculture. While Structure from Motion and Multi-View Stereo algorithms can finely reconstruct the 3D structure of a field with low-cost image sensors, these algorithms fail to capture the dynamic nature of continuously growing crops. In this paper we propose a 4D reconstruction approach to crop monitoring, which employs a spatio-temporal model of dynamic scenes that is useful for precision agriculture applications. Additionally, we provide a robust data association algorithm to address the problem of large appearance changes due to scenes being viewed from different angles at different points in time, which is critical to achieving 4D reconstruction. Finally, we collected a high-quality dataset with ground-truth statistics to evaluate the performance of our method. We demonstrate that our 4D reconstruction approach provides models that are qualitatively correct with respect to visual appearance and quantitatively accurate when measured against the ground truth geometric properties of the monitored crops.},
}

@InProceedings{leonardos2017icra,
  author        = {S. Leonardos and X. Zhou and K. Daniilidis},
  title         = {{Distributed Consistent Data Association Via Permutation Synchronization}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Sensor Networks, Agent-Based Systems, Autonomous Agents},
  abstract      = {Data association is one of the fundamental problems in multi-sensor systems. Most current techniques rely on pairwise data associations which can be spurious even after the employment of outlier rejection schemes. Considering multiple pairwise associations at once significantly increases accuracy and leads to consistency. In this work, we propose a fully decentralized method for globally consistent data association from pairwise data associations based on a distributed averaging scheme on the set of doubly stochastic matrices. We demonstrate the effectiveness of the proposed method using theoretical analysis and experimental evaluation.},
}

@InProceedings{wang2017icra,
  author        = {S. Wang and R. Clark and H. Wen and N. Trigoni},
  title         = {{DeepVO: Towards End-To-End Visual Odometry with Deep Recurrent Convolutional Neural Networks}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Visual-Based Navigation, Localization, CNN},
  abstract      = {This paper studies monocular visual odometry (VO) problem. Most of existing VO algorithms are developed under a standard pipeline including feature extraction, feature matching, motion estimation, local optimisation, etc. Although some of them have demonstrated superior performance, they usually need to be carefully designed and specifically fine-tuned to work well in different environments. Some prior knowledge is also required to recover an absolute scale for monocular VO. This paper presents a novel end-to-end framework for monocular VO by using deep Recurrent Convolutional Neural Networks (RCNNs). Since it is trained and deployed in an end-to-end manner, it infers poses directly from a sequence of raw RGB images (videos) without adopting any module in the conventional VO pipeline. Based on the RCNNs, it not only automatically learns effective feature representation for the VO problem through Convolutional Neural Networks, but also implicitly models sequential dynamics and relations using deep Recurrent Neural Networks. Extensive experiments on the KITTI VO dataset show competitive performance to state-ofthe-art methods, verifying that the end-to-end Deep Learning technique can be a viable complement to the traditional VO systems.},
}

@InProceedings{loquercio2017icra,
  author        = {A. Loquercio and M.T. Dymczyk and B. Zeisl and S. Lynen and R. Siegwart and I. Gilitschenski},
  title         = {{Efficient Descriptor Learning for Large Scale Localization}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Localization, Mapping, Recognition},
  abstract      = {Many robotics and Augmented Reality (AR) systems that use sparse keypoint-based visual maps operate in large and highly repetitive environments, where pose tracking and localization are challenging tasks. Additionally, these systems usually face further challenges, such as limited computational power, or insufficient memory for storing large maps of the entire environment. Thus, developing compact map representations and improving retrieval is of considerable interest for enabling large-scale visual place recognition and loop-closure. In this paper, we propose a novel approach to compress descriptors while increasing their discriminability and matchability, based on recent advances in neural networks. At the same time, we target resource-constrained robotics applications in our design choices. The main contributions of this work are twofold. First, we propose a linear projection from descriptor space to a lower-dimensional Euclidean space, based on a novel supervised learning strategy employing a triplet loss. Second, we show the importance of including contextual appearance information to the visual feature in order to improve matching under strong viewpoint, illumination and scene changes. Through detailed experiments on three challenging datasets, we demonstrate significant gains in performance over state-ofthe-art methods.},
}

@InProceedings{wang2017icra-fflb,
  author        = {X. Wang and S. Vozar and E. Olson},
  title         = {{FLAG: Feature-based Localization between Air and Ground}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Localization, Mapping, Computer Vision for Other Robotic Applications},
  abstract      = {In GPS-denied environments, robot systems typically revert to navigating with dead-reckoning and relative mapping, accumulating error in their global pose estimate. In this paper, we propose Feature-based Localization between Air and Ground (FLAG), a method for computing global position updates by matching features observed from ground to features in an aerial image. Our method uses stable, descriptorless features associated with vertical structure in the environment around a ground robot in previously unmapped areas, referencing only overhead imagery, without GPS. Multiple-hypothesis data association with a particle filter enables efficient recovery from data association error and odometry uncertainty. We implement a stereo system to demonstrate our vertical feature based global positioning approach in both indoor and outdoor scenarios, and show comparable performance to laser-scanmatching results in both environments.},
}

@InProceedings{xu2017icra,
  author        = {D. Xu and X. He and H. Zhao and J. Cui and H. Zha and F. Guillemard and S. Geronimi and F. Aioun},
  title         = {{Ego-Centric Traffic Behavior Understanding through Multi-Level Vehicle Trajectory Analysis}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Intelligent Transportation Systems},
  abstract      = {This study proposes a multi-level trajectory analysis method for modeling traffic behavior from an ego-centric view, where on-road vehicle trajectories are collected based on the authors previous studies of an on-board system consisting of multiple 2D lidar sensors. From an input set of trajectories, a set of hot regions (topics) that trajectory points most frequently present are first discovered using a sticky HDP-HMM; then, the major paths of the trajectories transitions across different hot regions are extracted by recursively mining frequent subsequences of topics; and finally, paths are modeled using a hierarchical hidden Markov model (HHMM), where the intra-path dynamics is represented using an HMM, in which each state corresponds to a hot region, while the inter-path transition is assumed to be Markovian. The model could be used for behavior prediction, i.e. whenever a vehicle is detected in a scene, predicting which route it will probably follow and how its trajectory will probably develop over time, which is essential to interpreting the potential risks for longer time horizons. Experiments are conducted using a large set of vehicle trajectories collected from motorways in Beijing, and promising results are presented.},
}

@InProceedings{hsiao2017icra,
  author        = {M. Hsiao and E. Westman and G. Zhang and M. Kaess},
  title         = {{Keyframe-Based Dense Planar SLAM}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {SLAM, Mapping, Localization},
  abstract      = {In this work, we develop a novel keyframe-based dense planar SLAM (KDP-SLAM) system, based on CPU only, to reconstruct large indoor environments in real-time using a hand-held RGB-D sensor. Our keyframe-based approach applies a fast dense method to estimate odometry, fuses depth measurements from small baseline images, extracts planes from the fused depth map, and optimizes the poses of the keyframes and landmark planes in a global factor graph using incremental smoothing and mapping (iSAM). Using the fast odometry estimation, correct plane correspondences may be found projectively, and the pose of each frame can be estimated accurately even without sufficient planes to fully constrain the 6 degree-of-freedom transformation. The depth map generated from the local fusion process generates higher quality reconstructions and plane segmentations by eliminating noise. Moreover, explicitly modeling plane landmarks in the fully probabilistic global optimization significantly reduces the drift that plagues other dense SLAM algorithms. We test our system on standard RGB-D benchmarks as well as additional indoor environments, demonstrating its state-of-the-art performance as a real-time dense 3D SLAM algorithm, without the use of GPU.},
}

@InProceedings{salaris2017icra,
  author        = {P. Salaris and R. Spica and P.R. Giordano and P. Rives},
  title         = {{Online Optimal Active Sensing Control}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Motion and Path Planning, Optimization and Optimal Control, Localization},
  abstract      = {This paper deals with the problem of active sensing control for nonlinear differentially flat systems. The objective is to improve the estimation accuracy of an observer by determining the inputs of the system that maximise the amount of information gathered by the outputs over a time horizon. In particular, we use the Observability Gramian (OG) to quantify the richness of the acquired information. First, we define a trajectory for the flat outputs of the system by using B-Spline curves. Then, we exploit an online gradient descent strategy to move the control points of the B-Spline in order to actively maximise the smallest eigenvalue of the OG over the whole planning horizon. While the system travels along its planned (optimized) trajectory, an Extended Kalman Filter (EKF) is used to estimate the system state. In order to keep memory of the past acquired sensory data for online re-planning, the OG is also computed on the past estimated state trajectories. This is then used for an online replanning of the optimal trajectory during the robot motion which is continuously refined by exploiting the state estimation obtained by the EKF. In order to show the effectiveness of our method we consider a simple but significant case of a planar robot with a single range measurement. The simulation results show that, along the optimal path, the EKF converges faster and provides a more accurate estimate than along other possible (non-optimal) paths.},
}

@InProceedings{bargoti2017icra,
  author        = {S. Bargoti and J.P. Underwood},
  title         = {{Deep Fruit Detection in Orchards}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Robotics in Agriculture and Forestry, Computer Vision for Other Robotic Applications, Object Detection, Segmentation, Categorization, CNN},
  abstract      = {An accurate and reliable image based fruit detection system is critical for supporting higher level agriculture tasks such as yield mapping and robotic harvesting. This paper presents the use of a state-of-the-art object detection framework, Faster R-CNN, in the context of fruit detection in orchards, including mangoes, almonds and apples. Ablation studies are presented to better understand the practical deployment of the detection network, including how much training data is required to capture variability in the dataset. Data augmentation techniques are shown to yield significant performance gains, resulting in a greater than two-fold reduction in the number of training images required. In contrast, transferring knowledge between orchards contributed to negligible performance gain over initialising the Deep Convolutional Neural Network directly from ImageNet features. Finally, to operate over orchard data containing between 100-1000 fruit per image, a tiling approach is introduced for the Faster R-CNN framework. The study has resulted in the best yet detection performance for these orchards relative to previous works, with an F1-score of > 0.9 achieved for apples and mangoes.},
}

@InProceedings{carlucci2017icra,
  author        = {F.M. Carlucci and P. Russo and B. Caputo},
  title         = {{A Deep Representation for Depth Images from Synthetic Data}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Object detection, Segmentation, Categorization, RGB-D Perception, Computer Vision for Other Robotic Applications, CNN},
  abstract      = {Convolutional Neural Networks (CNNs) trained on large scale RGB databases have become the secret sauce in the majority of recent approaches for object categorization from RGB-D data. Thanks to colorization techniques, these methods exploit the filters learned from 2D images to extract meaningful representations in 2.5D. Still, the perceptual signature of these two kind of images is very different, with the first usually strongly characterized by textures, and the second mostly by silhouettes of objects. Ideally, one would like to have two CNNs, one for RGB and one for depth, each trained on a suitable data collection, able to capture the perceptual properties of each channel for the task at hand. This has not been possible so far, due to the lack of a suitable depth database. This paper addresses this issue, proposing to opt for synthetically generated images rather than collecting by hand a 2.5D large scale database. While being clearly a proxy for real data, synthetic images allow to trade quality for quantity, making it possible to generate a virtually infinite amount of data. We show that the filters learned from such data collection, using the very same architecture typically used on visual data, learns very different filters, resulting in depth features (a) able to better characterize the different facets of depth images, and (b) complementary with respect to those derived from CNNs pretrained on 2D datasets. Experiments on two publicly available databases show the power of our approach.},
}

@InProceedings{suger2017icra,
  author        = {B. Suger and W. Burgard},
  title         = {{Global Outer-Urban Navigation with OpenStreetMap}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Autonomous Vehicle Navigation, Field Robots, Localization},
  abstract      = {Publicly available map services are widely used by humans for navigation and nowadays provide almost complete road network data. When utilizing such maps for autonomous navigation with mobile robots one is faced with the problem of inaccuracies of the map and the uncertainty about the position of the robot relative to the map. In this paper, we present a probabilistic approach to autonomous robot navigation using data from OpenStreetMap that associates tracks from OpenStreeetMap with the trails detected by the robot based on its 3DLiDAR data. It combines semantic terrain information, derived from the 3D-LiDAR data, with a Markov-Chain Monte-Carlo technique to match the tracks from OpenStreetMap with the sensor data. This enables our robot to utilize OpenStreetMap for navigation planning and to still stay on the trails during the execution of these plans. We present the results of extensive experiments carried out in real world settings that demonstrate the robustness of our system regarding the alignment of the vehicle pose relative to the OpenStreetMap data.},
}

@InProceedings{naseer2017icra,
  author        = {T. Naseer and G. Oliveira and T. Brox and W. Burgard},
  title         = {{Semantics-Aware Visual Localization under Challenging Perceptual Conditions}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Localization, Visual Learning, Semantic Scene Understanding},
  abstract      = {Visual place recognition under difficult perceptual conditions remains a challenging problem due to changing weather conditions, illumination and seasons. Long-term visual navigation approaches for robot localization should be robust to these dynamics of the environment. Existing methods typically leverage feature descriptions of whole images or image regions from Deep Convolutional Neural Networks. Some approaches also exploit sequential information to alleviate the problem of spatially inconsistent and non-perfect image matches. In this paper, we propose a novel approach for learning a discriminative holistic image representation which exploits the image content to create a dense and salient scene description. These salient descriptions are learnt over a variety of datasets under large perceptual changes. Such an approach enables us to precisely segment the regions of an image which are geometrically stable over large time lags. We combine features from these salient regions and an off-the-shelf holistic representation to form a more robust scene descriptor. We also introduce a semantically labeled dataset which captures extreme perceptual and structural scene dynamics over the course of 3 years. We evaluated our approach with extensive experiments on data collected over several kilometers in Freiburg and show that our learnt image representation outperforms off-the-shelf features from the deep networks and hand-crafted features.},
}

@InProceedings{lottes2017icra,
  author        = {P. Lottes and R. Khanna and J. Pfeifer and R. Siegwart and C. Stachniss},
  title         = {{UAV-Based Crop and Weed Classification for Smart Farming}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Robotics in Agriculture and Forestry, Mapping},
  abstract      = {Unmanned aerial vehicles (UAVs) and other robots in smart farming applications offer the potential to monitor farm land on a per-plant basis, which in turn can reduce the amount of herbicides and pesticides that must be applied. A central information for the farmer as well as for autonomous agriculture robots is the knowledge about the type and distribution of the weeds in the field. In this regard, UAVs offer excellent survey capabilities at low cost. In this paper, we address the problem of detecting value crops such as sugar beets as well as typical weeds using a camera installed on a light-weight UAV. We propose a system that performs vegetation detection, plant-tailored feature extraction, and classification to obtain an estimate of the distribution of crops and weeds in the field. We implemented and evaluated our system using UAVs on two farms, one in Germany and one in Switzerland and demonstrate that our approach allows for analyzing the field and classifying individual plants.},
  url           = {http://www.ipb.uni-bonn.de/wp-content/papercite-data/pdf/lottes17icra.pdf},
}

@InProceedings{roelofsen2017icra,
  author        = {S. Roelofsen and D. Gillet and A. Martinoli},
  title         = {{Collision Avoidance with Limited Field of View Sensing: A Velocity Obstacle Approach}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Collision Avoidance, Distributed Robot Systems, Sensor-based Control},
  abstract      = {Collision avoidance, in particular between robots, is an important component for autonomous robots. It is a necessary component in numerous applications such as humanrobot interaction, automotive or unmanned aerial vehicles. While many collision avoidance algorithms take into account actuation constraints, only a few consider sensing limitations. In this paper, we present a reciprocal collision avoidance algorithm based on the velocity obstacle approach that guarantees collision-free maneuvers even when the robots are only capable to sense their environment within a limited Field Of View (FOV). We also present the challenges associated to sensors with limited FOV, show the conditions under which maneuvering can be safely done, and the modifications that a velocity obstacle approach requires to satisfy such conditions. We provide simulations and real robot experiments to validate our approach.},
}

@InProceedings{fukui2017icra,
  author        = {R. Fukui and J. Schneider and T. Nishioka and S. Warisawa and I. Yamada},
  title         = {{Growth Measurement of Tomato Fruit Based on Whole Image Processing}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Computer Vision for Other Robotic Applications, Robotics in Agriculture and Forestry, Object detection, Segmentation, Categorization},
  abstract      = {- Crop grow measurement technologies are important to increase the farm productivity. Detection and measurement of fruit volume are useful for forecasting and harvesting applications. Some environmental challenges such as lighting conditions or occlusions make the fruit detection difficult. Our approach is based on features extraction from images through a sub-image clustering technique. Then images being described as a number of pixel in various labels are used in a regression model to estimate the fruit volume. The validity of the proposed method in experimental condition is successfully verified. The method is evaluated also in a field condition but results were inferior to the expectation. This paper tries to elucidate the reasons of the insufficient performance and tries to improve the proposed method in terms of illumination condition, precision and calculation time. Key Words: Agriculture, Field Robotics Growth measurement, Image processing, Tomato volume, Regression},
}

@InProceedings{yang2017icra-rmdm,
  author        = {Z. Yang and F. Gao and S. Shen},
  title         = {{Real-Time Monocular Dense Mapping on Aerial Robots Using Visual-Inertial Fusion}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Aerial Robotics, Mapping, Autonomous Vehicle Navigation},
  abstract      = {In this work, we present a solution to real-time monocular dense mapping. A tightly-coupled visual-inertial localization module is designed to provide metric and highaccuracy odometry. A motion stereo algorithm is proposed to take the video input from one camera to produce local depth measurements with semi-global regularization. The local measurements are then integrated into a global map for noise filtering and map refinement. The global map obtained is able to support navigation and obstacle avoidance for aerial robots through our indoor and outdoor experimental verification. Our system runs at 10Hz on an Nvidia Jetson TX1 by properly distributing computation to CPU and GPU. Through onboard experiments, we demonstrate its ability to close the perceptionaction loop for autonomous aerial robots. We release our implementation as open-source software1 .},
}

@InProceedings{gehrig2017icra,
  author        = {M. Gehrig and E. Stumm and T. Hinzmann and R. Siegwart},
  title         = {{Visual Place Recognition with Probabilistic Voting}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Localization, SLAM, Recognition},
  abstract      = {We propose a novel scoring concept for visual place recognition based on nearest neighbor descriptor voting and demonstrate how the algorithm naturally emerges from the problem formulation. Based on the observation that the number of votes for matching places can be evaluated using a binomial distribution model, loop closures can be detected with high precision. By casting the problem into a probabilistic framework, we not only remove the need for commonly employed heuristic parameters but also provide a powerful score to classify matching and non-matching places. We present methods for both a 2D-2D image matching and a 2D-3D landmark matching based on the above scoring. The approach maintains accuracy while being efficient enough for online application through the use of compact (low-dimensional) descriptors and fast nearest neighbor retrieval techniques. The proposed methods are evaluated on several challenging datasets in varied environments, showing state-of-the-art results with high precision and high recall.},
}

@InProceedings{jatavallabhula2017icra,
  author        = {K.M. Jatavallabhula and S.K. Gottipati and F. Chhaya and M. Krishna},
  title         = {{Reconstructing Vehicles from a Single Image: Shape Priors for Road Scene Understanding}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Object detection, Segmentation, Categorization, Visual Learning, Localization, CNN},
  abstract      = {We present an approach for reconstructing vehicles from a single (RGB) image, in the context of autonomous driving. Though the problem appears to be ill-posed, we demonstrate that prior knowledge about how 3D shapes of vehicles project to an image can be used to reason about the reverse process, i.e., how shapes (back-)project from 2D to 3D. We encode this knowledge in shape priors, which are learnt over a small keypoint-annotated dataset. We then formulate a shapeaware adjustment problem that uses the learnt shape priors to recover the 3D pose and shape of a query object from an image. For shape representation and inference, we leverage recent successes of Convolutional Neural Networks (CNNs) for the task of object and keypoint localization, and train a novel cascaded fully-convolutional architecture to localize vehicle keypoints in images. The shape-aware adjustment then robustly recovers shape (3D locations of the detected keypoints) while simultaneously filling in occluded keypoints. To tackle estimation errors incurred due to erroneously detected keypoints, we use an Iteratively Re-weighted Least Squares (IRLS) scheme for robust optimization, and as a by-product characterize noise models for each predicted keypoint. We evaluate our approach on autonomous driving benchmarks, and present superior results to existing monocular, as well as stereo approaches.},
}

@InProceedings{schneider2017icra,
  author        = {T. Schneider and M. Li and M. Burri and J. Nieto and R. Siegwart and I. Gilitschenski},
  title         = {{Visual-Inertial Self-Calibration on Informative Motion Segments}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Calibration and Identification, Visual-Based Navigation, SLAM},
  abstract      = {Environmental conditions and external effects, such as shocks, have a significant impact on the calibration parameters of visual-inertial sensor systems. Thus longterm operation of these systems cannot fully rely on factory calibration. Since the observability of certain parameters is highly dependent on the motion of the device, using short data segments at device initialization may yield poor results. When such systems are additionally subject to energy constraints, it is also infeasible to use full-batch approaches on a big dataset and careful selection of the data is of high importance. In this paper, we present a novel approach for resource efficient self-calibration of visual-inertial sensor systems. This is achieved by casting the calibration as a segment-based optimization problem that can be run on a small subset of informative segments. Consequently, the computational burden is limited as only a predefined number of segments is used. We also propose an efficient information-theoretic selection to identify such informative motion segments. In evaluations on a challenging dataset, we show our approach to significantly outperform state-of-the-art in terms of computational burden while maintaining a comparable accuracy.},
}

@InProceedings{honegger2017icra,
  author        = {D. Honegger and T. Sattler and M. Pollefeys},
  title         = {{Embedded Real-Time Multi-Baseline Stereo}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Range Sensing, Autonomous Vehicle Navigation, Object detection, Segmentation, Categorization},
  abstract      = {Dense depth map estimation from stereo cameras has many applications in robotic vision, e.g., obstacle detection, especially when performed in real-time. The range in which depth values can be accurately estimated is usually limited for two-camera stereo setups due to the fixed baseline between the cameras. In addition, two-camera setups suffer from wrong depth estimates caused by local minima in the matching cost functions. Both problems can be alleviated by adding more cameras as this creates multiple baselines of different lengths and since multi-image matching leads to unique minima. However, using more cameras usually comes at an increase in run-time. In this paper, we present a novel embedded system for multibaseline stereo. By exploiting the parallelization capabilities within FPGAs, we are able to estimate a depth map from multiple cameras in real-time. We show that our approach requires only little more power and weight compared to a twocamera stereo system. At the same time, we show that our system produces significantly better depth maps and is able to handle occlusion of some cameras, resulting in the redundancy typically desired for autonomous vehicles. Our system is small in size and leight-weight and can be employed even on a MAV platform with very strict power, weight, and size requirements.},
}

@InProceedings{khosravian2017icra,
  author        = {A. Khosravian and T. Chin and I. Reid and R. Mahony},
  title         = {A Discrete-Time Attitude Observer on SO(3) for Vision and GPS Fusion},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Localization, Sensor Fusion, Visual-Based Navigation},
  abstract      = {This paper proposes a discrete-time geometric attitude observer for fusing monocular vision with GPS velocity measurements. The observer takes the relative transformations obtained from processing monocular images with any visual odometry algorithm and fuses them with GPS velocity measurements. The objectives of this sensor fusion are twofold; first to mitigate the inherent drift of the attitude estimates of the visual odometry, and second, to estimate the orientation directly with respect to the North-East-Down frame. A key contribution of the paper is to present a rigorous stability analysis showing that the attitude estimates of the observer converge exponentially to the true attitude and to provide a lower bound for the convergence rate of the observer. Through experimental studies, we demonstrate that the observer effectively compensates for the inherent drift of the pure monocular vision based attitude estimation and is able to recover the North-East-Down orientation even if it is initialized with a very large attitude error.},
}

@InProceedings{dube2017icra,
  author        = {R. Dub{\'e} and D. Dugas and E. Stumm and J. Nieto and R. Siegwart and C.C. Lerma},
  title         = {{SegMatch: Segment Based Place Recognition in 3D Point Clouds}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Localization, Object detection, Segmentation, Categorization, SLAM},
  abstract      = {Place recognition in 3D data is a challenging task that has been commonly approached by adapting imagebased solutions. Methods based on local features suffer from ambiguity and from robustness to environment changes while methods based on global features are viewpoint dependent. We propose SegMatch, a reliable place recognition algorithm based on the matching of 3D segments. Segments provide a good compromise between local and global descriptions, incorporating their strengths while reducing their individual drawbacks. SegMatch does not rely on assumptions of perfect segmentation, or on the existence of objects in the environment, which allows for reliable execution on large scale, unstructured environments. We quantitatively demonstrate that SegMatch can achieve accurate localization at a frequency of 1Hz on the largest sequence of the KITTI odometry dataset. We furthermore show how this algorithm can reliably detect and close loops in real-time, during online operation. In addition, the source code for the SegMatch algorithm is made publicly available.},
}

@InProceedings{pfeiffer2017icra,
  author        = {M. Pfeiffer and M. Schaeuble and J. Nieto and R. Siegwart and C.C. Lerma},
  title         = {{From Perception to Decision: A Data-Driven Approach to End-To-End Motion Planning for Autonomous Ground Robots}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Learning and Adaptive Systems, Motion and Path Planning},
  abstract      = {Learning from demonstration for motion planning is an ongoing research topic. In this paper we present a model that is able to learn the complex mapping from raw 2D-laser range findings and a target position to the required steering commands for the robot. To our best knowledge, this work presents the first approach that learns a target-oriented endto-end navigation model for a robotic platform. The supervised model training is based on expert demonstrations generated in simulation with an existing motion planner. We demonstrate that the learned navigation model is directly transferable to previously unseen virtual and, more interestingly, real-world environments. It can safely navigate the robot through obstaclecluttered environments to reach the provided targets. We present an extensive qualitative and quantitative evaluation of the neural network-based motion planner, and compare it to a grid-based global approach, both in simulation and in realworld experiments.},
}

@InProceedings{gomez-ojeda2017icra,
  author        = {R. Gomez-Ojeda and F. Moreno and J. Gonzlez-Jimnez},
  title         = {{Accurate Stereo Visual Odometry with Gamma Distributions}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Autonomous Vehicle Navigation, Localization, Visual-Based Navigation},
  abstract      = {Point-based stereo visual odometry systems typically estimate the camera motion by minimizing a cost function of the projection residuals between consecutive frames. Under some mild assumptions, such minimization is equivalent to maximizing the probability of the measured residuals given a certain pose change, for which a suitable model of the error distribution (sensor model) becomes of capital importance in order to obtain accurate results. This paper proposes a robust probabilistic model for projection errors, based on real world data. For that, we argue that projection distances follow Gamma distributions, and hence, the introduction of these models in a probabilistic formulation of the motion estimation process increases both precision and accuracy. Our approach has been validated through a series of experiments with both synthetic and real data, revealing an improvement in accuracy while not increasing the computational burden.},
}

@InProceedings{merzic2017icra,
  author        = {H. Merzic and E. Stumm and M.T. Dymczyk and R. Siegwart and I. Gilitschenski},
  title         = {{Map Quality Evaluation for Visual Localization}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Localization, Mapping, Visual-Based Navigation},
  abstract      = {A variety of end-user devices involving keypointbased mapping systems are about to hit the market e.g. as part of smartphones, cars, robotic platforms, or virtual and augmented reality applications. Thus, the generated map data requires automated evaluation procedures that do not require experienced personnel or ground truth knowledge of the underlying environment. A particularly important question enabling commercial applications is whether a given map is of sufficient quality for localization. This paper proposes a framework for predicting localization performance in the context of visual landmark-based mapping. Specifically, we propose an algorithm for predicting performance of vision-based localization systems from different poses within the map. To achieve this, a metric is defined that assigns a score to a given query pose based on the underlying map structure. The algorithm is evaluated on two challenging datasets involving indoor data generated using a handheld device and outdoor data from an autonomous fixedwing unmanned aerial vehicle (UAV). Using these, we are able to show that the score provided by our method is highly correlated to the true localization performance. Furthermore, we demonstrate how the predicted map quality can be used within a belief based path planning framework in order to provide reliable trajectories through high-quality areas of the map.},
}

@InProceedings{pathak2017icra,
  author        = {S. Pathak and A. Thomas and V. Indelman},
  title         = {{Nonmyopic Data Association Aware Belief Space Planning for Robust Active Perception}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {SLAM, Autonomous Vehicle Navigation, Localization},
  abstract      = {One key assumption of Belief Space Planning (BSP) is that the data association is known perfectly. In this paper, we relax this assumption in the context of non-myopic planning as well as belief being a Gaussian Mixture Model (GMM). Interestingly, explicit reasoning about the data association within the belief enables our framework to have parsimonious data association, thereby resulting in a scalable solution compared with nave permutational approaches. Unlike in some of the recent approaches where the number of components in a GMM belief can only be reduced, in our approach this can also go up such as due to perceptual aliasing present in the environment. Furthermore, our approach naturally integrates with inference, providing a unified framework for robust passive and active perception. We demonstrate key aspects of our approach and its comparison with the state of the art on a general abstract domain as well as in a real robot setup.},
}

@InProceedings{hietanen2017icra,
  author        = {A.E. Hietanen and J. Halme and A.G. Buch and J.M. Latokartano and J. Kamarainen},
  title         = {{Robustifying Correspondence Based 6D Object Pose Estimation}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Computer Vision for Other Robotic Applications, Recognition, RGB-D Perception},
  abstract      = {We propose two methods to robustify point correspondence based 6D object pose estimation. The first method, curvature filtering, is based on the assumption that low curvature regions provide false matches, and removing points in these regions improves robustness. The second method, region pruning, is more general by making no assumptions about local surface properties. Our region pruning segments a model point cloud into cluster regions and searches good region combinations using a validation set. The robustifying methods are general and can be used with any correspondence based method. For the experiments, we evaluated three correspondence selection methods, Geometric Consistency (GC) [1], Hough Grouping (HG) [2] and Search of Inliers (SI) [3] and report systematic improvements for their robustified versions with two distinct datasets.},
}

@InProceedings{fermn-len2017icra,
  author        = {L. Fermn-Len and J. Neira and J.A. Castellanos},
  title         = {{Incremental Contour-Based Topological Segmentation for Robot Exploration}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Mapping, Semantic Scene Understanding, SLAM},
  abstract      = {We propose an alternative to the common approaches to the topological segmentation in structured or unstructured environments, Contour-Based Segmentation. It is faster and equally accurate, without the need of fine tuning parameters or heuristics. During robotic exploration, we propose an incremental version that reduces the processing time by reusing the previous segmentation. Tests demonstrate the velocity and quality in room segmentation in both batch and incremental mode. Tests also demonstrate the incremental version outperforms the state of the art in incremental topological segmentation.},
}

@InProceedings{popovic2017icra,
  author        = {M. Popovic and G. Hitz and J. Nieto and I. Sa and R. Siegwart and E. Galceran},
  title         = {{Online Informative Path Planning for Active Classification Using UAVs}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Motion and Path Planning, Robotics in Agriculture and Forestry, Planning, Scheduling and Coordination},
  abstract      = {In this paper, we introduce an informative path planning (IPP) framework for active classification using unmanned aerial vehicles (UAVs). Our algorithm uses a combination of global viewpoint selection and evolutionary optimization to refine the planned trajectory in continuous 3D space while satisfying dynamic constraints. Our approach is evaluated on the application of weed detection for precision agriculture. We model the presence of weeds on farmland using an occupancy grid and generate adaptive plans according to informationtheoretic objectives, enabling the UAV to gather data efficiently. We validate our approach in simulation by comparing against existing methods, and study the effects of different planning strategies. Our results show that the proposed algorithm builds maps with over 50\% lower entropy compared to traditional lawnmower coverage in the same amount of time. We demonstrate the planning scheme on a multirotor platform with different artificial farmland set-ups.},
}

@InProceedings{schaffernicht2017icra,
  author        = {E. Schaffernicht and V.M.H. Bennetts and A.J. Lilienthal},
  title         = {{Mobile Robots for Learning Spatio-temporal Interpolation Models in Sensor Networks - the Echo State Map Approach}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Sensor Networks, Field Robots},
  abstract      = {Sensor networks have limited capabilities to model complex phenomena occuring between sensing nodes. Mobile robots can be used to close this gap and learn local interpolation models. In this paper, we utilize Echo State Networks in order to learn the calibration and interpolation model between sensor nodes using measurements collected by a mobile robot. The use of Echo State Networks allows to deal with temporal dependencies implicitly, while the spatial mapping with a Gaussian Process estimator exploits the fact that Echo State Networks learn linear combinations of complex temporal dynamics. The resulting Echo State Map elegantly combines spatial and temporal cues into a single representation. We showcase the method in the exposure modeling task of building dust distribution maps for foundries, a challenge which is of great interest to occupational health researchers. Results from simulated data and real world experiments highlight the potential of Echo State Maps. While we focus on particulate matter measurements, the method can be applied for any other environmental variables like temperature or gas concentration.},
}

@InProceedings{withers2017icra,
  author        = {D. Withers and P. Newman},
  title         = {{Modelling Scene Change in Large Scale Long Term Laser Localisation}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Localization, Mapping, Autonomous Vehicle Navigation},
  abstract      = {This paper addresses a difficulty in large-scale long term laser localisation - how to deal with scene change. We pose this as a distraction suppression problem. Urban driving environments are frequently subject to large dynamic outliers, such as buses, trucks etc. These objects can mask the static elements of the prior map that we rely on for localisation. At the same time some objects change shape in a way that is less dramatic but equally pernicious during localisation for example trees over seasons and in wind, shop fronts and doorways. In this paper, we show how we can learn in high resolution, the areas of our map that are subject to such distractions (low value data) in a place-dependent approach. We demonstrate how to utilise this model to select individual laser measurements for localisation. Specifically, by leveraging repeated operation over weeks and months, for each point in our map pointcloud we build distributions of the errors associated with that point for multiple localisation passes. These distributions are then used to determine the legitimacy of laser measurements prior to their use in localisation. We demonstrate distraction suppression as a front-end process to large scale localiser by incrementally adding 50km of error data to our base map and show that robustness is improved over the base system with a further 10km of urban driving.},
}

@InProceedings{mactavish2017icra,
  author        = {K. MacTavish and M. Paton and T. Barfoot},
  title         = {{Visual Triage: A Bag-Of-Words Experience Selector for Long-Term Visual Route Following}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Visual-Based Navigation, Autonomous Vehicle Navigation},
  abstract      = {Our work builds upon Visual Teach & Repeat 2 (VT&R2): a vision-in-the-loop autonomous navigation system that enables the rapid construction of route networks, safely built through operator-controlled driving. Added routes can be followed autonomously using visual localization. To enable long-term operation that is robust to appearance change, its Multi-Experience Localization (MEL) leverages many previously driven experiences when localizing to the manually taught network. While this multi-experience method is effective across appearance change, the computation becomes intractable as the number of experiences grows into the tens and hundreds. This paper introduces an algorithm that prioritizes experiences most relevant to live operation, limiting the number of experiences required for localization. The proposed algorithm uses a visual Bag-of-Words description of the live view to select relevant experiences based on what the vehicle is seeing right now, without having to factor in all possible environmental influences on scene appearance. This system runs in the loop, in real time, does not require bootstrapping, can be applied to any pointfeature MEL paradigm, and eliminates the need for visual training using an online, local visual vocabulary. By picking a subset of visually similar experiences to the live view, we demonstrate safe, vision-in-the-loop route following over a 31 hour period, despite appearance as different as night and day.},
}

@InProceedings{yousif2017icra,
  author        = {K. Yousif and Y. Taguchi and S. Ramalingam},
  title         = {{MonoRGBD-SLAM: Simultaneous Localization and Mapping Using Both Monocular and RGBD Cameras}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {SLAM, RGB-D Perception, Sensor Fusion},
  abstract      = {RGBD SLAM systems have shown impressive results, but the limited field of view (FOV) and depth range of typical RGBD cameras still cause problems for registering distant frames. Monocular SLAM systems, in contrast, can exploit wide-angle cameras and do not have the depth range limitation, but are unstable for textureless scenes. We present a SLAM system that uses both an RGBD camera and a wide-angle monocular camera for combining the advantages of the two sensors. Our system extracts 3D point features from RGBD frames and 2D point features from monocular frames, which are used to perform both RGBD-to-RGBD and RGBDto-monocular registration. To compensate for different FOV and resolution of the cameras, we generate multiple virtual images for each wide-angle monocular image and use the feature descriptors computed on the virtual images to perform the RGBD-to-monocular matches. To compute the poses of the frames, we construct a graph where nodes represent RGBD and monocular frames and edges denote the pairwise registration results between the nodes. We compute the global poses of the nodes by first finding the minimum spanning trees (MSTs) of the graph and then pruning edges that have inconsistent poses due to possible mismatches using the MST result. We finally run bundle adjustment on the graph using all the consistent edges. Experimental results show that our system registers a larger number of frames than using only an RGBD camera, leading to larger-scale 3D reconstruction.},
}

@InProceedings{palmieri2017icra,
  author        = {L. Palmieri and T.P. Kucner and M. Magnusson and A.J. Lilienthal and K.O. Arras},
  title         = {{Kinodynamic Motion Planning on Gaussian Mixture Fields}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Nonholonomic Motion Planning, Motion and Path Planning, Reactive and Sensor-Based Planning},
  abstract      = {We present a mobile robot motion planning approach under kinodynamic constraints that exploits learned perception priors in the form of continuous Gaussian mixture fields. Our Gaussian mixture fields are statistical multi-modal motion models of discrete objects or continuous media in the environment that encode e.g. the dynamics of air or pedestrian flows. We approach this task using a recently proposed circular linear flow field map based on semi-wrapped GMMs whose mixture components guide sampling and rewiring in an RRT* algorithm using a steer function for non-holonomic mobile robots. In our experiments with three alternative baselines, we show that this combination allows the planner to very efficiently generate high-quality solutions in terms of path smoothness, path length as well as natural yet minimum control effort motions through multi-modal representations of Gaussian mixture fields.},
}

@InProceedings{teixeira2017icra,
  author        = {L. Teixeira and M. Chli},
  title         = {{Real-Time Local 3D Reconstruction for Aerial Inspection using Superpixel Expansion}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Aerial Robotics, Visual-Based Navigation, Computer Vision for Automation},
  abstract      = {On the quest of automating the navigation of challenging and promising Robotics platforms such as small Unmanned Aerial Vehicles (UAVs), the community has been increasingly active in developing perception capabilities able to run onboard such platforms in real-time. Despite that visionbased techniques have been at the heart of recent advancements, the realistic employment onboard UAVs is still in its infancy. Inspired by some of the most recent breakthroughs in online dense scene estimation and borrowing fundamental concepts from Computer Vision, in this work we propose a new pipeline for real-time, local scene reconstruction using a single camera for aerial navigation. Aiming for denser scene estimation than traditional feature-based maps with the ability to run onboard a small UAV in real-time, the proposed approach is demonstrated to achieve unprecedented performance producing rich maps of the cameras workspace, timely enough to serve in obstacle avoidance and real-time interaction of a robot with its direct surroundings. Evaluation on benchmarking datasets and on challenging aerial footage captured with a UAV featuring a conventional camera, reveals dramatic speed-ups, as well as denser and more accurate local reconstructions with respect to the state of the art.},
}

@InProceedings{pumarola2017icra,
  author        = {A. Pumarola and A. Vakhitov and A. Agudo and A. Sanfeliu and F. Moreno-Noguer},
  title         = {{PL-SLAM: Real-Time Monocular Visual SLAM with Points and Lines}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {SLAM, Visual Tracking},
  abstract      = {Low textured scenes are well known to be one of the main Achilles heels of geometric computer vision algorithms relying on point correspondences, and in particular for visual SLAM. Yet, there are many environments in which, despite being low textured, one can still reliably estimate line-based geometric primitives, for instance in city and indoor scenes, or in the so-called Manhattan worlds, where structured edges are predominant. In this paper we propose a solution to handle these situations. Specifically, we build upon ORBSLAM, presumably the current state-of-the-art solution both in terms of accuracy as efficiency, and extend its formulation to simultaneously handle both point and line correspondences. We propose a solution that can even work when most of the points are vanished out from the input images, and, interestingly it can be initialized from solely the detection of line correspondences in three consecutive frames. We thoroughly evaluate our approach and the new initialization strategy on the TUM RGB-D benchmark and demonstrate that the use of lines does not only improve the performance of the original ORB-SLAM solution in poorly textured frames, but also systematically improves it in sequence frames combining points and lines, without compromising the efficiency.},
}

@InProceedings{contreras-toledo2017icra,
  author        = {L.A. Contreras-Toledo and W. Mayol},
  title         = {{O-POCO: Online POint Cloud COmpression Mapping for Visual Odometry and SLAM}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Localization, SLAM, Mapping},
  abstract      = {This paper presents O-POCO, a visual odometry and SLAM system that makes online decisions regarding what to map and what to ignore. It takes a point cloud from classical SfM and aims to sample it on-line by selecting map features useful for future 6D relocalisation. We use the cameras traveled trajectory to compartamentalize the point cloud, along with visual and spatial information to sample and compress the map. We propose and evaluate a number of different information layers such as the descriptor informations relative entropy, map-feature occupancy grid, and the point clouds geometry error. We compare our proposed system against both SfM, and online and offline ORB-SLAM using publicly available datasets in addition to our own. Results show that our online compression strategy is capable of outperforming the baseline even for conditions when the number of features per key-frame used for mapping is four times less.},
}

@InProceedings{jafari2017icra,
  author        = {O.H. Jafari and O. Groth and A. Kirillov and M.Y. Yang and C. Rother},
  title         = {{Analyzing Modular CNN Architectures for Joint Depth Prediction and Semantic Segmentation}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Semantic Scene Understanding, CNN},
  abstract      = {This paper addresses the task of designing a modular neural network architecture that jointly solves different tasks. As an example we use the tasks of depth estimation and semantic segmentation given a single RGB image. The main focus of this work is to analyze the cross-modality influence between depth and semantic prediction maps on their joint refinement. While most of the previous works solely focus on measuring improvements in accuracy, we propose a way to quantify the cross-modality influence. We show that there is a relationship between final accuracy and cross-modality influence, although not a simple linear one. Hence a larger cross-modality influence does not necessarily translate into an improved accuracy. We find that a beneficial balance between the cross-modality influences can be achieved by network architecture and conjecture that this relationship can be utilized to understand different network design choices. Towards this end we propose a Convolutional Neural Network (CNN) architecture that fuses the state-of-the-art results for depth estimation and semantic labeling. By balancing the crossmodality influences between depth and semantic prediction, we achieve improved results for both tasks using the NYU-Depth v2 benchmark.},
}

@InProceedings{agarwal2017icra,
  author        = {S. Agarwal and V. Shree and S. Chakravorty},
  title         = {{RFM-SLAM: Exploiting Relative Feature Measurements to Separate Orientation and Position Estimation in SLAM}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {SLAM},
  abstract      = {The SLAM problem is known to have a special property that when robot orientation is known, estimating the history of robot poses and feature locations can be posed as a standard linear least squares problem. In this work, we develop a SLAM framework that uses relative featureto-feature measurements to exploit this structural property of SLAM. Relative feature measurements are used to pose a linear estimation problem for pose-to-pose orientation constraints. This is followed by solving an iterative non-linear on-manifold optimization problem to compute the maximum likelihood estimate for robot orientation given relative rotation constraints. Once the robot orientation is computed, we solve a linear problem for robot position and map estimation. Our approach reduces the computational complexity of non-linear optimization by posing a smaller optimization problem as compared to standard graph-based methods for feature-based SLAM. Further, empirical results show our method avoids catastrophic failures that arise in existing methods due to using odometery as an initial guess for non-linear optimization, while its accuracy degrades gracefully as sensor noise is increased. We demonstrate our method through extensive simulations and comparisons with an existing state-of-the-art solver. Keywords: SLAM, graph-based SLAM, non-linear optimization, relative measurements},
}

@InProceedings{alzugaray2017icra,
  author        = {I. Alzugaray and L. Teixeira and M. Chli},
  title         = {{Short-Term UAV Path-Planning with Monocular-Inertial SLAM in the Loop}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Aerial Robotics, Visual-Based Navigation, SLAM},
  abstract      = {Small Unmanned Aerial Vehicles (UAVs) are some of the most promising robotic platforms in a variety of applications due to their high mobility. Their restricted computational and payload capabilities, however, translate into significant challenges in automating their navigation. With Simultaneous Localization And Mapping (SLAM) systems recently demonstrated to be employable onboard UAVs, the focus fall on path-planning on the quest of achieving autonomous navigation. With the vast body of path-planning literature often assuming perfect maps or maps known a priori, the biggest challenge lies in dealing with the robustness and accuracy limitations of onboard SLAM in real missions. In this spirit, this paper proposes a path-planning algorithm designed to work in the loop of the SLAM estimation of a monocular-inertial system. This point-to-point planner is demonstrated to navigate in an unknown environment using the incrementally generated SLAM map, while dictating the navigation strategy for preferable acquisition of sensor data for better estimations within SLAM. A thorough evaluation testbed of both simulated and real data is presented, demonstrating the robustness of the proposed pipeline against the state-of-the-art and its dramatically lower computational complexity, revealing its suitability to UAV navigation. Videohttps://youtu.be/Izn_vVb_M-E},
}

@InProceedings{schwarting2017icra,
  author        = {W. Schwarting and J. Alonso-Mora and L. Paull and S. Karaman and D. Rus},
  title         = {{Parallel Autonomy in Automated Vehicles: Safe Motion Generation with Minimal Intervention}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Collision Avoidance, Motion and Path Planning, Autonomous Vehicle Navigation},
  abstract      = {Current state-of-the-art vehicle safety systems, such as assistive braking or automatic lane following, are still only able to help in relatively simple driving situations. We introduce a Parallel Autonomy shared-control framework that produces safe trajectories based on human inputs even in much more complex driving scenarios, such as those commonly encountered in an urban setting. We minimize the deviation from the human inputs while ensuring safety via a set of collision avoidance constraints. We develop a receding horizon planner formulated as a Non-linear Model Predictive Control (NMPC) including analytic descriptions of road boundaries, and the configurations and future uncertainties of other traffic participants, and directly supplying them to the optimizer without linearization. The NMPC operates over both steering and acceleration simultaneously. Furthermore, the proposed receding horizon planner also applies to fully autonomous vehicles. We validate the proposed approach through simulations in a wide variety of complex driving scenarios such as leftturns across traffic, passing on busy streets, and under dynamic constraints in sharp turns on a race track.},
}

@InProceedings{massiceti2017icra,
  author        = {D. Massiceti and A. Krull and E. Brachmann and C. Rother and P. Torr},
  title         = {Random Forests versus Neural Networks - What's Best for Camera Localization?},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Localization, Visual-Based Navigation, Computer Vision for Other Robotic Applications},
  abstract      = {This work addresses the task of camera localization in a known 3D scene given a single input RGB image. State-of-the-art approaches accomplish this in two steps: firstly, regressing for every pixel in the image its 3D scene coordinate and subsequently, using these coordinates to estimate the final 6D camera pose via RANSAC. To solve the first step, Random Forests (RFs) are typically used. On the other hand, Neural Networks (NNs) reign in many dense regression tasks, but are not test-time efficient. We ask the question: which of the two is best for camera localization? To address this, we make two method contributions: (1) a test-time efficient NN architecture which we term a ForestNet that is derived and initialized from a RF, and (2) a new fully-differentiable robust averaging technique for regression ensembles which can be trained endto-end with a NN. Our experimental findings show that for scene coordinate regression, traditional NN architectures are superior to test-time efficient RFs and ForestNets, however, this does not translate to final 6D camera pose accuracy where RFs and ForestNets perform slightly better. To summarize, our best method, a ForestNet with a robust average, which has an equivalent fast and lightweight RF, improves over the stateof-the-art for camera localization on the 7-Scenes dataset [1]. While this work focuses on scene coordinate regression for camera localization, our innovations may also be applied to other continuous regression tasks.},
}

@InProceedings{scheel2017icra,
  author        = {A. Scheel and S. Reuter and K. Dietmayer},
  title         = {{Vehicle Tracking Using Extended Object Methods: An Approach for Fusing Radar and Laser}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Intelligent Transportation Systems, Sensor Fusion, Range Sensing},
  abstract      = {Combining data from heterogeneous sensors allows to enhance tracking systems by increasing the field of view, incorporating redundancy, and improving the performance by exploiting complementary sensor characteristics. This paper proposes a new vehicle tracking approach for vehicle environment perception that fuses radar and laser data. A Random-Finite-Set-based tracking filter, which permits a clear mathematical formulation of the multi-object problem, is used as fusion center. In combination with extended object measurement models that work on the raw sensor data directly, the filter uses all available information without the need for further preprocessing routines, considers object interdependencies, and works in ambiguous situations. The results are evaluated using experimental data from a test vehicle.},
}

@InProceedings{park2017icra,
  author        = {S. Park and T. Sch{\"o}ps and M. Pollefeys},
  title         = {{Illumination Change Robustness in Direct Visual SLAM}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {SLAM, Visual Tracking, RGB-D Perception},
  abstract      = {Direct visual odometry and Simultaneous Localization and Mapping (SLAM) methods determine camera poses by means of direct image alignment. This optimizes a photometric cost term based on the Lucas-Kanade method. Many recent works use the brightness constancy assumption in the alignment cost formulation and therefore cannot cope with significant illumination changes. Such changes are especially likely to occur for loop closures in SLAM. Alternatives exist which attempt to match images more robustly. In our paper, we perform a systematic evaluation of real-time capable methods. We determine their accuracy and robustness in the context of odometry and of loop closures, both on real images as well as synthetic datasets with simulated lighting changes. We find that for real images, a Census-based method outperforms the others. We make our new datasets available online3 .},
}

@InProceedings{lukierski2017icra,
  author        = {R. Lukierski and S. Leutenegger and A.J. Davison},
  title         = {{Room Layout Estimation from Rapid Omnidirectional Exploration}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Omnidirectional Vision, SLAM, Domestic Robots},
  abstract      = {A new generation of practical, low-cost indoor robots is now using wide-angle cameras to aid navigation, but usually this is limited to position estimation via sparse featurebased SLAM. Such robots usually have little global sense of the dimensions, demarcation or identities of the rooms they are in, information which would be very useful to enable behaviour with much more high level intelligence. In this paper we show that we can augment an omnidirectional SLAM pipeline with straightforward dense stereo estimation and simple and robust room model fitting to obtain rapid and reliable estimation of the global shape of typical rooms from short robot motions. We have tested our method extensively in real homes, offices and on synthetic data. We also give examples of how our method can extend to making composite maps of larger rooms, and detecting room transitions.},
}

@InProceedings{jaimez2017icra,
  author        = {M. Jaimez and C. Kerl and J. Gonzalez and D. Cremers},
  title         = {{Fast Odometry and Scene Flow from RGB-D Cameras Based on Geometric Clustering}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {RGB-D Perception, Visual Tracking},
  abstract      = {In this paper we propose an efficient solution to jointly estimate the camera motion and a piecewise-rigid scene flow from an RGB-D sequence. The key idea is to perform a two-fold segmentation of the scene, dividing it into geometric clusters that are, in turn, classified as static or moving elements. Representing the dynamic scene as a set of rigid clusters drastically accelerates the motion estimation, while segmenting it into static and dynamic parts allows us to separate the camera motion (odometry) from the rest of motions observed in the scene. The resulting method robustly and accurately determines the motion of an RGB-D camera in dynamic environments with an average runtime of 80 milliseconds on a multi-core CPU. The code is available for public use/test.},
}

@InProceedings{schenck2017icra,
  author        = {C. Schenck and D. Fox},
  title         = {{Visual Closed-Loop Control for Pouring Liquids}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Learning and Adaptive Systems, Domestic Robots},
  abstract      = {Pouring a specific amount of liquid is a challenging task. In this paper we develop methods for robots to use visual feedback to perform closed-loop control for pouring liquids. We propose both a model-based and a model-free method utilizing deep learning for estimating the volume of liquid in a container. Our results show that the model-free method is better able to estimate the volume. We combine this with a simple PID controller to pour specific amounts of liquid, and show that the robot is able to achieve an average 38ml deviation from the target amount. To our knowledge, this is the first use of raw visual feedback to pour liquids in robotics.},
}

@InProceedings{siam2017icra,
  author        = {S.M. Siam and H. Zhang},
  title         = {{Fast-SeqSLAM: A Fast Appearance Based Place Recognition Algorithm}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {SLAM, Localization, Mapping},
  abstract      = {Loop closure detection or place recognition is a fundamental problem in robot simultaneous localization and mapping (SLAM). SeqSLAM is considered to be one of the most successful algorithms for loop closure detection as it has been demonstrated to be able to handle significant environmental condition changes including those due to illumination, weather, and time of the day. However, SeqSLAM relies heavily on exhaustive sequence matching, a computationally expensive process that prevents the algorithm from being used in dealing with large maps. In this paper, we propose Fast-SeqSLAM, an efficient version of SeqSLAM. Fast-SeqSLAM has a much reduced time complexity without degrading the accuracy, and this is achieved by using an approximate nearest neighbor (ANN) algorithm to match the current image with those in the robot map and extending the idea of SeqSLAM to greedily search a sequence of images that best match with the current sequence. We demonstrate the effectiveness of our Fast-SeqSLAM algorithm in appearance based loop closure detection.},
}

@InProceedings{meier2017icra,
  author        = {L. Meier and D. Honegger and V. Vilhjalmsson and M. Pollefeys},
  title         = {{Real-Time Stereo Matching Failure Prediction and Resolution Using Orthogonal Stereo Setups}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Range Sensing, Computer Vision for Automation, Failure Detection and Recovery},
  abstract      = {Estimating the depth from two images with a baseline has a well-known regular problem: When a line is parallel to the epipolar geometry it is not possible to estimate the depth from pixels on this line. Moreover, the classic measure for the certainty of the depth estimate fails as well: The matching score between the template and any pixel on the epipolar line is perfect. This results for common scenes in incorrect matches with very high confidence, some even resistant to leftright image checks. It is straightforward to try to address this by adding a second stereo head in a perpendicular direction. However, it is nontrivial to identify the failure and fuse the two depth maps in a real-time system. A simple weighted average will alleviate the problem but still result in a very large error in the depth map. Our contributions are: 1) We derive a model to predict the failure of stereo by leveraging the matching scores and 2) we propose a combined cost function to fuse two depth maps from orthogonal stereo heads using the failure prediction, matching score and consistency. We show the resulting system in real-time operation on a low-latency system in indoor, urban and natural environments.},
}

@InProceedings{alahi2017icra,
  author        = {A. Alahi and J. Wilson and L. Fei-Fei and S. Savarese},
  title         = {{Unsupervised Camera Localization in Crowded Spaces}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Sensor Networks, Surveillance Systems, Human detection and tracking},
  abstract      = {Existing camera networks in public spaces such as train terminals or malls can help social robots to navigate crowded scenes. However, the localization of the cameras is required, i.e., the positions and poses of all cameras in a unique reference. In this work, we estimate the relative location of any pair of cameras by solely using noisy trajectories observed from each camera. We propose a fully unsupervised learning technique using unlabelled pedestrians motion patterns captured in crowded scenes. We first estimate the pairwise camera parameters by optimally matching single-view pedestrian tracks using social awareness. Then, we show the impact of jointly estimating the network parameters. This is done by formulating a nonlinear least square optimization problem, leveraging a continuous approximation of the matching function. We evaluate our approach in real-world environments such as train terminals, where several hundreds of individuals need to be tracked across dozens of cameras every second.},
}

@InProceedings{kuhlman2017icra,
  author        = {M.J. Kuhlman and M.W. Otte and D. Sofge and S.K. Gupta},
  title         = {{Maximizing Mutual Information for Multipass Target Search in Changing Environments}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Motion and Path Planning, Search and Rescue Robots},
  abstract      = {Motion planning for multi-target autonomous search requires efciently gathering as much information over an area as possible with an imperfect sensor. In disaster scenarios and contested environments the spatial connectivity may unexpectedly change (due to aftershock, avalanche, ood, building collapse, adversary movements, etc.) and the ight envelope may evolve as a known function of time to ensure rescue worker safety or to facilitate other mission goals. Algorithms designed to handle both expected and unexpected changes must: (1) reason over a sufciently long time horizon to respect expected changes, and (2) replan quickly in response to unexpected changes. These ambitions are hindered by the submodularity property of mutual information, which makes optimal solutions NP-hard to compute. We present an algorithm for autonomous search in changing environments that uses a variety of techniques to improve both the speed and time horizon, including using admissible heuristics to speed up the search.},
}

@InProceedings{ballardini2017icra,
  author        = {A.L. Ballardini and D. Cattaneo and S. Fontana and D.G. Sorrenti},
  title         = {{An Online Probabilistic Road Intersection Detector}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Computer Vision for Transportation, Intelligent Transportation Systems, Localization},
  abstract      = {In this paper we propose a probabilistic approach for detecting and classifying urban road intersections from a moving vehicle. The approach is based on images from an onboard stereo rig; it relies on the detection of the road ground plane on one side, and on a pixel-level classification of the road on the other. The two processing pipelines are then integrated and the parameters of the road components, i.e., the intersection geometry, are inferred. As opposed to other state-of-the-art offline methods, which require processing of the whole video sequence, our approach integrates the image data by means of an online procedure. The experiments have been performed on well-known KITTI datasets, allowing for future comparisons.},
}

@InProceedings{gregorio2017icra,
  author        = {D.D. Gregorio and L.D. Stefano},
  title         = {{SkiMap: An Efficient Mapping Framework for Robot Navigation}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Mapping, SLAM, RGB-D Perception},
  abstract      = {We present a novel mapping framework for robot navigation which features a multi-level querying system capable to obtain rapidly representations as diverse as a 3D voxel grid, a 2.5D height map and a 2D occupancy grid. These are inherently embedded into a memory and time efficient core data structure organized as a Tree of SkipLists. Compared to the wellknown Octree representation, our approach exhibits a better time efficiency, thanks to its simple and highly parallelizable computational structure, and a similar memory footprint when mapping large workspaces. Peculiarly within the realm of mapping for robot navigation, our framework supports realtime erosion and re-integration of measurements upon reception of optimized poses from the sensor tracker, so as to improve continuously the accuracy of the map.},
}

@InProceedings{fridovich-keil2017icra,
  author        = {D. Fridovich-Keil and E. Nelson and A. Zakhor},
  title         = {{AtomMap: A Probabilistic Amorphous 3D Map Representation for Robotics and Surface Reconstruction}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Mapping, Autonomous Vehicle Navigation, SLAM},
  abstract      = {We present a new 3D probabilistic occupancy map representation for robotics applications by relaxing the commonly-assumed constraint that space must be perfectly tessellated. We replace the regular structure of 3D grids with an unstructured collection of non-overlapping, equally-sized spheres, which we call atoms. Abandoning the grid structure allows a more accurate representation of space directly tangent to surfaces, which facilitates a number of applications such as high fidelity surface reconstruction and surface-guided path planning. Maps composed of atoms can distinguish between free, occupied, and unknown space, support computationally efficient insertions and collision queries, provide free space planning guarantees, and achieve state-of-the-art memory efficiency over large volumes. This is achieved while simultaneously reducing quantization effects in the vicinity of surfaces and defining a useful implicit surface representation.},
}

@InProceedings{khanna2017icra,
  author        = {R. Khanna and I. Sa and J. Nieto and R. Siegwart},
  title         = {{On Field Radiometric Calibration for Multispectral Cameras}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Calibration and Identification, Aerial Robotics, Range Sensing},
  abstract      = {Perception systems for outdoor robotics have to deal with varying environmental conditions. Variations in illumination in particular, are currently the biggest challenge for vision-based perception. In this paper we present an approach for radiometric characterisation of multispectral cameras. To enable spatio-temporal mapping we also present a procedure for in-situ illumination estimation, resulting in radiometric calibration of the collected images. In contrast to current approaches, we present a purely data driven, parameter free approach, based on maximum likelihood estimation which can be performed entirely on the field, without requiring specialised laboratory equipment. Our routine requires three simple datasets which are easily acquired using most modern multispectral cameras. We evaluate the framework with a cost-effective snapshot multispectral camera. The results show that our method enables the creation of quatitatively accurate relative reflectance images with challenging on field calibration datasets under a variety of ambient conditions.},
}

@InProceedings{saxena2017icra,
  author        = {D.M. Saxena and V. Kurtz and M. Hebert},
  title         = {{Learning Robust Failure Response for Autonomous Vision Based Flight}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Failure Detection and Recovery, Visual-Based Navigation, Aerial Robotics},
  abstract      = {The ability of autonomous mobile robots to react to and recover from potential failures of on-board systems is an important area of ongoing robotics research. With increasing emphasis on robust systems and long-term autonomy, mobile robots must be able to respond safely and intelligently to dangerous situations. Recent developments in computer vision have made autonomous vision based navigation possible. However, vision systems are known to be imperfect and prone to failure due to variable lighting, terrain changes, and other environmental variables. We describe a system for learning simple failure recovery maneuvers based on experience. This involves both recognizing when the vision system is prone to failure, and associating failures with appropriate responses that will most likely help the robot recover. We implement this system on an autonomous quadrotor and demonstrate that behaviors learned with our system are effective in recovering from situational perception failure, thereby improving reliability in cluttered and uncertain environments.},
}

@InProceedings{morere2017icra,
  author        = {P. Morere and R. Marchant and F. Ramos},
  title         = {{Sequential Bayesian Optimisation as a POMDP for Environment Monitoring with UAVs}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {AI Reasoning Methods, Learning and Adaptive Systems, Motion and Path Planning},
  abstract      = {Bayesian Optimisation has gained much popularity lately, as a global optimisation technique for functions that are expensive to evaluate or unknown a priori. While classical BO focuses on where to gather an observation next, it does not take into account practical constraints for a robotic system such as where it is physically possible to gather samples from, nor the sequential nature of the problem while executing a trajectory. In field robotics and other real-life situations, physical and trajectory constraints are inherent problems. This paper addresses these issues by formulating Bayesian Optimisation for continuous trajectories within a Partially Observable Markov Decision Process (POMDP) framework. The resulting POMDP is solved using Monte-Carlo Tree Search (MCTS), which we adapt to using a reward function balancing exploration and exploitation. Experiments on monitoring a spatial phenomenon with a UAV illustrate how our BO-POMDP algorithm outperforms competing techniques.},
}

@InProceedings{mueller-sim2017icra,
  author        = {T. Mueller-Sim and M. Jenkins and J. Abel and G. Kantor},
  title         = {{The Robotanist: A Ground-Based Agricultural Robot for High-Throughput Crop Phenotyping}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Robotics in Agriculture and Forestry, Agricultural Automation, Field Robots},
  abstract      = {The established processes for measuring physiological and morphological traits (phenotypes) of crops in outdoor test plots are labor intensive and error-prone. Low-cost, reliable, field-based robotic phenotyping will enable geneticists to more easily map genotypes to phenotypes, which in turn will improve crop yields. In this paper, we present a novel robotic ground-based platform capable of autonomously navigating below the canopy of row crops such as sorghum or corn. The robot is also capable of deploying a manipulator to measure plant stalk strength and gathering phenotypic data with a modular array of non-contact sensors. We present data obtained from deployments to Sorghum bicolor test plots at various sites in South Carolina, USA.},
}

@InProceedings{platinsky2017icra,
  author        = {L. Platinsky and A.J. Davison and S. Leutenegger},
  title         = {{Monocular Visual Odometry: Sparse Joint Optimisation or Dense Alternation?}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {SLAM, Visual-Based Navigation},
  abstract      = {Real-time monocular SLAM is increasingly mature and entering commercial products. However, there is a divide between two techniques providing similar performance. Despite the rise of dense and semi-dense methods which use large proportions of the pixels in a video stream to estimate motion and structure via alternating estimation, they have not eradicated feature-based methods which use a significantly smaller amount of image information from keypoints and retain a more rigorous joint estimation framework. Dense methods provide more complete scene information, but in this paper we focus on how the amount of information and different optimisation methods affect the accuracy of local motion estimation (monocular visual odometry). This topic becomes particularly relevant after the recent results from a direct sparse system. We propose a new method for fairly comparing the accuracy of SLAM frontends in a common setting. We suggest computational cost models for an overall comparison which indicates that there is relative parity between the approaches at the settings allowed by current serial processors when evaluated under equal conditions.},
}

@InProceedings{osep2017icra,
  author        = {A. Osep and W. Mehner and M. Mathias and B. Leibe},
  title         = {{Combined Image and World-Space Tracking in Traffic Scenes}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Visual Tracking, Computer Vision for Automation, Computer Vision for Transportation},
  abstract      = {Tracking in urban street scenes plays a central role in autonomous systems such as self-driving cars. Most of the current vision-based tracking methods perform tracking in the image domain. Other approaches, e.g. based on LIDAR and radar, track purely in 3D. While some vision-based tracking methods invoke 3D information in parts of their pipeline, and some 3D-based methods utilize image-based information in components of their approach, we propose to use image- and world-space information jointly throughout our method. We present our tracking pipeline as a 3D extension of image-based tracking. From enhancing the detections with 3D measurements to the reported positions of every tracked object, we use worldspace 3D information at every stage of processing. We accomplish this by our novel coupled 2D-3D Kalman filter, combined with a conceptually clean and extendable hypothesize-andselect framework. Our approach matches the current stateof-the-art on the official KITTI benchmark, which performs evaluation in the 2D image domain only. Further experiments show significant improvements in 3D localization precision by enabling our coupled 2D-3D tracking.},
}

@InProceedings{raposo2017icra,
  author        = {C. Raposo and J.P. Barreto},
  title         = {Using 2 Point+Normal Sets for Fast Registration of Point Clouds with Small Overlap},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Computational Geometry},
  abstract      = {Global 3D point cloud registration has been solved by finding putative matches between the point clouds for establishing alignment hypotheses. A naive approach would try to perform exhaustive search of triplets with a cubic runtime complexity in the number of data points. Super4PCS reduces this complexity to linear by making use of sets of 4 coplanar points. This paper proposes 2-Point-Normal Sets (2PNS), a new global 3D registration approach that advances Super4PCS by using 2 points and their normals for generating alignment hypotheses. The dramatic improvement in the complexity of 2PNS when compared to Super4PCS is demonstrated by the experiments that show speed-ups of two orders of magnitude in noise-free datasets and up to 5.2 in Kinect scans, while improving robustness and alignment accuracy, even in datasets with overlaps as low as 5\%.},
}

@InProceedings{doherty2017icra,
  author        = {K. Doherty and J. Wang and B. Englot},
  title         = {{Bayesian Generalized Kernel Inference for Occupancy Map Prediction}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Mapping, Range Sensing, Learning and Adaptive Systems},
  abstract      = {We consider the problem of building accurate and descriptive 3D occupancy maps of an environment from sparse and noisy range sensor data. We seek to accomplish this task by constructing a predictive model online and inferring the occupancy probability of regions we have not directly observed. We propose a novel algorithm leveraging recent advances in data structures for mapping, sparse kernels, and Bayesian nonparametric inference. The resulting inference model has several desirable properties in comparison to existing methods, including speed of computation, the ability to be recursively updated without approximation, and consistency between batch and online inference. The method also reverts to the use of a specified prior state when insufficient relevant training data exist to predict the occupancy probability of a query point, a property which is attractive for motion planning and exploration applications with mobile robots.},
}

@InProceedings{williams2017icra,
  author        = {G. Williams and N. Wagener and B. Goldfain and P. Drews and J. Rehg and B. Boots and E. Theodorou},
  title         = {{Information Theoretic MPC for Model-Based Reinforcement Learning}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Learning and Adaptive Systems, Optimization and Optimal Control, Autonomous Vehicle Navigation},
  abstract      = {We introduce an information theoretic model predictive control (MPC) algorithm capable of handling complex cost criteria and general nonlinear dynamics. The generality of the approach makes it possible to use multi-layer neural networks as dynamics models, which we incorporate into our MPC algorithm in order to solve model-based reinforcement learning tasks. We test the algorithm in simulation on a cartpole swing up and quadrotor navigation task, as well as on actual hardware in an aggressive driving task. Empirical results demonstrate that the algorithm is capable of achieving a high level of performance and does so only utilizing data collected from the system. Fig. 1. Aggressive driving with model predictive path integral control and neural network dynamics.},
}

@InProceedings{garimella2017icra,
  author        = {G. Garimella and M. Sheckells and M. Kobilarov},
  title         = {{Robust Obstacle Avoidance for Aerial Platforms Using Adaptive Model Predictive Control}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Robust/Adaptive Control of Robotic Systems, Aerial Robotics, Learning and Adaptive Systems},
  abstract      = {This work addresses the problem of motion planning among obstacles for quadrotor platforms under external disturbances and with model uncertainty. A novel Nonlinear Model Predictive Control (NMPC) optimization technique is proposed which incorporates specified uncertainties into the planned trajectories. At the core of the procedure lies the propagation of model parameter uncertainty and initial state uncertainty as high-confidence ellipsoids in pose space. The quadrotor trajectories are then computed to avoid obstacles by a required safety margin, expressed as ellipsoid penetration while minimizing control effort and achieving a user-specified goal location. Combining this technique with online model identification results in robust obstacle avoidance behavior. Experiments in outdoor scenarios with virtual obstacles show that the quadrotor can avoid obstacles robustly, even under the influence of external disturbances.},
}

@InProceedings{bowman2017icra,
  author        = {S. Bowman and N. Atanasov and K. Daniilidis and G.J. Pappas},
  title         = {{Probabilistic Data Association for Semantic SLAM}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {SLAM, Localization, Recognition},
  abstract      = {Traditional approaches to simultaneous localization and mapping (SLAM) rely on low-level geometric features such as points, lines, and planes. They are unable to assign semantic labels to landmarks observed in the environment. Furthermore, loop closure recognition based on low-level features is often viewpoint-dependent and subject to failure in ambiguous or repetitive environments. On the other hand, object recognition methods can infer landmark classes and scales, resulting in a small set of easily recognizable landmarks, ideal for view-independent unambiguous loop closure. In a map with several objects of the same class, however, a crucial data association problem exists. While data association and recognition are discrete problems usually solved using discrete inference, classical SLAM is a continuous optimization over metric information. In this paper, we formulate an optimization problem over sensor states and semantic landmark positions that integrates metric information, semantic information, and data associations, and decompose it into two interconnected problems: an estimation of discrete data association and landmark class probabilities, and a continuous optimization over the metric states. The estimated landmark and robot poses affect the association and class distributions, which in turn affect the robot-landmark pose optimization. The performance of our algorithm is demonstrated on indoor and outdoor datasets.},
}

@InProceedings{mccormac2017icra,
  author        = {J. McCormac and A. Handa and A.J. Davison and S. Leutenegger},
  title         = {{SemanticFusion: Dense 3D Semantic Mapping with Convolutional Neural Networks}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Semantic Scene Understanding, SLAM, Object detection, Segmentation, Categorization, CNN},
  abstract      = {Ever more robust, accurate and detailed mapping using visual sensing has proven to be an enabling factor for mobile robots across a wide variety of applications. For the next level of robot intelligence and intuitive user interaction, maps need to extend beyond geometry and appearance they need to contain semantics. We address this challenge by combining Convolutional Neural Networks (CNNs) and a state-of-the-art dense Simultaneous Localisation and Mapping (SLAM) system, ElasticFusion, which provides long-term dense correspondences between frames of indoor RGB-D video even during loopy scanning trajectories. These correspondences allow the CNNs semantic predictions from multiple view points to be probabilistically fused into a map. This not only produces a useful semantic 3D map, but we also show on the NYUv2 dataset that fusing multiple predictions leads to an improvement even in the 2D semantic labelling over baseline single frame predictions. We also show that for a smaller reconstruction dataset with larger variation in prediction viewpoint, the improvement over single frame segmentation increases. Our system is efficient enough to allow real-time interactive use at frame-rates of 25Hz.},
}

@InProceedings{zhou2017icra,
  author        = {Y. Zhou and K. Hauser},
  title         = {{Incorporating Side-Channel Information into Convolutional Neural Networks for Robotic Tasks}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Learning and Adaptive Systems, Collision Avoidance, Robust/Adaptive Control of Robotic Systems, CNN},
  abstract      = {Convolutional neural networks (CNN) are a deep learning technique that has achieved state-of-the-art prediction performance in computer vision and robotics, but assume the input data can be formatted as an image or video (e.g. predicting a robot grasping location given RGB-D image input). This paper considers the problem of augmenting a traditional CNN for handling image-like input (called main-channel input) with additional, highly predictive, non-image-like input (called side-channel input). An example of such a task would be to predict whether a robot path is collision-free given an occupancy grid of the environment and the paths start and goal configurations; the occupancy grid is the main-channel and the start and goal are the side-channel. This paper presents several candidate network architectures for doing so. Empirical tests on robot collision prediction and control problems compare the the proposed architectures in terms of learning speed, memory usage, learning capacity, and susceptibility to overfitting.},
}

@InProceedings{briales2017icra,
  author        = {J. Briales and J. Gonzalez},
  title         = {{Initialization of 3D Pose Graph Optimization Using Lagrangian Duality}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {SLAM, Optimization and Optimal Control, Localization},
  abstract      = {Pose Graph Optimization (PGO) is the de facto choice to solve the trajectory of an agent in Simultaneous Localization and Mapping (SLAM). The Maximum Likelihood Estimation (MLE) for PGO is a non-convex problem for which no known technique is able to guarantee a globally optimal solution under general conditions. In recent years, Lagrangian duality has proved suitable to provide good, frequently tight relaxations of the hard PGO problem through convex Semidefinite Programming (SDP). In this work, we build from the state-ofthe-art Lagrangian relaxation [1] and contribute a complete recovery procedure that, given the (tractable) optimal solution of the relaxation, provides either the optimal MLE solution if the relaxation is tight, or a remarkably good feasible guess if the relaxation is non-tight, which occurs in specially challenging PGO problems (very noisy observations, low graph connectivity, etc.). In the latter case, when used for initialization of local iterative methods, our approach outperforms other state-ofthe-art approaches converging to better solutions. We support our claims with extensive experiments.},
}

@InProceedings{zhen2017icra,
  author        = {W. Zhen and S. Zeng and S. Scherer},
  title         = {{Robust Localization and Localizability Estimation with a Rotating Laser Scanner}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Localization, Aerial Robotics, Range Sensing},
  abstract      = {This paper presents a robust localization approach that fuses measurements from inertial measurement unit (IMU) and a rotating laser scanner. An Error State Kalman Filter (ESKF) is used for sensor fusion and is combined with a Gaussian Particle Filter (GPF) for measurements update. We experimentally demonstrated the robustness of this implementation in various challenging situations such as kidnapped robot situation, laser range reduction and various environment scales and characteristics. Additionally, we propose a new method to evaluate localizability of a given 3D map and show that the computed localizability can precisely predict localization errors, thus helps to find safe routes during flight.},
}

@InProceedings{lee2017icra,
  author        = {S. Lee and S. Seo},
  title         = {{A Learning-Based Framework for Handling Dilemmas in Urban Automated Driving}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Autonomous Vehicle Navigation, Motion and Path Planning, Intelligent Transportation Systems},
  abstract      = {Over the last decade, automated vehicles have been widely researched and their massive potential has been verified through several milestone demonstrations. However, there are still many challenges ahead. One of the biggest challenges is integrating them into urban environments in which dilemmas occur frequently. Conventional automated driving strategies make automated vehicles foolish in dilemmas such as making lane-changes in heavy traffic, handling a yellow traffic light, and crossing a double-yellow line to pass an illegally parked car. In this paper, we introduce a novel automated driving strategy that allows automated vehicles to tackle these dilemmas. The key insight behind our automated driving strategy is that expert drivers understand human interactions on the road and comply with mutually-accepted rules, which are learned from countless experiences. In order to teach the driving strategy of expert drivers to automated vehicles, we propose a general learning framework based on maximum entropy inverse reinforcement learning and Gaussian process. Experiments are conducted on a 5.2 km-long campus road at Seoul National University and demonstrate that our framework performs comparably to expert drivers in planning trajectories to handle various dilemmas.},
}

@InProceedings{papachristos2017icra,
  author        = {C. Papachristos and S. Khattak and K. Alexis},
  title         = {{Uncertainty-aware Receding Horizon Exploration and Mapping Using Aerial Robots}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Aerial Robotics},
  abstract      = {This paper presents a novel path planning algorithm for autonomous, uncertaintyaware exploration and mapping of unknown environments using aerial robots. The proposed planner follows a twostep, receding horizon, belief spacebased approach. At first, in an online computed tree the algorithm finds the branch that optimizes the amount of space expected to be explored. The first viewpoint configuration of this branch is selected, but the path towards it is decided through a second planning step. Within that, a new tree is sampled, admissible branches arriving at the reference viewpoint are found and the robot belief about its state and the tracked landmarks of the environment is propagated. The branch that minimizes the expected localization and mapping uncertainty is selected, the corresponding path is executed by the robot and the whole process is iteratively repeated. The proposed planner is capable of running online onboard a small aerial robot and its performance is evaluated using experimental studies in a challenging environment.},
}

@InProceedings{kim2017icra-silv,
  author        = {D. Kim and M. Walter},
  title         = {{Satellite Image-Based Localization Via Learned Embeddings}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Visual-Based Navigation, Localization},
  abstract      = {We propose a vision-based method that localizes a ground vehicle using publicly available satellite imagery as the only prior knowledge of the environment. Our approach takes as input a sequence of ground-level images acquired by the vehicle as it navigates, and outputs an estimate of the vehicles pose relative to a georeferenced satellite image. We overcome the significant viewpoint and appearance variations between the images through a neural multi-view model that learns locationdiscriminative embeddings in which ground-level images are matched with their corresponding satellite view of the scene. We use this learned function as an observation model in a filtering framework to maintain a distribution over the vehicles pose. We evaluate our method on different benchmark datasets and demonstrate its ability localize ground-level images in environments novel relative to training, despite the challenges of significant viewpoint and appearance variations.},
}

@InProceedings{arora2017icra,
  author        = {S. Arora and S. Scherer},
  title         = {{Randomized Algorithm for Informative Path Planning with Budget Constraints}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Motion and Path Planning, Aerial Robotics, Surveillance Systems},
  abstract      = {Maximizing information gathered within a budget is a relevant problem for information gathering tasks for robots with cost or operating time constraints. This problem is also known as the informative path planning (IPP) problem or correlated orienteering. It can be formalized as that of finding budgeted routes in a graph such that the reward collected by the route is maximized, where the reward at nodes can be dependent. Unfortunately, the problem is NP-Hard and the state of the art methods are too slow to even present an approximate solution online. Here we present Randomized Anytime Orienteering (RAOr) algorithm that provides near optimal solutions while demonstrably converging to an efficient solution in runtimes that allows the solver to be run online. The key idea of our approach is to pose orienteering as a combination of a Constraint Satisfaction Problem and a Traveling Salesman Problem. This formulation allows us to restrict the search space to routes that incur minimum distance to visit a set of selected nodes, and rapidly search this space using random sampling. The paper provides the analysis of asymptotic nearoptimality, convergence rates for RAOr algorithms, and present strategies to improve anytime performance of the algorithm. Our experimental results suggest an improvement by an order of magnitude over the state of the art methods in relevant simulation and in real world scenarios.},
}

@InProceedings{ushani2017icra,
  author        = {A. Ushani and R. Wolcott and J. Walls and R. Eustice},
  title         = {{A Learning Approach for Real-Time Temporal Scene Flow Estimation from LIDAR Data}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Field Robots},
  abstract      = {Many autonomous systems require the ability to perceive and understand motion in a dynamic environment. We present a novel algorithm that estimates this motion from raw LIDAR data in real-time without the need for segmentation or model-based tracking. The sensor data is first used to construct an occupancy grid. The foreground is then extracted via a learned background filter. Using the filtered occupancy grid, raw scene flow between successive scans is computed. Finally, we incorporate these measurements in a filtering framework to estimate temporal scene flow. We evaluate our method on the KITTI dataset.},
}

@InProceedings{desaraju2017icra,
  author        = {V. Desaraju and N. Michael},
  title         = {{Leveraging Experience for Computationally Efficient Adaptive Nonlinear Model Predictive Control}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Robust/Adaptive Control of Robotic Systems, Learning and Adaptive Systems, Optimization and Optimal Control},
  abstract      = {This work presents Experience-driven Predictive Control (EPC) as a fast technique for solving nonlinear model predictive control (NMPC) problems with uncertain system dynamics. EPC leverages an affine dynamics model that is updated online via Locally Weighted Projection Regression (LWPR) to capture nonlinearities, uncertainty, and changes in the system dynamics. This model enables the NMPC problem to be re-cast as a quadratic program (QP). The QP can then be solved via multi-parametric techniques to generate a mapping from state, reference, and dynamics model to a locally optimal, affine feedback control law. These mappings, in conjunction with the basis functions learned via LWPR, define a notion of experience for the controller as they capture the full inputoutput relationship for previous actions the controller has taken. The resulting experience database allows EPC to avoid solving redundant optimization problems, and as it is constructed online, enables the system to operate more efficiently over time. We demonstrate the performance of EPC through a set of hardware-in-the-loop simulation studies of a quadrotor micro air vehicle that is subjected to unmodeled exogenous perturbations.},
}

@InProceedings{zheng2017icra,
  author        = {X. Zheng and Z. Moratto and M. Li and A. Mourikis},
  title         = {{Photometric Patch-Based Visual-Inertial Odometry}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Sensor Fusion, Localization, SLAM},
  abstract      = {In this paper we present a novel direct visualinertial odometry algorithm, for estimating motion in unknown environments. The algorithm utilizes image patches extracted around image features, and formulates measurement residuals in the image intensity space directly. One key characteristic of the proposed method is that it models the true irradiance at each pixel as a random variable to be estimated and marginalized out. The formulation of the photometric residual explicitly accounts for the camera response function and lens vignetting (which can be calibrated in advance), as well as unknown illumination gains and biases, which are estimated on a per-feature or per-image basis. We present a detailed evaluation of our algorithm on 50 datasets with high-precision ground truth, which amount to approximately 1.5 hours of localization data. Through a direct comparison with a pointfeature based method, we demonstrate that the use of photometric residuals results in increased pose estimation accuracy, with approximately 23\% lower estimation errors, on average.},
}

@InProceedings{ma2017icra-ipao,
  author        = {K. Ma and L. Liu and G. Sukhatme},
  title         = {{Informative Planning and Online Learning with Sparse Gaussian Processes}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Marine Robotics, Motion and Path Planning, Learning and Adaptive Systems},
  abstract      = {A big challenge in environmental monitoring is the spatiotemporal variation of the phenomena to be observed. To enable persistent sensing and estimation in such a setting, it is beneficial to have a time-varying underlying environmental model. Here we present a planning and learning method that enables an autonomous marine vehicle to perform persistent ocean monitoring tasks by learning and refining an environmental model. To alleviate the computational bottleneck caused by large-scale data accumulated, we propose a framework that iterates between a planning component aimed at collecting the most information-rich data, and a sparse Gaussian Process learning component where the environmental model and hyperparameters are learned online by taking advantage of only a subset of data that provides the greatest contribution. Our simulations with ground-truth ocean data shows that the proposed method is both accurate and efficient.},
}

@InProceedings{fourie2017icra,
  author        = {D. Fourie and S.D. Claassens and S. Pillai and R. Mata and J. Leonard},
  title         = {{SLAMinDB: Centralized Graph Databases for Mobile Robotics}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Autonomous Vehicle Navigation, SLAM, Distributed Robot Systems},
  abstract      = {Robotic systems typically require memory recall mechanisms for a variety of tasks including localization, mapping, planning, visualization etc. We argue for a novel memory recall framework that enables more complex inference schemas by separating the computation from its associated data. In this work we propose a shared, centralized data persistence layer that maintains an ensemble of online, situationally-aware robot states. This is realized through a queryable graph-database with an accompanying key-value store for larger data. In turn, this approach is scalable and enables a multitude of capabilities such as experience-based learning and long-term autonomy. Using multi-modal simultaneous localization and mapping and a few example use-cases, we demonstrate the versatility and extensible nature that centralized persistence and SLAMinDB can provide. In order to support the notion of life-long autonomy, we envision robots to be endowed with such a persistence model, enabling them to revisit previous experiences and improve upon their existing task-specific capabilities.},
}

@InProceedings{wu2017icra,
  author        = {K. WU and X. Li and R. Ranasinghe and G. Dissanayake and Y. Liu},
  title         = {{RISAS: A Novel Rotation, Illumination, Scale Invariant Appearance and Shape Feature}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {RGB-D Perception},
  abstract      = {This paper presents a novel appearance and shape feature, RISAS, which is robust to viewpoint, illumination, scale and rotation variations. RISAS consists of a keypoint detector and a feature descriptor both of which utilise texture and geometric information present in the appearance and shape channels. A novel response function based on the surface normals is used in combination with the Harris corner detector for selecting keypoints in the scene. A strategy that uses the depth information for scale estimation and background elimination is proposed to select the neighbourhood around the keypoints in order to build precise invariant descriptors. Proposed descriptor relies on the ordering of both grayscale intensity and shape information in the neighbourhood. Comprehensive experiments which confirm the effectiveness of the proposed RGB-D feature when compared with CSHOT [1] and LOIND[2] are presented. Furthermore, we highlight the utility of incorporating texture and shape information in the design of both the detector and the descriptor by demonstrating the enhanced performance of CSHOT and LOIND when combined with RISAS detector.},
}

@InProceedings{kim2017icra-rrrd,
  author        = {J. Kim and Y. Latif and I. Reid},
  title         = {{RRD-SLAM: Radial-Distorted Rolling-Shutter Direct SLAM}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {SLAM},
  abstract      = {In this paper, we present a monocular direct semi-dense SLAM (Simultaneous Localization And Mapping) method that can handle both radial distortion and rollingshutter distortion. Such distortions are common in, but not restricted to, situations when an inexpensive wide-angle lens and a CMOS sensor are used, and leads to significant inaccuracy in the map and trajectory estimates if not modeled correctly. The apparent naive solution of simply undistorting the images using pre-calibrated parameters does not apply to this case since rows in the undistorted image are no longer captured at the same time. To address this we develop an algorithm that incorporates radial distortion into an existing state-of-theart direct semi-dense SLAM system that takes rolling-shutters into account. We propose a method for finding the generalized epipolar curve for each rolling-shutter radially distorted image. Our experiments demonstrate the efficacy of our approach and compare it favorably with the state-of-the-art in direct semidense rolling-shutter SLAM.},
}

@InProceedings{dutoit2017icra,
  author        = {R. DuToit and J.A. Hesch and E. Nerurkar and S. Roumeliotis},
  title         = {{Consistent Map-Based 3D Localization on Mobile Devices}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Localization},
  abstract      = {In this paper, we seek to provide consistent, realtime 3D localization capabilities to mobile devices navigating within previously mapped areas. To this end, we introduce the Cholesky-Schmidt-Kalman filter (C-SKF), which explicitly considers the uncertainty of the prior map, by employing the sparse Cholesky factor of the maps Hessian, instead of its dense covarianceas is the case for the Schmidt-Kalman filter. By doing so, the C-SKF has memory requirements typically linear in the size of the map, as opposed to quadratic for storing the maps covariance. Moreover, and in order to bound the processing needs of the C-SKF (between linear and quadratic in the size of the map), we introduce two relaxations of the CSKF algorithm: (i) The sC-SKF, which operates on the Cholesky factors of independent sub-maps resulting from dividing the map into overlapping segments. (ii) We formulate an efficient method for sparsifying the Cholesky factor by selecting and processing a subset of loop-closure measurements based on their temporal distribution. Lastly, we assess the processing and memory requirements of the proposed algorithms, and compare their positioning accuracy against other inconsistent mapbased localization approaches that employ measurement-noisecovariance inflation to compensate for the maps uncertainty.},
}

@InProceedings{ammirato2017icra,
  author        = {P. Ammirato and P. Poirson and E. Park and J. Kosecka and A. Berg},
  title         = {{A Dataset for Developing and Benchmarking Active Vision}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Object detection, Segmentation, Categorization, Computer Vision for Other Robotic Applications, Recognition},
  abstract      = {We present a new public dataset with a focus on simulating robotic vision tasks in everyday indoor environments using real imagery. The dataset includes 20,000+ RGB-D images and 50,000+ 2D bounding boxes of object instances densely captured in 9 unique scenes. We train a fast object category detector for instance detection on our data. Using the dataset we show that, although increasingly accurate and fast, the state of the art for object detection is still severely impacted by object scale, occlusion, and viewing direction all of which matter for robotics applications. We next validate the dataset for simulating active vision, and use the dataset to develop and evaluate a deep-network-based system for next best move prediction for object classification using reinforcement learning. Our dataset is available for download at cs.unc. edu/ammirato/active_vision_dataset_website/.},
}

@InProceedings{valada2017icra,
  author        = {A. Valada and J. Vertens and A. Dhall and W. Burgard},
  title         = {{AdapNet: Adaptive Semantic Segmentation in Adverse Environmental Conditions}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Semantic Scene Understanding, Learning and Adaptive Systems, Object detection, Segmentation, Categorization},
  abstract      = {Robust scene understanding of outdoor environments using passive optical sensors is a onerous and essential task for autonomous navigation. The problem is heavily characterized by changing environmental conditions throughout the day and across seasons. Robots should be equipped with models that are impervious to these factors in order to be operable and more importantly to ensure safety in the real-world. In this paper, we propose a novel semantic segmentation architecture and the convoluted mixture of deep experts (CMoDE) fusion technique that enables a multi-stream deep neural network to learn features from complementary modalities and spectra, each of which are specialized in a subset of the input space. Our model adaptively weighs class-specific features of expert networks based on the scene condition and further learns fused representations to yield robust segmentation. We present results from experimentation on three publicly available datasets that contain diverse conditions including rain, summer, winter, dusk, fall, night and sunset, and show that our approach exceeds the state-of-the-art. In addition, we evaluate the performance of autonomously traversing several kilometres of a forested environment using only the segmentation for perception.},
}

@InProceedings{pavlakos2017icra,
  author        = {G. Pavlakos and X. Zhou and A. Chan and K. Derpanis and K. Daniilidis},
  title         = {{6-DoF Object Pose from Semantic Keypoints}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Visual Tracking, Computer Vision for Automation, Computer Vision for Other Robotic Applications},
  abstract      = {This paper presents a novel approach to estimating the continuous six degree of freedom (6-DoF) pose (3D translation and rotation) of an object from a single RGB image. The approach combines semantic keypoints predicted by a convolutional network (convnet) with a deformable shape model. Unlike prior work, we are agnostic to whether the object is textured or textureless, as the convnet learns the optimal representation from the available training image data. Furthermore, the approach can be applied to instance- and class-based pose recovery. Empirically, we show that the proposed approach can accurately recover the 6-DoF object pose for both instanceand class-based scenarios with a cluttered background. For class-based object pose estimation, state-of-the-art accuracy is shown on the large-scale PASCAL3D+ dataset.},
}

@InProceedings{mehta2017icra,
  author        = {D. Mehta and G. Ferrer and E. Olson},
  title         = {{Fast Discovery of Influential Outcomes for Risk-Aware MPDM}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Motion and Path Planning, Intelligent Transportation Systems, Field Robots},
  abstract      = {In the Multi-Policy Decision Making (MPDM) framework, a robots policy is elected by sampling from the distribution of current states, predicting future outcomes through forward simulation, and selecting the policy with the best expected performance. Electing the best plan depends on sampling initial conditions with influential (very high costs) outcomes. Discovering these configurations through random sampling may require drawing many samples, which becomes a performance bottleneck. In this paper, we describe a risk-aware approach which augments this sampling with an optimization process that helps discover those influential outcomes. We describe how we overcome several practical difficulties with this approach, and demonstrate significant performance improvements on a real robot platform navigating a semi-crowded, highly dynamic environment.},
}

@InProceedings{huang2017icra,
  author        = {E. Huang and M. Mukadam and Z. Liu and B. Boots},
  title         = {{Motion Planning with Graph-Based Trajectories and Gaussian Process Inference}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Motion and Path Planning},
  abstract      = {Motion planning as trajectory optimization requires generating trajectories that minimize a desired objective function or performance metric. Finding a globally optimal solution is often intractable in practice: despite the existence of fast motion planning algorithms, most are prone to local minima, which may require re-solving the problem multiple times with different initializations. In this work we provide a novel motion planning algorithm, GPMP-GRAPH, that considers a graph-based initialization that simultaneously explores multiple homotopy classes, helping to contend with the local minima problem. Drawing on previous work to represent continuoustime trajectories as samples from a Gaussian process (GP) and formulating the motion planning problem as inference on a factor graph, we construct a graph of interconnected states such that each path through the graph is a valid trajectory and efficient inference can be performed on the collective factor graph. We perform a variety of benchmarks and show that our approach allows the evaluation of an exponential number of trajectories within a fraction of the computational time required to evaluate them one at a time, yielding a more thorough exploration of the solution space and a higher success rate.},
}

@InProceedings{paul2017icra,
  author        = {M.K. Paul and K. Wu and J.A. Hesch and E. Nerurkar and S. Roumeliotis},
  title         = {{A Comparative Analysis of Tightly-Coupled Monocular, Binocular, and Stereo VINS}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Localization, SLAM, Visual-Based Navigation},
  abstract      = {In this paper, a sliding-window two-camera visionaided inertial navigation system (VINS) is presented in the square-root inverse domain. The performance of the system is assessed for the cases where feature matches across the two-camera images are processed with or without any stereo constraints (i.e., stereo vs. binocular). To support the comparison results, a theoretical analysis on the information gain when transitioning from binocular to stereo is also presented. Additionally, the advantage of using a two-camera (both stereo and binocular) system over a monocular VINS is assessed. Furthermore, the impact on the achieved accuracy of different image-processing frontends and estimator design choices is quantified. Finally, a thorough evaluation of the algorithms processing requirements, which runs in real-time on a mobile processor, as well as its achieved accuracy as compared to alternative approaches is provided, for various scenes and motion profiles.},
}

@InProceedings{wang2017icra-sctl,
  author        = {W. Wang and N. Wang and X. Wu and S. You and U. Neumann},
  title         = {{Self-Paced Cross-Modality Transfer Learning for Efficient Road Segmentation}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Object detection, Segmentation, Categorization, Computer Vision for Transportation, Computer Vision for Other Robotic Applications, CNN},
  abstract      = {Accurate road segmentation is a prerequisite for autonomous driving. Current state-of-the-art methods are mostly based on convolutional neural networks (CNNs). Nevertheless, their good performance is at expense of abundant annotated data and high computational cost. In this work, we address these two issues by a self-paced cross-modality transfer learning framework with efficient projection CNN. To be specific, with the help of stereo images, we first tackle a relevant but easier task, i.e. free-space detection with well developed unsupervised methods. Then, we transfer these useful but noisy knowledge in depth modality to single RGB modality with self-paced CNN learning. Finally, we only need to finetune the CNN with a few annotated images to get good performance. In addition, we propose an efficient projection CNN, which can improve the fine-grained segmentation results with little additional cost. At last, we test our method on KITTI road benchmark. Our proposed method surpasses all published methods at a speed of 15fps.},
}

@InProceedings{runz2017icra,
  author        = {M. Runz and L. Agapito},
  title         = {{Co-Fusion: Real-Time Segmentation, Tracking and Fusion of Multiple Objects}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Visual Tracking, Sensor Fusion},
  abstract      = {In this paper we introduce Co-Fusion, a dense SLAM system that takes a live stream of RGB-D images as input and segments the scene into different objects (using either motion or semantic cues) while simultaneously tracking and reconstructing their 3D shape in real time. We use a multiple model fitting approach where each object can move independently from the background and still be effectively tracked and its shape fused over time using only the information from pixels associated with that object label. Previous attempts to deal with dynamic scenes have typically considered moving regions as outliers, and consequently do not model their shape or track their motion over time. In contrast, we enable the robot to maintain 3D models for each of the segmented objects and to improve them over time through fusion. As a result, our system can enable a robot to maintain a scene description at the object level which has the potential to allow interactions with its working environment; even in the case of dynamic scenes.},
}

@InProceedings{byravan2017icra,
  author        = {A. Byravan and D. Fox},
  title         = {{SE3-Nets: Learning Rigid Body Motion Using Deep Neural Networks}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Learning and Adaptive Systems, RGB-D Perception, Object detection, Segmentation, Categorization},
  abstract      = {We introduce SE3-N ETS which are deep neural networks designed to model and learn rigid body motion from raw point cloud data. Based only on sequences of depth images along with action vectors and point wise data associations, SE3-N ETS learn to segment effected object parts and predict their motion resulting from the applied force. Rather than learning point wise flow vectors, SE3-N ETS predict SE(3) transformations for different parts of the scene. Using simulated depth data of a table top scene and a robot manipulator, we show that the structure underlying SE3-N ETS enables them to generate a far more consistent prediction of object motion than traditional flow based networks. Additional experiments with a depth camera observing a Baxter robot pushing objects on a table show that SE3-N ETS also work well on real data.},
}

@InProceedings{song2017icra,
  author        = {S. Song and S. Jo},
  title         = {{Online Inspection Path Planning for Autonomous 3D Modeling Using a Micro-Aerial Vehicle}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Motion and Path Planning, Aerial Robotics, Visual-Based Navigation},
  abstract      = {In this paper, we propose a novel algorithm for planning exploration paths to generate 3D models of unknown environments by using a micro-aerial vehicle (MAV). Our algorithm initially determines a next-best-view (NBV) that maximizes information gain and plans a collision-free path to reach the NBV. Along the path, the MAV explores the greatest unknown area although it sometimes misses minor unreconstructed region, such as a hole or a sparse surface. To cover such a region, we propose an online inspection algorithm that consistently provides an optimal coverage path toward the NBV in real time. The algorithm iteratively refines an inspection path according to the acquired information until the modeling of a specific local area is complete. We evaluated the proposed algorithm by comparing it with other state-of-the-art approaches through simulated experiments. The results show that our algorithm outperforms the other approaches in both exploration and 3D modeling scenarios.},
}

@InProceedings{delmerico2017icra,
  author        = {J. Delmerico and E. Mueggler and J. Nitsch and D. Scaramuzza},
  title         = {{Active Autonomous Aerial Exploration for Ground Robot Path Planning}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Search and Rescue Robots, Motion and Path Planning, Visual-Based Navigation},
  abstract      = {We address the problem of planning a path for a ground robot through unknown terrain, using observations from a flying robot. In search and rescue missions, which are our target scenarios, the time from arrival at the disaster site to the delivery of aid is critically important. Previous works required exhaustive exploration before path planning, which is time-consuming but eventually leads to an optimal path for the ground robot. Instead, we propose active exploration of the environment, where the flying robot chooses regions to map in a way that optimizes the overall response time of the system, which is the combined time for the air and ground robots to execute their missions. In our approach, we estimate terrain classes throughout our terrain map, and we also add elevation information in areas where the active exploration algorithm has chosen to perform 3D reconstruction. This terrain information is used to estimate feasible and efficient paths for the ground robot. By exploring the environment actively, we achieve superior response times compared to both exhaustive and greedy exploration strategies. We demonstrate the performance and capabilities of the proposed system in simulated and real-world outdoor experiments. To the best of our knowledge, this is the first work to address ground robot path planning using active aerial exploration.},
}

@InProceedings{dong2017icra-anmf,
  author        = {W. Dong and V. Isler},
  title         = {{A Novel Method for the Extrinsic Calibration of a 2-D Laser-Rangefinder and a Camera}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Calibration and Identification, Range Sensing, RGB-D Perception},
  abstract      = {We present a novel method for extrinsically calibrating a camera and a 2-D Laser Rangefinder (LRF) whose beams are invisible from the camera image. We show that pointto-plane constraints from a single observation of a V-shaped calibration pattern composed of two non-coplanar triangles suffice to uniquely constrain the relative pose between two sensors. Next, we present an approach to obtain solutions using point-to-plane constraints from single or multiple observations. Along the way, we also show that previous solutions, in contrast to our method, have inherent ambiguities and therefore must rely on a good initial estimate. Real and synthetic experiments validate our method and show that it achieves better accuracy than previous methods.},
}

@InProceedings{ambrus2017icra,
  author        = {R. Ambrus and S. Claici and A.J. Wendt},
  title         = {{Automatic Room Segmentation from Unstructured 3D Data of Indoor Environments}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Mapping, Semantic Scene Understanding, RGB-D Perception},
  abstract      = {We present an automatic approach for the task of reconstructing a 2D floor plan from unstructured point clouds of building interiors. Our approach emphasizes accurate and robust detection of building structural elements, and unlike previous approaches does not require prior knowledge of scanning device poses. The reconstruction task is formulated as a multiclass labeling problem that we approach using energy minimization. We use intuitive priors to define the costs for the energy minimization problem, and rely on accurate wall and opening detection algorithms to ensure robustness. We provide detailed experimental evaluation results, both qualitative and quantitative, against state of the art methods and labeled ground truth data.},
}

@InProceedings{jadidi2017icra,
  author        = {M.G. Jadidi and J.V. Miro and G. Dissanayake},
  title         = {{Warped Gaussian Processes Occupancy Mapping with Uncertain Inputs}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Mapping},
  abstract      = {In this paper, we study extensions to the Gaussian Processes (GPs) continuous occupancy mapping problem. There are two classes of occupancy mapping problems that we particularly investigate. The first problem is related to mapping under pose uncertainty and how to propagate pose estimation uncertainty into the map inference. We develop expected kernel and expected sub-map notions to deal with uncertain inputs. In the second problem, we account for the complication of the robots perception noise using Warped Gaussian Processes (WGPs). This approach allows for non-Gaussian noise in the observation space and captures the possible nonlinearity in that space better than standard GPs. The developed techniques can be applied separately or concurrently to a standard GP occupancy mapping problem. According to our experimental results, although taking into account pose uncertainty leads, as expected, to more uncertain maps, by modeling the nonlinearities present in the observation space WGPs can improve the map quality.},
}

@InProceedings{zhou2017icra-svof,
  author        = {Y. Zhou and L. Kneip and H. Li},
  title         = {{Semi-Dense Visual Odometry for RGB-D Cameras Using Approximate Nearest Neighbour Fields}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Localization, Mapping, SLAM},
  abstract      = {This paper presents a robust and efficient semidense visual odometry solution for RGB-D cameras. The core of our method is a 2D-3D ICP pipeline which estimates the pose of the sensor by registering the projection of a 3D semidense map of a reference frame with the 2D semi-dense region extracted in the current frame. The processing is speeded up by efficiently implemented approximate nearest neighbour fields under the Euclidean distance criterion, which permits the use of compact Gauss-Newton updates in the optimization. The registration is formulated as a maximum a posterior problem to deal with outliers and sensor noise, and the equivalent weighted least squares problem is consequently solved by iteratively reweighted least squares method. A variety of robust weight functions are tested and the optimum is determined based on the probabilistic characteristics of the sensor model. Extensive evaluation on publicly available RGB-D datasets shows that the proposed method predominantly outperforms existing state-ofthe-art methods.},
}

@InProceedings{mur-artal2017icra,
  author        = {R. Mur-Artal and J.D. Tardos},
  title         = {{Visual-Inertial Monocular SLAM with Map Reuse}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {SLAM, Sensor Fusion, Visual-Based Navigation},
  abstract      = {In recent years there have been excellent results in Visual-Inertial Odometry techniques, which aim to compute the incremental motion of the sensor with high accuracy and robustness. However these approaches lack the capability to close loops, and trajectory estimation accumulates drift even if the sensor is continually revisiting the same place. In this work we present a novel tightly-coupled Visual-Inertial Simultaneous Localization and Mapping system that is able to close loops and reuse its map to achieve zero-drift localization in already mapped areas. While our approach can be applied to any camera configuration, we address here the most general problem of a monocular camera, with its well-known scale ambiguity. We also propose a novel IMU initialization method, which computes the scale, the gravity direction, the velocity, and gyroscope and accelerometer biases, in a few seconds with high accuracy. We test our system in the 11 sequences of a recent micro-aerial vehicle public dataset achieving a typical scale factor error of 1\% and centimeter precision. We compare to the state-of-the-art in visual-inertial odometry in sequences with revisiting, proving the better accuracy of our method due to map reuse and no drift accumulation. Index Terms SLAM, Sensor Fusion, Visual-Based Navigation},
}

@InProceedings{sa2017icra,
  author        = {I. Sa and C. Lehnert and A. English and C.S. McCool and F. Dayoub and B. Upcroft and T. Perez},
  title         = {{Peduncle Detection of Sweet Pepper for Autonomous Crop Harvesting - Combined Colour and 3D Information}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Agricultural Automation, Robotics in Agriculture and Forestry, RGB-D Perception},
  abstract      = {This paper presents a 3D visual detection method for the challenging task of detecting peduncles of sweet peppers (Capsicum annuum) in the field. Cutting the peduncle cleanly is one of the most difficult stages of the harvesting process, where the peduncle is the part of the crop that attaches it to the main stem of the plant. Accurate peduncle detection in 3D space is therefore a vital step in reliable autonomous harvesting of sweet peppers, as this can lead to precise cutting while avoiding damage to the surrounding plant. This paper makes use of both colour and geometry information acquired from an RGB-D sensor and utilises a supervised-learning approach for the peduncle detection task. The performance of the proposed method is demonstrated and evaluated using qualitative and quantitative results (the Area-Under-the-Curve (AUC) of the detection precision-recall curve). We are able to achieve an AUC of 0.71 for peduncle detection on field-grown sweet peppers. We release a set of manually annotated 3D sweet pepper and peduncle images to assist the research community in performing further research on this topic.},
}

@InProceedings{daudelin2017icra,
  author        = {J.J. Daudelin and M. Campbell},
  title         = {{An Adaptable, Probabilistic, Next Best View Algorithm for Reconstruction of Unknown 3D Objects}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Autonomous Agents, Probability and Statistical Methods, Motion and Path Planning},
  abstract      = {Autonomous mobile robots perform many tasks, such as grasping and inspection, that may require complete models of 3D objects in the environment. If little or no knowledge about an object is known a priori, the robot must take sensor measurements from strategically determined viewpoints in order to reconstruct a 3D model of the object. We propose an autonomous object reconstruction approach for mobile robots that is very general, with no assumptions about object shape or size, such as a bounding box or predetermined set of candidate viewpoints. A probabilistic, volumetric method for determining the optimal next best view is developed based on a partial model of a 3D object of unknown shape and size. The proposed method integrates an object probability characteristic to determine sensor views that incrementally reconstruct a 3D model of the object. Experiments in simulation and on a real world robot validate the work and compare it to the state of the art.},
}

@InProceedings{lehnert2017icra,
  author        = {C. Lehnert and A. English and C.S. McCool and A.M.W. Tow and T. Perez},
  title         = {{Autonomous Sweet Pepper Harvesting for Protected Cropping Systems}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Agricultural Automation, Dexterous Manipulation, Mechanism Design of Manipulators},
  abstract      = {In this paper we present a new robotic harvester (Harvey) that can autonomously harvest sweet pepper in protected cropping environments. Our approach combines effective vision algorithms with a novel end-effector design to enable successful harvesting of sweet peppers. Initial field trials in protected cropping environments, with two cultivar, demonstrate the efficacy of this approach achieving a 46\% success rate for unmodified crop, and 58\% for modified crop. Furthermore, for the more favourable cultivar we were also able to detach 90\% of sweet peppers, indicating that improvements in the grasping success rate would result in greatly improved harvesting performance.},
}

@InProceedings{fleckenstein2017icra,
  author        = {F.V. Fleckenstein and C. Dornhege and W. Burgard},
  title         = {{Efficient Path Planning for Mobile Robots with Adjustable Wheel Positions}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Agricultural Automation, Motion and Path Planning},
  abstract      = {Efficient navigation planning for mobile robots in complex environments is a challenging problem. In this paper we consider the path planning problem for mobile robots with adjustable relative wheel positions, which further increase the navigation capabilities. In particular we account for changes of these relative wheel positions during planning time, thus fully leveraging the capabilities of the robot. Whereas these additional degrees of freedom increase flexibility, they introduce a more challenging planning problem. The approach proposed in this paper is built upon a search-based planner. We describe how to flexibly integrate joint angle changes in the path planning process and furthermore propose a representation of the robot configuration that substantially reduces the computational burden. In addition, we introduce search guidance heuristics that are particularly useful in environments in which a robot is required to pass over obstacles, such as on agricultural fields. An extensive evaluation on simulated and real-world data with our BoniRob agricultural robot demonstrates the efficiency of our approach.},
}

@InProceedings{chen2017icra-caao,
  author        = {S.W. Chen and S. Skandan and S. Dcunha and J. Das and C. Qu and C.J. Taylor and V. Kumar},
  title         = {{Counting Apples and Oranges with Deep Learning: A Data Driven Approach}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Agricultural Automation, Object detection, Segmentation, Categorization, Visual Learning},
  abstract      = {This paper describes a fruit counting pipeline based on deep learning that accurately counts fruit in unstructured environments. Obtaining reliable fruit counts is challenging because of variations in appearance due to illumination changes and occlusions from foliage and neighboring fruits. We propose a novel approach that uses deep learning to map from input images to total fruit counts. The pipeline utilizes a custom crowd-sourcing platform to quickly label large data sets. A blob detector based on a fully convolutional network extracts candidate regions in the images. A counting algorithm based on a second convolutional network then estimates the number of fruit in each region. Finally, a linear regression model maps that fruit count estimate to a final fruit count. We analyze the performance of the pipeline on two distinct data sets of oranges in daylight, and green apples at night, utilizing human generated labels as ground truth. We also show that the pipeline has a short training time and performs well with a limited data set size. Our method generalizes across both data sets and is able to perform well even on highly occluded fruits that are challenging for human labelers to annotate.},
}

@InProceedings{bonanni2017icra,
  author        = {T.M. Bonanni and B.D. Corte and G. Grisetti},
  title         = {{3D Map Merging}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Mapping, SLAM, Localization},
  abstract      = {In this paper, we propose an approach for merging 3D maps represented as pose graphs of point clouds. Our method can effectively deal with typical distortions affecting SLAM-generated maps. Traditional map merging techniques that use a single rigid body transformation to relate the reference frames of different maps. Instead, our approach achieves more accurate results by eliminating the inconsistencies resulting from distortions affecting the inputs, and can succeed in those situations where traditional approaches fail for substantial deformations. The core idea behind our solution is to localize the robot in a reference map by using the data from another map as observations. We validated our approach on publicly available datasets, and provide quantitative results that confirm its effectiveness on challenging instances of the merging problem.},
}

@InProceedings{deray2017icra,
  author        = {J. Deray and J. Sol and J. Andrade-Cetto},
  title         = {{Word Ordering and Document Adjacency for Large Loop Closure Detection in 2D Laser Maps}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Localization, Recognition, SLAM},
  abstract      = {We address in this paper the problem of loop closure detection for laser-based simultaneous localization and mapping (SLAM) of very large areas. Consistent with the state of the art, the map is encoded as a graph of poses, and to cope with very large mapping capabilities, loop closures are asserted by comparing the features extracted from a query laser scan against a previously acquired corpus of scan features using a bag-of-words (BoW) scheme. Two contributions are here presented. First, to benefit from the graph topology, feature frequency scores in the BoW are computed not only for each individual scan but also from neighboring scans in the SLAM graph. This has the effect of enforcing neighbor relational information during document matching. Secondly, a weak geometric check that takes into account feature ordering and occlusions is introduced that substantially improves loop closure detection performance. The two contributions are evaluated both separately and jointly on four common SLAM datasets, and are shown to improve the state-of-the-art performance both in terms of precision and recall in most of the cases. Moreover, our current implementation is designed to work at nearly frame rate, allowing loop closure query resolution at nearly 22 Hz for the best case scenario and 2 Hz for the worst case scenario.},
}

@InProceedings{schmidt2017icra,
  author        = {T. Schmidt and R. Newcombe and D. Fox},
  title         = {{Self-Supervised Learning of Dense Visual Descriptors}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Visual Learning, RGB-D Perception, Recognition, CNN},
  abstract      = {Robust estimation of correspondences between image pixels is an important problem in robotics, with applications in tracking, mapping, and recognition of objects, environments, and other agents. Correspondence estimation has long been the domain of hand-engineered features, but more recently deep learning techniques have provided powerful tools for learning features from raw data. The drawback of the latter approach is that a vast amount of (labelled, typically) training data is required for learning. This paper advocates a new approach to learning visual descriptors for dense correspondence estimation in which we harness the power of a strong 3D generative model to automatically label correspondences in RGB-D video data. A fully-convolutional network is trained using a contrastive loss to produce viewpoint- and lighting-invariant descriptors. As a proof of concept, we collected two datasets: the first depicts the upper torso and head of the same person in widely varied settings, and the second depicts an office as seen on multiple days with objects re-arranged within. Our datasets focus on re-visitation of the same objects and environments, and we show that by training the CNN only from local tracking data, our learned visual descriptor generalizes towards identifying non-labelled correspondences across videos. We furthermore show that our approach to descriptor learning can be used to achieve state-of-the-art single-frame localization results on the MSR 7-scenes dataset without using any labels identifying correspondences between separate videos of the same scenes at training time.},
}

@InProceedings{chen2017icra-dlfa,
  author        = {Z. Chen and A. Jacobson and N. S{\"u}nderhauf and B. Upcroft and L. Liu and C. Shen and I. Reid and M.J. Milford},
  title         = {{Deep Learning Features at Scale for Visual Place Recognition}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Visual Learning, Localization, Computer Vision for Other Robotic Applications, CNN},
  abstract      = {The success of deep learning techniques in the computer vision domain has triggered a range of initial investigations into their utility for visual place recognition, all using generic features from networks that were trained for other types of recognition tasks. In this paper, we train, at large scale, two CNN architectures for the specific place recognition task and employ a multi-scale feature encoding method to generate condition- and viewpoint-invariant features. To enable this training to occur, we have developed a massive Specific PlacEs Dataset (SPED) with hundreds of examples of place appearance change at thousands of different places, as opposed to the semantic place type datasets currently available. This new dataset enables us to set up a training regime that interprets place recognition as a classification problem. We comprehensively evaluate our trained networks on several challenging benchmark place recognition datasets and demonstrate that they achieve an average 10\% increase in performance over other place recognition algorithms and pre-trained CNNs. By analyzing the network responses and their differences from pre-trained networks, we provide insights into what a network learns when training for place recognition, and what these results signify for future research in this area.},
}

@InProceedings{mccool2017icra,
  author        = {C.S. McCool and T. Perez and B. Upcroft},
  title         = {{Mixtures of Lightweight Deep Convolutional Neural Networks: Applied to Agricultural Robotics}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Computer Vision for Automation, Recognition, Agricultural Automation, CNN},
  abstract      = {We propose a novel approach for training deep convolutional neural networks (DCNNs) that allows us to tradeoff complexity and accuracy to learn lightweight models suitable for robotic platforms such as AgBot II (which performs automated weed management). Our approach consists of three stages, the first is to adapt a pre-trained model to the task at hand. This provides state-of-the-art performance but at the cost of high computational complexity resulting in a low frame rate of just 0.12 frames per second (fps). Second, we use the adapted model and employ model compression techniques to learn a lightweight DCNN that is less accurate but has two orders of magnitude fewer parameters. Third, K lightweight models are combined as a mixture model to further enhance the performance of the lightweight models. Applied to the challenging task of weed segmentation, we improve accuracy from 85.9\%, using a traditional approach, to 93.9\% by adapting a complicated pre-trained DCNN with 25M parameters (Inception-v3). The downside to this adapted model, Adapted-IV3, is that it can only process 0.12fps. To make this approach fast while still retaining accuracy, we learn lightweight DCNNs which when combined can achieve accuracy greater than 90\% while using considerably fewer parameters capable of processing between 1.07 and 1.83 frames per second, up to an order of magnitude faster and up to an order of magnitude fewer parameters.},
}

@InProceedings{alismail2017icra,
  author        = {H. Alismail and M. Kaess and B. Browning and S. Lucey},
  title         = {{Direct Visual Odometry in Low Light Using Binary Descriptors}},
  booktitle     = icra,
  year          = 2017,
  keywords      = {Visual-Based Navigation, SLAM, Mining Robotics},
  abstract      = {Feature descriptors are powerful tools for photometrically and geometrically invariant image matching. To date, however, their use has been tied to sparse interest point detection, which is susceptible to noise under adverse imaging conditions. In this work, we propose to use binary feature descriptors in a direct tracking framework without relying on sparse interest points. This novel combination of feature descriptors and direct tracking is shown to achieve robust and efficient visual odometry with applications to poorly lit subterranean environments.},
}

@InProceedings{cieslewski2015icra,
  author        = {T. Cieslewski and S. Lynen and M.T. Dymczyk and S. Magnenat and R. Siegwart},
  title         = {{Map API - Scalable Decentralized Map Building for Robots}},
  booktitle     = icra,
  year          = 2015,
  keywords      = {Robotic Software, Middleware and Programming Environments, Networked Robots, Mapping},
  abstract      = {Large scale, long-term, distributed mapping is a core challenge to modern field robotics. Using the sensory output of multiple robots and fusing it in an efficient way enables the creation of globally accurate and consistent metric maps. To combine data from multiple agents into a global map, most existing approaches use a central entity that collects and manages the information from all agents. Often, the raw sensor data of one robot needs to be made available to processing algorithms on other agents due to the lack of computational resources on that robot. Unfortunately, network latency and low bandwidth in the field limit the generality of such an approach and make multi-robot map building a tedious task. In this paper, we present a distributed and decentralized back-end for concurrent and consistent robotic mapping. We propose a set of novel approaches that reduce the bandwidth usage and increase the effectiveness of inter-robot communication for distributed mapping. Instead of locking access to the map during operations, we define a version control system which allows concurrent and consistent access to the map data. Updates to the map are then shared asynchronously with agents which previously registered notifications. A technique for data lookup is provided by state-of-the-art algorithms from distributed computing. We validate our approach on real-world datasets and demonstrate the effectiveness of the proposed algorithms.},
}

@InProceedings{dymczyk2015icra,
  author        = {M.T. Dymczyk and S. Lynen and T. Cieslewski and M. Bosse and R. Siegwart and P.T. Furgale},
  title         = {{The Gist of Maps - Summarizing Experience for Lifelong Localization}},
  booktitle     = icra,
  year          = 2015,
  keywords      = {Localization, Visual-Based Control and/or Navigation, Mapping},
  abstract      = {Robust, scalable place recognition is a core competency for many robotic applications. However, when revisiting places over and over, many state-of-the-art approaches exhibit reduced performance in terms of computation and memory complexity and in terms of accuracy. For successful deployment of robots over long time scales, we must develop algorithms that get better with repeated visits to the same environment, while still working within a fixed computational budget. This paper presents and evaluates an algorithm that alternates between online place recognition and offline map maintenance with the goal of producing the best performance with a fixed map size. At the core of the algorithm is the concept of a Summary Map, a reduced map representation that includes only the landmarks that are deemed most useful for place recognition. To assign landmarks to the map, we use a scoring function that ranks the utility of each landmark and a sampling policy that selects the landmarks for each place. The Summary Map can then be used by any descriptor-based inference method for constant-complexity online place recognition. We evaluate a number of scoring functions and sampling policies and show that it is possible to build and maintain maps of a constant size and that place-recognition performance improves over multiple visits.},
}

@InProceedings{khan2015icra,
  author        = {S. Khan and D. Wollherr},
  title         = {{IBuILD: Incremental Bag of Binary Words for Appearance Based Loop Closure Detection}},
  booktitle     = icra,
  year          = 2015,
  keywords      = {Visual Place Recognition},
  abstract      = {In robotics applications such as SLAM (Simultaneous Localization and Mapping), loop closure detection is an integral component required to build a consistent topological or metric map. This paper presents an appearance based loop closure detection mechanism titled IBuILD (Incremental bag of BInary words for Appearance based Loop closure Detection). The presented approach focuses on an online, incremental formulation of binary vocabulary generation for loop closure detection. The proposed approach does not require a prior vocabulary learning phase and relies purely on the appearance of the scene for loop closure detection without the need of odometry or GPS estimates. The vocabulary generation process is based on feature tracking between consecutive images to incorporate pose invariance. In addition, this process is coupled with a simple likelihood function to generate the most suitable loop closure candidate and a temporal consistency constraint to filter out inconsistent loop closures. Evaluation on different publicly available outdoor urban and indoor datasets shows that the presented approach is capable of generating higher recall at 100\% precision in comparison to the state of the art.},
}

@InProceedings{zhang2015icra,
  author        = {J. Zhang and S. Singh},
  title         = {{Visual-Lidar Odometry and Mapping: Low-Drift, Robust, and Fast}},
  booktitle     = icra,
  year          = 2015,
  keywords      = {Visual-Based Control and/or Navigation, Range Sensing, Mapping},
  abstract      = {Here, we present a general framework for combining visual odometry and lidar odometry in a fundamental and first principle method. The method shows improvements in performance over the state of the art, particularly in robustness to aggressive motion and temporary lack of visual features. The proposed on-line method starts with visual odometry to estimate the ego-motion and to register point clouds from a scanning lidar at a high frequency but low fidelity. Then, scan matching based lidar odometry refines the motion estimation and point cloud registration simultaneously. We show results with datasets collected in our own experiments as well as using the KITTI odometry benchmark. Our proposed method is ranked #1 on the benchmark in terms of average translation and rotation errors, with a 0.75\% of relative position drift. In addition to comparison of the motion estimation accuracy, we evaluate robustness of the method when the sensor suite moves at a high speed and is subject to significant ambient lighting changes.},
}

@InProceedings{maddern2015icra,
  author        = {W. Maddern and G.M. Pascoe and P. Newman},
  title         = {{Leveraging Experience for Large-Scale LIDAR Localisation in Changing Cities}},
  booktitle     = icra,
  year          = 2015,
  keywords      = {Localization, Intelligent Transportation Systems, Mapping},
  abstract      = {Recent successful approaches to autonomous vehicle localisation and navigation typically involve 3D LIDAR scanners and a static, curated 3D map, both of which are expensive to acquire and maintain. In this paper we propose an experience-based approach to matching a local 3D swathe built using a push-broom 2D LIDAR to a number of prior 3D maps, each of which has been collected during normal driving in different conditions. Local swathes are converted to a combined 2D height and reflectance representation, and we exploit the GPU rendering pipeline to densely sample the localisation cost function to provide robustness and a wide basin of convergence. Prior maps are incrementally built into an experience-based framework from multiple traversals of the same environment, capturing changes in environment structure and appearance over time. The LIDAR localisation solutions from each prior map are fused with vehicle odometry in a probabilistic framework to provide a single pose solution suitable for automated driving. Using this framework we demonstrate realtime centimetre-level localisation using LIDAR data collected in a dynamic city environment over a period of a year.},
}

@InProceedings{pascoe2015icra,
  author        = {G.M. Pascoe and W. Maddern and A. Stewart and P. Newman},
  title         = {{FARLAP: Fast Robust Localisation Using Appearance Priors}},
  booktitle     = icra,
  year          = 2015,
  keywords      = {Localization, Visual Tracking, Intelligent Transportation Systems},
  abstract      = {This paper is concerned with large-scale localisation at city scales with monocular cameras. Our primary motivation lies with the development of autonomous road vehicles an application domain in which low-cost sensing is particularly important. Here we present a method for localising against a textured 3-dimensional prior mesh using a monocular camera. We first present a system for generating and texturing the prior using a LIDAR scanner and camera. We then describe how we can localise against that prior with a single camera, using an information-theoretic measure of image similarity. This process requires dealing with the distortions induced by a wide-angle camera. We present and justify an interesting approach to this issue in which we distort the prior map into the image rather than vice-versa. Finally we explain how the general purpose computation functionality of a modern GPU is particularly apt for our task, allowing us to run the system in real time. We present results showing centimetre-level localisation accuracy through a city over six kilometres.},
}

@InProceedings{paton2015icra,
  author        = {M. Paton and K.A. MacTavish and C.J. Ostafew and T. Barfoot},
  title         = {It's Not Easy Seeing Green: Lighting-Resistant Stereo Visual Teach & Repeat Using Color-Constant Images},
  booktitle     = icra,
  year          = 2015,
  keywords      = {Computer Vision for Robotics and Automation, Field Robots, Visual-Based Control and/or Navigation},
  abstract      = {Stereo Visual Teach & Repeat (VT&R) is a system for long-range, autonomous route following in unstructured 3D environments. As this system relies on a passive sensor to localize, it is highly susceptible to changes in lighting conditions. Recent work in the optics community has provided a method to transform images collected from a three-channel passive sensor into color-constant images that are resistant to changes in outdoor lighting conditions. This paper presents a lightingresistant VT&R system that uses experimentally trained colorconstant images to autonomously navigate difficult outdoor terrain despite changes in lighting. We show through an extensive field trial that our algorithm is capable of autonomously following a 1km outdoor route spanning sandy/rocky terrain, grassland, and wooded areas. Using a single visual map created at midday, the route was autonomously repeated 26 times over a period of four days, from sunrise to sunset with an autonomy rate (by distance) of over 99.9\%. These experiments show that a simple image transformation can extend the operation of VT&R from a few hours to multiple days.},
}

@InProceedings{carlone2015icra,
  author        = {L. Carlone and F. Dellaert},
  title         = {{Duality-Based Verification Techniques for 2D SLAM}},
  booktitle     = icra,
  year          = 2015,
  keywords      = {SLAM, Localization, Autonomous Navigation},
  abstract      = {While iterative optimization techniques for Simultaneous Localization and Mapping (SLAM) are now very efficient and widely used, none of them can guarantee global convergence to the maximum likelihood estimate. Local convergence usually implies artifacts in map reconstruction and large localization errors, hence it is very undesirable for applications in which accuracy and safety are of paramount importance. We provide a technique to verify if a given 2D SLAM solution is globally optimal. The insight is that, while computing the optimal solution is hard in general, duality theory provides tools to compute tight bounds on the optimal cost, via convex programming. These bounds can be used to evaluate the quality of a SLAM solution, hence providing a sanity check for stateof-the-art incremental and batch solvers. Experimental results show that our technique successfully identifies wrong estimates (i.e., local minima) in large-scale SLAM scenarios. This work, together with [1], represents a step towards the objective of having SLAM techniques with guaranteed performance, that can be used in safety-critical applications.},
}

@InProceedings{carlone2015icra-itf3,
  author        = {L. Carlone and R. Tron and K. Daniilidis and F. Dellaert},
  title         = {{Initialization Techniques for 3D SLAM: A Survey on Rotation Estimation and Its Use in Pose Graph Optimization}},
  booktitle     = icra,
  year          = 2015,
  keywords      = {SLAM, Localization, Mapping},
  abstract      = {Pose graph optimization is the non-convex optimization problem underlying pose-based Simultaneous Localization and Mapping (SLAM). If robot orientations were known, pose graph optimization would be a linear leastsquares problem, whose solution can be computed efficiently and reliably. Since rotations are the actual reason why SLAM is a difficult problem, in this work we survey techniques for 3D rotation estimation. Rotation estimation has a rich history in three scientific communities: robotics, computer vision, and control theory. We review relevant contributions across these communities, assess their practical use in the SLAM domain, and benchmark their performance on representative SLAM problems (Fig. 1). We show that the use of rotation estimation to bootstrap iterative pose graph solvers entails significant boost in convergence speed and robustness.},
}

@InProceedings{kaess2015icra,
  author        = {M. Kaess},
  title         = {{Simultaneous Localization and Mapping with Infinite Planes}},
  booktitle     = icra,
  year          = 2015,
  keywords      = {SLAM, Mapping, RGB-D Perception},
  abstract      = {Simultaneous localization and mapping with infinite planes is attractive because of the reduced complexity with respect to both sparse point-based and dense volumetric methods. We show how to include infinite planes into a leastsquares formulation for mapping, using a homogeneous plane parametrization with a corresponding minimal representation for the optimization. Because it is a minimal representation, it is suitable for use with Gauss-Newton, Powells Dog Leg and incremental solvers such as iSAM. We also introduce a relative plane formulation that improves convergence. We evaluate our proposed approach on simulated data to show its advantages over alternative solutions. We also introduce a simple mapping system and present experimental results, showing real-time mapping of select indoor environments with a hand-held RGBD sensor.},
}

@InProceedings{pepperell2015icra,
  author        = {E. Pepperell and P. Corke and M.J. Milford},
  title         = {{Automatic Image Scaling for Place Recognition in Changing Environments}},
  booktitle     = icra,
  year          = 2015,
  keywords      = {Visual Place Recognition, Visual-Based Control and/or Navigation},
  abstract      = {Robustness to variations in environmental conditions and camera viewpoint is essential for long-term place recognition, navigation and SLAM. Existing systems typically solve either of these problems, but invariance to both remains a challenge. This paper presents a training-free approach to lateral viewpoint- and condition-invariant, vision-based place recognition. Our successive frame patch-tracking technique infers average scene depth along traverses and automatically rescales views of the same place at different depths to increase their similarity. We combine our system with the condition-invariant SMART algorithm and demonstrate place recognition between day and night, across entire 4-lane-plus-median-strip roads, where current algorithms fail.},
}

@InProceedings{choudhary2015icra,
  author        = {S. Choudhary and V. Indelman and H.I. Christensen and F. Dellaert},
  title         = {{Information Based Reduced Landmark SLAM}},
  booktitle     = icra,
  year          = 2015,
  keywords      = {SLAM, Mapping, Localization},
  abstract      = {In this paper, we present an information-based approach to select a reduced number of landmarks and poses for a robot to localize itself and simultaneously build an accurate map. We develop an information theoretic algorithm to efficiently reduce the number of landmarks and poses in a SLAM estimate without compromising the accuracy of the estimated trajectory. We also propose an incremental version of the reduction algorithm which can be used in SLAM framework resulting in information based reduced landmark SLAM. The results of reduced landmark based SLAM algorithm are shown on Victoria park dataset and a Synthetic dataset and are compared with standard graph SLAM (SAM [6]) algorithm. We demonstrate a reduction of 40-50\% in the number of landmarks and around 55\% in the number of poses with minimal estimation error as compared to standard SLAM algorithm.},
}

@InProceedings{english2015icra,
  author        = {A. English and P. Ross and D. Ball and B. Upcroft and P. Corke},
  title         = {{TriggerSync: A Time Synchronisation Tool}},
  booktitle     = icra,
  year          = 2015,
  keywords      = {Robotic Software, Middleware and Programming Environments, Calibration and Identification, Sensor Fusion},
  abstract      = {This paper presents a framework for synchronising multiple triggered sensors with respect to a local clock using standard computing hardware. Providing sensor measurements with accurate and meaningful timestamps is important for many sensor fusion, state estimation and control applications. Accurately synchronising sensor timestamps can be performed with specialised hardware, however, performing sensor synchronisation using standard computing hardware and non-real-time operating systems is difficult due to inaccurate and temperature sensitive clocks, variable communication delays and operating system scheduling delays. Results show the ability of our framework to estimate time offsets to sub-millisecond accuracy. We also demonstrate how synchronising timestamps with our framework results in a tenfold reduction in image stabilisation error for a vehicle driving on rough terrain. The source code will be released as an open source tool for time synchronisation in ROS.},
}

@InProceedings{vineet2015icra,
  author        = {V. Vineet and O. Miksik and M. Lidegaard and M. Niessner and S. Golodetz and V. Prisacariu and O. Kahler and D. Murray and S. Izadi and P. Perez and P. Torr},
  title         = {{Incremental Dense Semantic Stereo Fusion for Large-Scale Semantic Scene Reconstruction}},
  booktitle     = icra,
  year          = 2015,
  keywords      = {Semantic Scene Understanding, Computer Vision for Robotics and Automation},
  abstract      = {Our abilities in scene understanding, which allow us to perceive the 3D structure of our surroundings and intuitively recognise the objects we see, are things that we largely take for granted, but for robots, the task of understanding large scenes quickly remains extremely challenging. Recently, scene understanding approaches based on 3D reconstruction and semantic segmentation have become popular, but existing methods either do not scale, fail outdoors, provide only sparse reconstructions or are rather slow. In this paper, we build on a recent hash-based technique for large-scale fusion and an efficient mean-field inference algorithm for densely-connected CRFs to present what to our knowledge is the first system that can perform dense, large-scale, outdoor semantic reconstruction of a scene in (near) real time. We also present a semantic fusion approach that allows us to handle dynamic objects more effectively than previous approaches. We demonstrate the effectiveness of our approach on the KITTI dataset, and provide qualitative and quantitative results showing high-quality dense reconstruction and labelling of a number of scenes.},
}

@InProceedings{agamennoni2015icra,
  author        = {G. Agamennoni and P.T. Furgale and R. Siegwart},
  title         = {{Self-Tuning M-Estimators}},
  booktitle     = icra,
  year          = 2015,
  keywords      = {SLAM},
  abstract      = {M-estimators are the de-facto standard method of robust estimation in robotics. They are easily incorporated into iterative non-linear least-squares estimation and provide seamless and effective handling of outliers in data. However, every M-estimators robust loss function has one or more tuning parameters that control the influence of different data. The choice of M-estimator and the manual tuning of these parameters is always a source of uncertainty when applying the technique to new data or a new problem. In this paper we develop the concept of self-tuning Mestimators. We first make the connection between many common M-estimators and elliptical probability distributions. This connection shows that the choice of M-estimator is an assumption that the residuals belong to a well-defined elliptical distribution. We exploit this implication in two ways. First, we develop an algorithm for tuning the M-estimator parameters during iterative optimization. Second, we show how to choose the correct M-estimator for your data by examining the likelihood of the data given the model. We fully derive these algorithms and show their behavior on a representative example of visual simultaneous localization and mapping.},
}

@InProceedings{stumm2015icra,
  author        = {E. Stumm and C. Mei and S. Lacroix and M. Chli},
  title         = {{Location Graphs for Visual Place Recognition}},
  booktitle     = icra,
  year          = 2015,
  keywords      = {Visual Place Recognition, Autonomous Navigation, Mapping},
  abstract      = {With the growing demand for deployment of robots in real scenarios, robustness in the perception capabilities for navigation lies at the forefront of research interest, as this forms the backbone of robotic autonomy. Existing place recognition approaches traditionally follow the feature-based bag-of-words paradigm in order to cut down on the richness of information in images. As structural information is typically ignored, such methods suffer from perceptual aliasing and reduced recall, due to the ambiguity of observations. In a bid to boost the robustness of appearance-based place recognition, we consider the world as a continuous constellation of visual words, while keeping track of their covisibility in a graph structure. Locations are queried based on their appearance, and modelled by their corresponding cluster of landmarks from the global covisibility graph, which retains important relational information about landmarks. Complexity is reduced by comparing locations by their graphs of visual words in a simplified manner. Test results show increased recall performance and robustness to noisy observations, compared to state-of-the-art methods.},
}

@InProceedings{suger2015icra,
  author        = {B. Suger and B. Steder and W. Burgard},
  title         = {{Traversability Analysis for Mobile Robots in Outdoor Environments: A Semi-Supervised Learning Approach Based on 3D-Lidar Data}},
  booktitle     = icra,
  year          = 2015,
  keywords      = {Field Robots, Learning and Adaptive Systems},
  abstract      = {The ability to safely navigate is a crucial prerequisite for truly autonomous systems. A robot has to distinguish obstacles from traversable ground. Failing on this task can cause great damage or restrict the robots movement unnecessarily. Due to the security relevance of this problem, great effort is typically spent to design models for individual robots and sensors, and the complexity of such models is correlated to the complexity of the environment and the capabilities of the robot. We present a semi supervised learning approach, where the robot learns its traversability capabilities from a human operating it. From this partially and only positive labeled training data, our approach infers a model for the traversability analysis, thereby requiring very little manual effort for the human. In practical experiments we show that our method can be used for robots that need to reliably navigate on dirt roads as well as for robots that have very restricted traversability capabilities. Fig. 1. Different mobile robot outdoor platforms with different capabilities and different fields of applications.},
}

@InProceedings{meier2015icra,
  author        = {L. Meier and D. Honegger and M. Pollefeys},
  title         = {{PX4: A Node-Based Multithreaded Open Source Robotics Framework for Deeply Embedded Platforms}},
  booktitle     = icra,
  year          = 2015,
  keywords      = {Robotic Software, Middleware and Programming Environments},
  abstract      = {We present a novel, deeply embedded robotics middleware and programming environment. It uses a multithreaded, publish-subscribe design pattern and provides a Unixlike software interface for micro controller applications. We improve over the state of the art in deeply embedded open source systems by providing a modular and standards-oriented platform. Our system architecture is centered around a publishsubscribe object request broker on top of a POSIX application programming interface. This allows to reuse common Unix knowledge and experience, including a bash-like shell. We demonstrate with a vertical takeoff and landing (VTOL) use case that the system modularity is well suited for novel and experimental vehicle platforms. We also show how the system architecture allows a direct interface to ROS and to run individual processes either as native ROS nodes on Linux or nodes on the micro controller, maximizing interoperability. Our microcontroller-based execution environment has substantially lower latency and better hardware connectivity than a typical Robotics Linux system and is therefore well suited for fast, high rate control tasks.},
}

@InProceedings{linegar2015icra,
  author        = {C. Linegar and W. Churchill and P. Newman},
  title         = {{Work Smart, Not Hard: Recalling Relevant Experiences for Vast-Scale but Time-Constrained Localisation}},
  booktitle     = icra,
  year          = 2015,
  keywords      = {Localization, Mapping, Computer Vision for Robotics and Automation},
  abstract      = {This paper is about life-long vast-scale localisation in spite of changes in weather, lighting and scene structure. Building upon our previous work in Experience-based Navigation [1], we continually grow and curate a visual map of the world that explicitly supports multiple representations of the same place. We refer to these representations as experiences, where a single experience captures the appearance of an environment under certain conditions. Pedagogically, an experience can be thought of as a visual memory. By accumulating experiences we are able to handle cyclic appearance change (diurnal lighting, seasonal changes, and extreme weather conditions) and also adapt to slow structural change. This strategy, although elegant and effective, poses a new challenge: In a region with many stored representations which one(s) should we try to localise against given finite computational resources? By learning from our previous use of the experience-map, we can make predictions about which memories we should consider next, conditioned on how the robot is currently localised in the experience-map. During localisation, we prioritise the loading of past experiences in order to minimise the expected computation required. We do this in a probabilistic way and show that this memory policy significantly improves localisation efficiency, enabling long-term autonomy on robots with limited computational resources. We demonstrate and evaluate our system over three challenging datasets, totalling 206km of outdoor travel. We demonstrate the system in a diverse range of lighting and weather conditions, scene clutter, camera occlusions, and permanent structural change in the environment.},
}

@InProceedings{steiner2015icra,
  author        = {T. Steiner and G. Huang and J. Leonard},
  title         = {{Location Utility-Based Map Reduction}},
  booktitle     = icra,
  year          = 2015,
  keywords      = {Autonomous Navigation, SLAM, Localization},
  abstract      = {Maps used for navigation often include a database of location descriptions for place recognition (loop closing), which permits bounded-error performance. A standard posegraph SLAM system adds a new entry for every new pose into the location database, which grows linearly and unbounded in time and thus becomes unsustainable. To address this issue, in this paper we propose a new map-reduction approach that pre-constructs a fixed-size place-recognition database amenable to the limited storage and processing resources of the vehicle by exploiting the high-level structure of the environment as well as the vehicle motion. In particular, we introduce the concept of location utility which encapsulates the visitation probability of a location and its spatial distribution relative to nearby locations in the database as a measure of the value of potential loop-closure events to occur at that location. While finding the optimal reduced location database is NPhard, we develop an efficient greedy algorithm to sort all the locations in a map based on their relative utility without access to sensor measurements or the vehicle trajectory. This enables pre-determination of a generic, limited-size place-recognition database containing the N best locations in the environment. To validate the proposed approach, we develop an open-source street-map simulator using real city-map data and show that an accurate map (pose-graph) can be attained even when using a place-recognition database with only 1\% of the entries of the corresponding full database.},
}

@InProceedings{carrillo2015icra,
  author        = {H. Carrillo and P. Dames and V. Kumar and J.A. Castellanos},
  title         = {{Autonomous Robotic Exploration Using Occupancy Grid Maps and Graph SLAM Based on Shannon and Rnyi Entropy}},
  booktitle     = icra,
  year          = 2015,
  keywords      = {Autonomous Navigation, SLAM, Mapping},
  abstract      = {In this paper we examine the problem of autonomously exploring and mapping an environment using a mobile robot. The robot uses a graph-based SLAM system to perform mapping and represents the map as an occupancy grid. In this setting, the robot must trade-off between exploring new area to complete the task and exploiting the existing information to maintain good localization. Selecting actions that decrease the map uncertainty while not significantly increasing the robots localization uncertainty is challenging. We present a novel information-theoretic utility function that uses both Shannons and Renyis definitions of entropy to jointly consider the uncertainty of the robot and the map. This allows us to fuse both uncertainties without the use of manual tuning. We present simulations and experiments comparing the proposed utility function to state-of-the-art utility functions, which only use Shannons entropy. We show that by using the proposed utility function, the robot and map uncertainties are smaller than using other existing methods.},
}

@InProceedings{merriaux2015icra,
  author        = {P. Merriaux and Y. Dupuis and P. Vasseur and X. Savatier},
  title         = {{Fast and Robust Vehicle Positioning on Graph-Based Representation of Drivable Maps}},
  booktitle     = icra,
  year          = 2015,
  keywords      = {Localization, Intelligent Transportation Systems},
  abstract      = {In this paper, we propose a car positioning approach that does not rely on GPS. We propose to use car wheel speeds and road maps in order to achieve robust positioning of the vehicle. The vehicle positioning is achieved by applying particle filtering on a graph-based representation of a road map. We show that the vehicle positioning is feasible and robust with these two inputs at a really low computational cost. We achieve car positioning with an averaged 5 m accuracy within a 100 km drivable road map on a 12 km sequence.},
}

@InProceedings{heise2015icra,
  author        = {P. Heise and B. Jensen and S. Klose and A. Knoll},
  title         = {{Fast Dense Stereo Correspondences by Binary Locality Sensitive Hashing}},
  booktitle     = icra,
  year          = 2015,
  keywords      = {Computer Vision for Robotics and Automation},
  abstract      = {The stereo correspondence problem is still a highly active topic of research with many applications in the robotic domain. Still many state of the art algorithms proposed to date are unable to reasonably handle high resolution images due to their run time complexities or memory requirements. In this work we propose a novel stereo correspondence estimation algorithm that employs binary locality sensitive hashing and is well suited to implementation on the GPU. Our proposed method is capable of processing very high-resolution stereo images at near real-time rates. An evaluation on the new Middlebury and Disney high-resolution stereo benchmarks demonstrates that our proposed method performs well compared to existing state of the art algorithms.},
}

@InProceedings{forster2015icra,
  author        = {C. Forster and M. Faessler and F. Fontana and M. Werlberger and D. Scaramuzza},
  title         = {{Continuous On-Board Monocular-Vision-based Elevation Mapping Applied to Autonomous Landing of Micro Aerial Vehicles}},
  booktitle     = icra,
  year          = 2015,
  keywords      = {Computer Vision for Robotics and Automation, Aerial Robotics},
  abstract      = {In this paper, we propose a resource-efficient system for real-time 3D terrain reconstruction and landingspot detection for micro aerial vehicles. The system runs on an on-board smartphone processor and requires only the input of a single downlooking camera and an inertial measurement unit. We generate a two-dimensional elevation map that is probabilistic, of fixed size, and robot-centric, thus, always covering the area immediately underneath the robot. The elevation map is continuously updated at a rate of 1 Hz with depth maps that are triangulated from multiple views using recursive Bayesian estimation. To highlight the usefulness of the proposed mapping framework for autonomous navigation of micro aerial vehicles, we successfully demonstrate fully autonomous landing including landing-spot detection in real-world experiments.},
}

@InProceedings{nelson2015icra,
  author        = {P. Nelson and W. Churchill and I. Posner and P. Newman},
  title         = {{From Dusk Till Dawn: Localisation at Night Using Artificial Light Sources}},
  booktitle     = icra,
  year          = 2015,
  keywords      = {Localization, Mapping, Visual-Based Control and/or Navigation},
  abstract      = {This paper is about localising at night in urban environments using vision. Despite it being dark exactly half of the time, surprisingly little attention has been given to this problem. A defining aspect of night-time urban scenes is the presence and effect of artificial lighting be that in the form of street or interior lighting through windows. By building a model of the environment which includes a representation of the spatial location of every light source, localisation becomes possible using monocular cameras. One of the challenges we face is the gross change in light appearance as a function of distance due to flare, saturation and bleeding city lights certainly do not appear as point features. To overcome this, we model the appearance of each light as a function of vehicle location, using this to inform our data-association decisions and to regularise the cost function which is used to infer vehicle pose. In this way we develop a place-dependent but stable sensor model which is customised for the particular environment in which we are operating. We demonstrate that our system is able to localise successfully at night over 12 km in situations where a traditional point feature based system fails.},
}

@InProceedings{mitzel2015icra,
  author        = {D. Mitzel and J. Diesel and A. Osep and U. Rafi and B. Leibe},
  title         = {{A Fixed-Dimensional 3D Shape Representation for Matching Partially Observed Objects in Street Scenes}},
  booktitle     = icra,
  year          = 2015,
  keywords      = {Object detection, Segmentation, Categorization, RGB-D Perception, Human detection & tracking},
  abstract      = {In this paper, we present an object-centric, fixeddimensional 3D shape representation for robust matching of partially observed object shapes, which is an important component for object categorization from 3D data. A main problem when working with RGB-D data from stereo, Kinect, or laser sensors is that the 3D information is typically quite noisy. For that reason, we accumulate shape information over time and register it in a common reference frame. Matching the resulting shapes requires a strategy for dealing with partial observations. We therefore investigate several distance functions and kernels that implement different such strategies and compare their matching performance in quantitative experiments. We show that the resulting representation achieves good results for a large variety of vision tasks, such as multi-class classification, person orientation estimation, and articulated body pose estimation, where robust 3D shape matching is essential.},
}

@InProceedings{coleman2015icra,
  author        = {D. Coleman and I.A. Sucan and M. Moll and K. Okada and N. Correll},
  title         = {{Experience-Based Planning with Sparse Roadmap Spanners}},
  booktitle     = icra,
  year          = 2015,
  keywords      = {Motion and Path Planning, Humanoid Robots, Learning and Adaptive Systems},
  abstract      = {We present an experience-based planning framework called Thunder that learns to reduce computation time required to solve high-dimensional planning problems in varying environments. The approach is especially suited for large configuration spaces that include many invariant constraints, such as those found with whole body humanoid motion planning. Experiences are generated using probabilistic sampling and stored in a sparse roadmap spanner (SPARS), which provides asymptotically near-optimal coverage of the configuration space, making storing, retrieving, and repairing past experiences very efficient with respect to memory and time. The Thunder framework improves upon past experience-based planners by storing experiences in a graph rather than in individual paths, eliminating redundant information, providing more opportunities for path reuse, and providing a theoretical limit to the size of the experience graph. These properties also lead to improved handling of dynamically changing environments, reasoning about optimal paths, and reducing query resolution time. The approach is demonstrated on a 30 degrees of freedom humanoid robot and compared with the Lightning framework, an experience-based planner that uses individual paths to store past experiences. In environments with variable obstacles and stability constraints, experiments show that Thunder is on average an order of magnitude faster than Lightning and planning from scratch. Thunder also uses 98.8\% less memory to store its experiences after 10,000 trials when compared to Lightning. Our framework is implemented and freely available in the Open Motion Planning Library.},
}

@InProceedings{churchill2015icra,
  author        = {W. Churchill and C.H. Tong and C. Gurau and I. Posner and P. Newman},
  title         = {{Know Your Limits: Embedding Localiser Performance Models in Teach and Repeat Maps}},
  booktitle     = icra,
  year          = 2015,
  keywords      = {Localization, Mapping, Computer Vision for Robotics and Automation},
  abstract      = {This paper is about building maps which not only contain the traditional information useful for localising such as point features but also embeds a spatial model of expected localiser performance. This often overlooked secondorder information provides vital context when it comes to map use and planning. Our motivation here is to improve the performance of the popular Teach and Repeat paradigm [1] which has been shown to enable truly large-scale field operation. When using the taught route for localisation, it is often assumed the robot is following exactly, or is sufficiently close to, the original path, enabling successful localisation. However, what happens if it is not possible, or not desirable to exactly follow the mapped path? How far off the beaten track can the robot travel before it gets lost? We present an approach for assessing this localisation area around a taught route, which we refer to as the localisation envelope. Using a combination of physical sampling and a Gaussian Process model, we are able to accurately predict the localisation performance at unseen points.},
}

@InProceedings{paz2015icra,
  author        = {L.M. Paz and P. Pinies and P. Newman},
  title         = {{A Variational Approach to Online Road and Path Segmentation with Monocular Vision}},
  booktitle     = icra,
  year          = 2015,
  keywords      = {Visual Learning, Intelligent Transportation Systems, Computer Vision for Robotics and Automation},
  abstract      = {In this paper we present an online approach to segmenting roads on large scale trajectories using only a monocular camera mounted on a car. We differ from popular 2D segmentation solutions which use single colour images and machine learning algorithms that require supervised training on huge image databases. Instead, we propose a novel approach that fuses 3D geometric data with appearance-based segmentation of 2D information in an automatic system. Our contribution is twofold: first, we propagate labels from frame to frame using depth priors of the segmented road avoiding user interaction most of the time; second, we transfer the segmented road labels to 3D laser point clouds. This reduces the complexity of stateof-the-art segmentation algorithms running on 3D Lidar data. Segmentation fails is in only 3\% of the cases over a sequence of 13,600 monocular images spanning an urban trajectory of more than 10km.},
}

@InProceedings{rhinehart2015icra,
  author        = {N. Rhinehart and J. Zhou and M. Hebert and J. Bagnell},
  title         = {{Visual Chunking: A List Prediction Framework for Region-Based Object Detection}},
  booktitle     = icra,
  year          = 2015,
  keywords      = {Computer Vision for Robotics and Automation},
  abstract      = {We consider detecting objects in an image by iteratively selecting from a set of arbitrarily shaped candidate regions. Our generic approach, which we term visual chunking, reasons about the locations of multiple object instances in an image while expressively describing object boundaries. We design an optimization criterion for measuring the performance of a list of such detections as a natural extension to a common per-instance metric. We present an efficient algorithm with provable performance for building a highquality list of detections from any candidate set of region-based proposals. We also develop a simple class-specific algorithm to generate a candidate region instance in near-linear time in the number of low-level superpixels that outperforms other region generating methods. In order to make predictions on novel images at testing time without access to ground truth, we develop learning approaches to emulate these algorithms behaviors. We demonstrate that our new approach outperforms sophisticated baselines on benchmark datasets.},
}

@InProceedings{berczi2015icra,
  author        = {L. Berczi and I. Posner and T. Barfoot},
  title         = {{Learning to Assess Terrain from Human Demonstration Using an Introspective Gaussian-Process Classifier}},
  booktitle     = icra,
  year          = 2015,
  keywords      = {Learning and Adaptive Systems, Autonomous Navigation, Robot Safety},
  abstract      = {This paper presents an approach to learning robot terrain assessment from human demonstration. An operator drives a robot for a short period of time, supervising the gathering of traversable and untraversable terrain data. After this initial training period, the robot can then predict the traversability of new terrain based on its experiences. We improve on current methods in two ways: first, we maintain a richer (higher-dimensional) representation of the terrain that is better able to distinguish between different training examples. Second, we use a Gaussian-process classifier for terrain assessment due to its superior introspective abilities (leading to better uncertainty estimates) when compared to other classifier methods in the literature. Our method is tested on real data and shown to outperform current methods both in classification accuracy and uncertainty estimation.},
}

@InProceedings{pinies2015icra,
  author        = {P. Pinies and L.M. Paz and P. Newman},
  title         = {{Dense Mono Reconstruction: Living with the Pain of the Plain Plane}},
  booktitle     = icra,
  year          = 2015,
  keywords      = {Computer Vision for Robotics and Automation, Mapping},
  abstract      = {This paper is about dense depthmap estimation using a monocular camera in workspaces with extensive textureless surfaces. Current state of the art techniques have been shown to work in real time with an admirable performance in desktop-size environments. Unfortunately, as we show in this paper, when applied to larger indoor environments, performance often degrades. A common cause is the presence of large affine texture-less areas like by walls, floors, ceilings and drab objects such as chairs and tables. These produce noisy and worse still, grossly erroneous initial seeds for the depthmap that greatly impede successful optimisation. We solve this problem via the introduction of a new nonlocal higher-order regularisation term that enforces piecewise affine constraints between image pixels that are far apart in the image. This property leverages the observation that the depth at the edges of bland regions are often well estimated whereas their inner pixels are deeply problematic. A welcome by-product of our proposed technique is an estimate of the surface normals at each pixel. We will show that in terms of implementation, our algorithm is a natural extension of the often used variational approaches. We evaluate the proposed technique using real datasets for which we have ground truth models.},
}

@InProceedings{jalobeanu2015icra,
  author        = {M. Jalobeanu and G. Shirakyan and G. Parent and H. Kikkeri and B. Peasley and A. Feniello},
  title         = {{Reliable Kinect-Based Navigation in Large Indoor Environments}},
  booktitle     = icra,
  year          = 2015,
  keywords      = {Autonomous Navigation, Localization, SLAM},
  abstract      = {Practical mapping and navigation solutions for large indoor environments continue to rely on relatively expensive range scanners, because of their accuracy, range and field of view. Microsoft Kinect on the other hand is inexpensive, is easy to use and has high resolution, but suffers from high noise, shorter range and a limiting field of view. We present a mapping and navigation system that uses the Microsoft Kinect sensor as the sole source of range data and achieves performance comparable to state-of-theart LIDAR-based systems. We show how we circumvent the main limitations of Kinect to generate usable 2D maps of relatively large spaces and to enable robust navigation in changing and dynamic environments. We use the Benchmark for Robotic Indoor Navigation (BRIN) to quantify and validate the performance of our system.},
}

@InProceedings{pinies2015icra-tmti,
  author        = {P. Pinies and L.M. Paz and P. Newman},
  title         = {{Too Much TV Is Bad: Dense Reconstruction from Sparse Laser with Non-Convex Regularisation}},
  booktitle     = icra,
  year          = 2015,
  keywords      = {Mapping, Computer Vision for Robotics and Automation},
  abstract      = {In this paper we address the problem of dense depth map estimation from sparse noisy range data to reconstruct large heterogeneous outdoor scenes. We propose a surface inpainting solution through energy minimisation with an adaptive selection of surface regularisers among a set of well known convex and non-convex regularisers. In fact, the selection of norm is pivotal with respect to the intrinsic surface characteristics. Our goal is to show how dense interpolation of sparse range data can be leveraged of more exotic and non-convex regularisers such as the log and logTGV [1] which can better capture the scene geometry. In contrast to state of the art solutions, we do not restrict ourselves to this set of norms, instead we search for the most apt norm for each semantically segmented part of the scene. Our energy model selection use Bayesian optimisation to learn the best choice of free parameters. This results in an adaptive model selection and the generalisation of well studied regularisation norms. We conclude with a detailed experimental analysis of our approach using a basis of four norms over a set of challenging outdoor scenes.},
}

@InProceedings{cunningham2015icra,
  author        = {A.G. Cunningham and E. Galceran and R. Eustice and E. Olson},
  title         = {{MPDM: Multipolicy Decision Making in Dynamic, Uncertain Environments for Autonomous Driving}},
  booktitle     = icra,
  year          = 2015,
  keywords      = {AI Reasoning Methods, Planning, Scheduling and Coordination, Intelligent Transportation Systems},
  abstract      = {Real-world autonomous driving in city traffic must cope with dynamic environments including other agents with uncertain intentions. This poses a challenging decisionmaking problem, e.g., deciding when to perform a passing maneuver or how to safely merge into traffic. Previous work in the literature has typically approached the problem using adhoc solutions that do not consider the possible future states of other agents, and thus have difficulty scaling to complex traffic scenarios where the actions of participating agents are tightly conditioned on one another. In this paper we present multipolicy decision-making (MPDM), a decision-making algorithm that exploits knowledge from the autonomous driving domain to make decisions online for an autonomous vehicle navigating in traffic. By assuming the controlled vehicle and other traffic participants execute a policy from a set of plausible closedloop policies at every timestep, the algorithm selects the best available policy for the controlled vehicle to execute. We perform policy election using forward simulation of both the controlled vehicle and other agents, efficiently sampling from the high-likelihood outcomes of their interactions. We then score the resulting outcomes using a user-defined cost function to accommodate different driving preferences, and select the policy with the highest score. We demonstrate the algorithm on a realworld autonomous vehicle performing passing maneuvers and in a simulated merging scenario.},
}

@InProceedings{ok2016icra,
  author        = {K. Ok and W.N. Greene and N. Roy},
  title         = {{Simultaneous Tracking and Rendering: Real-Time Monocular Localization for MAVs}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {Localization},
  abstract      = {We propose a method of real-time monocular camera-based localization in known environments. With the goal of controlling high-speed micro air vehicles (MAVs), we localize with respect to a mesh map of the environment that can support both pose estimation and trajectory planning. Using only limited hardware that can be carried on a MAV, we achieve accurate pose estimation at rates above 50 Hz, an order of magnitude faster than the current state-of-the-art meshbased localization algorithms. In our simultaneous tracking and rendering (STAR) approach, we render virtual images of the environment and track camera images with respect to them using a robust semi-direct image alignment technique. Our main contribution is the decoupling of camera tracking from virtual image rendering, which drastically reduces the number of rendered images and enables accurate full camerarate tracking without needing a high-end GPU. We demonstrate our approach in GPS-denied indoor environments.},
}

@InProceedings{bogoslavskyi2016icra,
  author        = {I. Bogoslavskyi and M. Mazuran and C. Stachniss},
  title         = {{Robust Homing for Autonomous Robots}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {Autonomous Vehicle Navigation, Search and Rescue Robots, Mapping},
  abstract      = {In autonomous exploration tasks, robots usually rely on a SLAM system to build a map of the environment online and then use it for navigation purposes. Although there has been substantial progress in robustly building accurate maps, these systems cannot guarantee the consistency of the resulting environment model. In this paper, we address the problem of robustly guiding a robot back to its starting location after exploring an unknown environmenteven if the mapping system fails to produce a consistent map. To tackle this problem, we propose a two-step procedure. First, we check if the current map is consistent using a statistical test. If the map is consistent, we navigate the robot back to its starting location using a standard navigation system. In case of an inconsistent map, however, we propose to rewind the trajectory from the current location to the start without relying on a map. We implemented the proposed system in ROS and showcase its effectiveness on an autonomous exploration robot in real underground and office environments.},
  url           = {http://www.ipb.uni-bonn.de/wp-content/papercite-data/pdf/bogoslavskyi16icra.pdf},
}

@InProceedings{zhang2016icra,
  author        = {J. Zhang and M. Kaess and S. Singh},
  title         = {{On Degeneracy of Optimization-Based State Estimation Problems}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {Visual-Based Navigation, Range Sensing, Mapping},
  abstract      = {Positioning and mapping can be conducted accurately by state-of-the-art state estimation methods. However, reliability of these methods is largely based on avoiding degeneracy that can arise from cases such as scarcity of texture features for vision sensors and lack of geometrical structures for range sensors. Since the problems are inevitably solved in uncontrived environments where sensors cannot function with their highest quality, it is important for the estimation methods to be robust to degeneracy. This paper proposes an online method to mitigate for degeneracy in optimizationbased problems, through analysis of geometric structure of the problem constraints. The method determines and separates degenerate directions in the state space, and only partially solves the problem in well-conditioned directions. We demonstrate utility of this method with data from a camera and lidar sensor pack to estimate 6-DOF ego-motion. Experimental results show that the system is able to improve estimation in environmentally degenerate cases, resulting in enhanced robustness for online positioning and mapping.},
}

@InProceedings{rosen2016icra,
  author        = {D. Rosen and J. Mason and J. Leonard},
  title         = {{Towards Lifelong Feature-Based Mapping in Semi-Static Environments}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {Mapping, SLAM, Probability and Statistical Methods},
  abstract      = {The feature-based graphical approach to robotic mapping provides a representationally rich and computationally efficient framework for an autonomous agent to learn a model of its environment. However, this formulation does not naturally support long-term autonomy because it lacks a notion of environmental change; in reality, everything changes and nothing stands still, and any mapping and localization system that aims to support truly persistent autonomy must be similarly adaptive. To that end, in this paper we propose a novel feature-based model of environmental evolution over time. Our approach is based upon the development of an expressive probabilistic generative feature persistence model that describes the survival of abstract semi-static environmental features over time. We show that this model admits a recursive Bayesian estimator, the persistence filter, that provides an exact online method for computing, at each moment in time, an explicit Bayesian belief over the persistence of each feature in the environment. By incorporating this feature persistence estimation into current state-of-the-art graphical mapping techniques, we obtain a flexible, computationally efficient, and information-theoretically rigorous framework for lifelong environmental modeling in an ever-changing world.},
}

@InProceedings{usenko2016icra,
  author        = {V. Usenko and J. Engel and J. St{\"u}ckler and D. Cremers},
  title         = {{Direct Visual-Inertial Odometry with Stereo Cameras}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {Sensor Fusion, Visual Tracking, Mapping},
  abstract      = {We propose a novel direct visual-inertial odometry method for stereo cameras. Camera pose, velocity and IMU biases are simultaneously estimated by minimizing a combined photometric and inertial energy functional. This allows us to exploit the complementary nature of vision and inertial data. At the same time, and in contrast to all existing visual-inertial methods, our approach is fully direct: geometry is estimated in the form of semi-dense depth maps instead of manually designed sparse keypoints. Depth information is obtained both from static stereo relating the fixed-baseline images of the stereo camera and temporal stereo relating images from the same camera, taken at different points in time. We show that our method outperforms not only vision-only or loosely coupled approaches, but also can achieve more accurate results than state-of-the-art keypoint-based methods on different datasets, including rapid motion and significant illumination changes. In addition, our method provides high-fidelity semi-dense, metric reconstructions of the environment, and runs in real-time on a CPU.},
}

@InProceedings{peretroukhin2016icra,
  author        = {V. Peretroukhin and W. Vega-Brown and N. Roy and J. Kelly},
  title         = {{PROBE-GK: Predictive Robust Estimation Using Generalized Kernels}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {Visual-Based Navigation, Visual Learning, Localization},
  abstract      = {Many algorithms in computer vision and robotics make strong assumptions about uncertainty, and rely on the validity of these assumptions to produce accurate and consistent state estimates. In practice, dynamic environments may degrade sensor performance in predictable ways that cannot be captured with static uncertainty parameters. In this paper, we employ fast nonparametric Bayesian inference techniques to more accurately model sensor uncertainty. By setting a prior on observation uncertainty, we derive a predictive robust estimator, and show how our model can be learned from sample images, both with and without knowledge of the motion used to generate the data. We validate our approach through Monte Carlo simulations, and report significant improvements in localization accuracy relative to a fixed noise model in several settings, including on synthetic data, the KITTI dataset, and our own experimental platform.},
}

@InProceedings{tanner2016icra,
  author        = {M. Tanner and P. Pinies and L.M. Paz and P. Newman},
  title         = {{What Lies Behind: Recovering Hidden Shape in Dense Mapping}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {Mapping, Range Sensing, Field Robots},
  abstract      = {In mobile robotics applications, generation of accurate static maps is encumbered by the presence of ephemeral objects such as vehicles, pedestrians, or bicycles. We propose a method to process a sequence of laser point clouds and back-fill dense surfaces into gaps caused by removing objects from the scene a valuable tool in scenarios where resource constraints permit only one mapping pass in a particular region. Our method processes laser scans in a three-dimensional voxel grid using the Truncated Signed Distance Function (TSDF) and then uses a Total Variation (TV) regulariser with a Kernel Conditional Density Estimation (KCDE) soft data term to interpolate missing surfaces. Using four scenarios captured with a push-broom 2D laser, our technique infills approximately 20 m2 of missing surface area for each removed object. Our reconstructions median error ranges between 5.64 cm - 9.24 cm with standard deviations between 4.57 cm - 6.08 cm.},
}

@InProceedings{rao2016icra,
  author        = {D. Rao and A. Bender and S.B. Williams and O. Pizarro},
  title         = {{Multimodal information-theoretic measures for autonomous exploration}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {Visual Learning, Reactive and Sensor-Based Planning, Marine Robotics},
  abstract      = {Autonomous underwater vehicles (AUVs) are widely used to perform information gathering missions in unseen environments. Given the sheer size of the ocean environment, and the time and energy constraints of an AUV, it is important to consider the potential utility of candidate missions when performing survey planning. In this paper, we utilise a multimodal learning approach to capture the relationship between in-situ visual observations, and shipborne bathymetry (ocean depth) data that are freely available a priori. We then derive information-theoretic measures under this model that predict the amount of visual information gain at an unobserved location based on the bathymetric features. Unlike previous approaches, these measures consider the value of additional visual features, rather than just the habitat labels obtained. Experimental results with a toy dataset and real marine data demonstrate that the approach can be used to predict the true utility of unexplored areas.},
}

@InProceedings{held2016icra,
  author        = {D. Held and S. Thrun and S. Savarese},
  title         = {{Robust Single-View Instance Recognition}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {Visual Learning, Computer Vision for Other Robotic Applications, Recognition},
  abstract      = {Some robots must repeatedly interact with a fixed set of objects in their environment. To operate correctly, it is helpful for the robot to be able to recognize the object instances that it repeatedly encounters. However, current methods for recognizing object instances require that, during training, many pictures are taken of each object from a large number of viewing angles. This procedure is slow and requires much manual effort before the robot can begin to operate in a new environment. We have developed a novel procedure for training a neural network to recognize a set of objects from just a single training image per object. To obtain robustness to changes in viewpoint, we take advantage of a supplementary dataset in which we observe a separate (non-overlapping) set of objects from multiple viewpoints. After pre-training the network in a novel multi-stage fashion, the network can robustly recognize new object instances given just a single training image of each object. If more images of each object are available, the performance improves. We perform a thorough analysis comparing our novel training procedure to traditional neural network pre-training techniques as well as previous stateof-the-art approaches including keypoint-matching, templatematching, and sparse coding, and we demonstrate that our method significantly outperforms these previous approaches. Our method can thus be used to easily teach a robot to recognize a novel set of object instances from unknown viewpoints.},
}

@InProceedings{velas2016icra,
  author        = {M. Velas and M. Spanel and A. Herout},
  title         = {{Collar Line Segments for Fast Odometry Estimation from Velodyne Point Clouds}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {Range Sensing, Mapping, SLAM},
  abstract      = {We present a novel way of odometry estimation from Velodyne LiDAR point cloud scans. The aim of our work is to overcome the most painful issues of Velodyne data the sparsity and the quantity of data points in an efficient way, enabling more precise registration. Alignment of the point clouds which yields the final odometry is based on random sampling of the clouds using Collar Line Segments (CLS). The closest line segment pairs are identified in two sets of line segments obtained from two consequent Velodyne scans. From each pair of correspondences, a transformation aligning the matched line segments into a 3D plane is estimated. By this, significant planes (ground, walls, . . . ) are preserved among aligned point clouds. Evaluation using the KITTI dataset shows that our method outperforms publicly available and commonly used state-of-the-art method GICP for point cloud registration in both accuracy and speed, especially in cases where the scene lacks significant landmarks or in typical urban elements. For such environments, the registration error of our method is reduced by 75\% compared to the original GICP error.},
}

@InProceedings{greene2016icra,
  author        = {W.N. Greene and K. Ok and P. Lommel and N. Roy},
  title         = {{Multi-Level Mapping: Real-Time Dense Monocular SLAM}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {SLAM, Visual-Based Navigation, RGB-D Perception},
  abstract      = {We present a method for Simultaneous Localization and Mapping (SLAM) using a monocular camera that is capable of reconstructing dense 3D geometry online without the aid of a graphics processing unit (GPU). Our key contribution is a multi-resolution depth estimation and spatial smoothing process that exploits the correlation between low-texture image regions and simple planar structure to adaptively scale the complexity of the generated keyframe depthmaps to the texture of the input imagery. High-texture image regions are represented at higher resolutions to capture fine detail, while low-texture regions are represented at coarser resolutions for smooth surfaces. The computational savings enabled by this approach allow for significantly increased reconstruction density and quality when compared to the state-ofthe-art. The increased depthmap density also improves tracking performance as more constraints can contribute to the pose estimation. A video of experimental results is available at http:// groups.csail.mit.edu/rrg/multi_level_mapping.},
}

@InProceedings{mentges2016icra,
  author        = {G. Mentges and R. Grigat},
  title         = {{Surface Reconstruction from Image Space Adjacency of Lines Using Breadth-First Plane Search}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {Mapping, Computer Vision for Automation},
  abstract      = {In this paper, we propose a novel multi-view method for surface reconstruction from matched line segments with applications to robotic mapping and image-based rendering. Starting from 3D line segments and poses obtained via Line-SLAM, we project segments from multiple frames into keyframes for image-space analysis. For each keyframe, a grid of image faces is created by optimized intersection of the segment projection lines. These faces define a segment adjacency graph, wherein we perform our Breadth-First Plane Search (BFPS). The found plane hypotheses are merged maximally with respect to a structure-preserving criterion by growing coplanar regions across the graph. Hypotheses violating the visibility constraint are discarded based on fast per-face and mostly nongeometrical evaluation of the scene and image graph. Finally, each image face gets back-projected onto an optimal plane to obtain a 3D surface model. The presented system is a complete and automatic solution suitable for mapping an environment in real-time scenarios like robotic exploration. We demonstrate the performance of our algorithm on several indoor scenes of varying complexity. Compared to a pure 3D analysis of segments, we see a speed-up by one to almost two orders of magnitude, while still improving on reconstruction accuracy.},
}

@InProceedings{chen2016icra,
  author        = {Y. Chen and M. Liu and J.P. How},
  title         = {{Augmented Dictionary Learning for Sparse Representations of Trajectories with Application to Motion Prediction}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {Autonomous Vehicle Navigation, Learning and Adaptive Systems, Robot Safety},
  abstract      = {Developing accurate models and efficient representations of multivariate trajectories is important for understanding the behavior patterns of mobile agents. This work presents a dictionary learning algorithm for developing a partbased trajectory representation, which combines merits of the existing Markovian-based and clustering-based approaches. In particular, this work presents the augmented semi-nonnegative sparse coding (ASNSC) algorithm for solving a constrained dictionary learning problem, and shows that the proposed method would converge to a local optimum given a convexity condition. We consider a trajectory modeling application, in which the learned dictionary atoms correspond to local motion patterns. Classical semi-nonnegative sparse coding approaches would add dictionary atoms with opposite signs to reduce the representational error, which can lead to learning noisy dictionary atoms that correspond poorly to local motion patterns. ASNSC addresses this problem and learns a concise set of intuitive motion patterns. ASNSC shows significant improvement over existing trajectory modeling methods in both prediction accuracy and computational time, as revealed by extensive numerical analysis on real datasets.},
}

@InProceedings{lee2016icra,
  author        = {S.U. Lee and R. GONZALEZ and K. Iagnemma},
  title         = {{Robust Sampling-Based Motion Planning for Autonomous Tracked Vehicles in Deformable High Slip Terrain}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {Motion and Path Planning, Autonomous Vehicle Navigation, Field Robots},
  abstract      = {This paper presents an optimal global planner for autonomous tracked vehicles navigating in off-road terrain with uncertain slip, which affects the vehicle as a process noise. This paper incorporates two fields of study: slip estimation and motion planning. For slip estimation, an experimental result from [9] is used to model the effect of the slip on the vehicle in various soil types. For motion planning, a robust incremental sampling based motion planning algorithm (CC-RRT*) is combined with the LQG-MP algorithm. CC-RRT* yields the optimal and probabilistically feasible trajectory by using a chance constrained approach under the RRT* framework. LQG-MP provides the capability of considering the role of compensator in the motion planning phase and bounds the degree of uncertainty to appropriate size. In simulation, the planner successfully finds the optimal and robust solution. In addition, the planner is compared with an RRT* algorithm with dilated obstacles to show that it avoids being overly conservative.},
}

@InProceedings{beksi2016icra,
  author        = {W. Beksi and N. Papanikolopoulos},
  title         = {{3D Point Cloud Segmentation Using Topological Persistence}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {RGB-D Perception, Object detection, Segmentation, Categorization, Computational Geometry},
  abstract      = {In this paper, we present an approach to segment 3D point cloud data using ideas from persistent homology theory. The proposed algorithms first generate a simplicial complex representation of the point cloud dataset. Next, we compute the zeroth homology group of the complex which corresponds to the number of connected components. Finally, we extract the clusters of each connected component in the dataset. We show that this technique has several advantages over state of the art methods such as the ability to provide a stable segmentation of point cloud data under noisy or poor sampling conditions and its independence of a fixed distance metric.},
}

@InProceedings{lottes2016icra,
  author        = {P. Lottes and H. Markus and S. Sander and M. Matthias and S.L. Peter and C. Stachniss},
  title         = {{An Effective Classification System for Separating Sugar Beets and Weeds for Precision Farming Applications}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {Robotics in Agriculture and Forestry, Semantic Scene Understanding},
  abstract      = {Robots for precision farming have the potential to reduce the reliance on herbicides and pesticides through selectively spraying individual plants or through manual weed removal. To achieve this, the value crops and the weeds must be identified by the robots perception system to trigger the actuators for spraying or removal. In this paper, we address the problem of detecting the sugar beet plants as well as weeds using a camera installed on a mobile robot operating on a field. We propose a system that performs vegetation detection, feature extraction, random forest classification, and smoothing through a Markov random field to obtain an accurate estimate of the crops and weeds. We implemented and thoroughly evaluated our system on a real farm robot on different sugar beet fields and illustrate that our approach allows for accurately identifying the weed on the field.},
  url           = {http://flourish-project.eu/fileadmin/user_upload/publications/lottes16icra.pdf},
}

@InProceedings{corcoran2016icra,
  author        = {P. Corcoran and G. Huang and P. Mooney},
  title         = {{Unsupervised Trajectory Compression}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {Intelligent Transportation Systems, Motion and Path Planning},
  abstract      = {We present a method for compressing trajectories in an unsupervised manner. Given a set of trajectories sampled from a space we construct a basis for compression whose elements correspond to paths in the space which are topologically distinct. This is achieved by computing a canonical representative for each element in a generating set for the first homology group and decomposing these representatives into a set of distinct paths. Trajectory compression is subsequently accomplished through representation in terms of this basis. Robustness with respect to outliers is achieved by only considering those elements of the first homology group which exist in the super-level sets of the Kernel Density Estimation (KDE) above a threshold. Robustness with respect to small scale topological artifacts is achieved by only considering those elements of the first homology group which exist for a sufficient range in the super-level sets. We demonstrate this approach to trajectory compression in the context of a large set of crowd-sourced GPS trajectories captured in the city of Chicago. On this set, the compression method achieves a mean geometrical accuracy of 108 meters with a compression ratio of over 12.},
}

@InProceedings{martn-martn2016icra,
  author        = {R. Mart{\'i}n-Mart{\'i}n and S. H{\"o}fer and O. Brock},
  title         = {{An Integrated Approach to Visual Perception of Articulated Objects}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {RGB-D Perception, Visual Tracking, Object detection, Segmentation, Categorization},
  abstract      = {We present an integrated approach for perception of unknown articulated objects. To robustly perceive objects and understand interactions, our method tightly integrates pose tracking, shape reconstruction, and the estimation of their kinematic structure. The key insight of our method is that these sub-problems complement each other: for example, tracking is greatly facilitated by knowing the shape of the object, whereas the shape and the kinematic structure can be more easily reconstructed if the motion of the object is known. Our combined method leverages these synergies to improve the performance of perception. We analyze the proposed method in average cases and difficult scenarios using a variety of rigid and articulated objects. The results show that our integrated solution achieves better results than solutions for the individual problems. This demonstrates the benefits of approaching robot perception problems in an integrated manner.},
}

@InProceedings{duggal2016icra,
  author        = {V. Duggal and M. Sukhwani and K. Bipin and M. Krishna and S.R. Gunnamreddy},
  title         = {{Plantation Monitoring and Yield Estimation Using Autonomous Quadcopter for Precision Agriculture}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {Robotics in Agriculture and Forestry, Agricultural Automation},
  abstract      = {Recently, quadcopters with their advance sensors and imaging capabilities have become an imperative part of the precision agriculture. In this work, we have described a framework which performs plantation monitoring and yield estimation using the supervised learning approach, while autonomously navigating through an inter-row path of the plantation. The proposed navigation framework assists the quadcopter to follow a sequence of collision-free GPS way points and has been integrated with ROS (Robot Operating System). The trajectory planning and control module of the navigation framework employ convex programming techniques to generate minimum time trajectory between way-points and produces appropriate control inputs for the quadcopter. A new pomegranate dataset comprising of plantation surveillance video and annotated frames capturing the varied stages of pomegranate growth along with the navigation framework are being delivered as a part of this work.},
}

@InProceedings{narr2016icra,
  author        = {A. Narr and R. Triebel and D. Cremers},
  title         = {{Stream-Based Active Learning for Efficient and Adaptive Classification of 3D Objects}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {Learning and Adaptive Systems, Object detection, Segmentation, Categorization, Semantic Scene Understanding},
  abstract      = {We present a new Active Learning approach for classifying objects from streams of 3D point cloud data. The major problems here are the non-uniform occurence of class instances and the unbalanced numbers of samples per class. We show that standard online learning methods based on decision trees perform comparably bad for such data streams, which are however particularly relevant for mobile robots that need to learn semantics persistently. To address this, we use Mondrian forests (MF), a recent online learning algorithm that is independent on the data order. We present an extension of that algorithm and show that MF are less overconfident than standard Random Forests. In experiments on the KITTI benchmark, we show that this leads to a substantially improved classification performance for data streams, rendering our approach very attractive for lifelong robot learning applications. Fig. 1. Example situation from the KITTI benchmark data set with a car, a cyclist and some pedestrians. The 3D point cloud data of this frame was classified after actively learning semantics from a stream of 10 000 previous samples. The bottom left image shows the result from actively learning an online Random Forest classifier, the right one shows the result using a modified Mondrian forest instead. A green bounding box refers to a correct classification, while red boxes are wrong predictions. As we show in this paper, the better performance of Mondrian forests comes from their higher capability to learn from streams of data.},
}

@InProceedings{viswanathan2016icra,
  author        = {A. Viswanathan and B. Pires and D. Huber},
  title         = {{Vision-Based Robot Localization across Seasons and in Remote Locations}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {Localization, Field Robots, Computer Vision for Other Robotic Applications},
  abstract      = {This paper studies the problem of GPS-denied unmanned ground vehicle (UGV) localization by matching ground images to a satellite map. We examine the realistic, but particularly challenging problem of navigation in remote areas using maps that may correspond to a different season of the year. The problem is difficult due to the limited UGV sensor horizon, the drastic shift in perspective between ground and aerial views, the absence of discriminative features in the environment due to the remote location, and the high variation in appearance of the satellite map caused by the change in seasons. We present an approach to image matching using semantic information that is invariant to seasonal change. This semantics-based matching is incorporated into a particle filter framework and successful localization of the ground vehicle is demonstrated for satellite maps captured in summer, spring, and winter.},
}

@InProceedings{fehr2016icra,
  author        = {M. Fehr and M.T. Dymczyk and S. Lynen and R. Siegwart},
  title         = {{Reshaping Our Model of the World Over Time}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {Computer Vision for Other Robotic Applications, Mapping, Visual-Based Navigation},
  abstract      = {An accurate estimate of the 3D-structure in the environment is key to robotic applications such as autonomous inspection, obstacle avoidance and manipulation. Recent years have seen substantial algorithmic advances towards creating highly accurate models of small objects as well as large scale architectural structures. Most commonly a rich set of images covering a static scene are used to jointly estimate the pose of the cameras and the observed 3D-structure. For many practical application however the assumption of static scenes and sufficient coverage by images does not hold. In fact for industrial inspection the change in the scene is of most interest and the limited resources on mobile platforms dont allow for extensive data captures. In this paper we investigate the potential of combining multiple independent captures of a place to selectively reconstruct a scene over time. We propose an incremental reconstruction algorithm which identifies and fuses novel data into a joint model of the scene. Being able to identify changing parts of the scene is particularly interesting for mobile applications where bandwidth, storage and processing power are limited. Through detailed experiments, we show the potential of our approach to use multiple mobile devices to reconstruct and update a model of the static part of the environment over time.},
}

@InProceedings{cieslewski2016icra,
  author        = {T. Cieslewski and E. Stumm and A.R. Gawel and M. Bosse and S. Lynen and R. Siegwart},
  title         = {{Point Cloud Descriptors for Place Recognition Using Sparse Visual Information}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {Localization, Visual-Based Navigation, SLAM},
  abstract      = {Place recognition is a core component in simultaneous localization and mapping (SLAM), limiting positional drift over space and time to unlock precise robot navigation. Determining which previously visited places belong together continues to be a highly active area of research as robotic applications demand increasingly higher accuracies. A large number of place recognition algorithms have been proposed, capable of consuming a variety of sensor data including laser, sonar and depth readings. The best performing solutions, however, have utilized visual information by either matching entire images or parts thereof. Most commonly, vision based approaches are inspired by information retrieval and utilize 3D-geometry information about the observed scene as a postverification step. In this paper we propose to use the 3D-scene information from sparse-visual feature maps directly at the core of the place recognition pipeline. We propose a novel structural descriptor which aggregates sparse triangulated landmarks from SLAM into a compact signature. The resulting 3Dfeatures provide a discriminative fingerprint to recognize places over seasonal and viewpoint changes which are particularly challenging for approaches based on sparse visual descriptors. We evaluate our system on publicly available datasets and show how its complementary nature can provide an improvement over visual place recognition.},
}

@InProceedings{handa2016icra,
  author        = {A. Handa and V. Patraucean and S. Stent and R. Cipolla},
  title         = {{SceneNet: An Annotated Model Generator for Indoor Scene Understanding}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {Semantic Scene Understanding, RGB-D Perception, Simulation and Animation},
  abstract      = {We introduce SceneNet, a framework for generating high-quality annotated 3D scenes to aid indoor scene understanding. SceneNet leverages manually-annotated datasets of real world scenes such as NYUv2 to learn statistics about object co-occurrences and their spatial relationships. Using a hierarchical simulated annealing optimisation, these statistics are exploited to generate a potentially unlimited number of new annotated scenes, by sampling objects from various existing databases of 3D objects such as ModelNet, and textures such as OpenSurfaces and ArchiveTextures. Depending on the task, SceneNet can be used directly in the form of annotated 3D models for supervised training and 3D reconstruction benchmarking, or in the form of rendered annotated sequences of RGB-D frames or videos.},
}

@InProceedings{mukadam2016icra,
  author        = {M. Mukadam and X. Yan and B. Boots},
  title         = {{Gaussian Process Motion Planning}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {Motion and Path Planning, Probability and Statistical Methods},
  abstract      = {Motion planning is a fundamental tool in robotics, used to generate collision-free, smooth, trajectories, while satisfying task-dependent constraints. In this paper, we present a novel approach to motion planning using Gaussian processes. In contrast to most existing trajectory optimization algorithms, which rely on a discrete state parameterization in practice, we represent the continuous-time trajectory as a sample from a Gaussian process (GP) generated by a linear time-varying stochastic differential equation. We then provide a gradientbased optimization technique that optimizes continuous-time trajectories with respect to a cost functional. By exploiting GP interpolation, we develop the Gaussian Process Motion Planner (GPMP), that finds optimal trajectories parameterized by a small number of states. We benchmark our algorithm against recent trajectory optimization algorithms by solving 7-DOF robotic arm planning problems in simulation and validate our approach on a real 7-DOF WAM arm.},
}

@InProceedings{pokorny2016icra,
  author        = {F.T. Pokorny and K. Goldberg and D. Kragic},
  title         = {{Topological Trajectory Clustering with Relative Persistent Homology}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {Computational Geometry, Motion and Path Planning, Learning and Adaptive Systems},
  abstract      = {Cloud Robotics techniques based on Learning from Demonstrations suggest promising alternatives to manual programming of robots and autonomous vehicles. One challenge is that demonstrated trajectories may vary dramatically: it can be very difficult, if not impossible, for a system to learn control policies unless the trajectories are clustered into meaningful consistent subsets. Metric clustering methods, based on a distance measure, require quadratic time to compute a pairwise distance matrix and do not naturally distinguish topologically distinct trajectories. This paper presents an algorithm for topological clustering based on relative persistent homology, which, for a fixed underlying simplicial representation and discretization of trajectories, requires only linear time in the number of trajectories. The algorithm incorporates global constraints formalized in terms of the topology of sublevel or superlevel sets of a function and can be extended to incorporate probabilistic motion models. In experiments with real automobile and ship GPS trajectories as well as pedestrian trajectories extracted from video, the algorithm clusters trajectories into meaningful consistent subsets and, as we show in an experiment with ship trajectories, results in a faster and more efficient clustering than a metric clustering by Frechet distance.},
}

@InProceedings{wang2016icra,
  author        = {R. Wang and M. Veloso and S. Seshan},
  title         = {{Active Sensing Data Collection with Autonomous Mobile Robots}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {Reactive and Sensor-Based Planning, Automation Technologies for Smart Cities, Surveillance Systems},
  abstract      = {With the introduction of autonomous robots that help perform various tasks in our environments, we can opportunistically use them for collecting fine-grain sensor measurements about our surroundings. Use of mobile robots for data collection scales much better than static sensors in terms of number of measurement locations and provide more fine-grain accuracy and reliability than alternate human crowd-sourcing efforts. One of the unique features of mobile robots is the ability to control and direct where and when measurements should be collected. In this paper, we present a system to compute paths for the robot to follow that incorporates the robots limited expected deployment time, expected measurement value at each location, and a history of when each location was last visited.},
}

@InProceedings{zhang2016icra-lbfo,
  author        = {G. Zhang and J.M. Lilly and P. Vela},
  title         = {{Learning Binary Features Online from Motion Dynamics for Incremental Loop-Closure Detection and Place Recognition}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {Visual-Based Navigation, Recognition, SLAM},
  abstract      = {This paper proposes a simple yet effective approach to learn visual features online for improving loopclosure detection and place recognition, based on bag-of-words frameworks. The approach learns a codeword in the bagof-words model from a pair of matched features from two consecutive frames, such that the codeword has temporallyderived perspective invariance to camera motion. The learning algorithm is efficient: the binary descriptor is generated from the mean image patch, and the mask is learned based on discriminative projection by minimizing the intra-class distances among the learned feature and the two original features. A codeword is generated by packaging the learned descriptor and mask, with a masked Hamming distance defined to measure the distance between two codewords. The geometric properties of the learned codewords are then mathematically justified. In addition, hypothesis constraints are imposed through temporal consistency in matched codewords, which improves precision. The approach, integrated in an incremental bag-of-words system, is validated on multiple benchmark data sets and compared to state-of-the-art methods. Experiments demonstrate improved precision/recall outperforming state of the art with little loss in runtime.},
}

@InProceedings{dewan2016icra,
  author        = {A. Dewan and T. Caselitz and G.D. Tipaldi and W. Burgard},
  title         = {{Motion-Based Detection and Tracking in 3D LiDAR Scans}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {Range Sensing, Object detection, Segmentation, Categorization, Autonomous Vehicle Navigation},
  abstract      = {Robots are expected to operate autonomously in increasingly complex scenarios such as crowded streets or heavy traffic situations. Perceiving the dynamics of moving objects in the environment is crucial for safe and smart navigation and therefore a key enabler for autonomous driving. In this paper we present a novel model-free approach for detecting and tracking dynamic objects in 3D LiDAR scans obtained by a moving sensor. Our method only relies on motion cues and does not require any prior information about the objects. We sequentially detect multiple motions in the scene and segment objects using a Bayesian approach. For robustly tracking objects, we utilize their estimated motion models. We present extensive quantitative results based on publicly available datasets and show that our approach outperforms the state of the art.},
}

@InProceedings{gomez-ojeda2016icra,
  author        = {R. Gomez-Ojeda and J. Gonzlez-Jimnez},
  title         = {{Robust Stereo Visual Odometry through a Probabilistic Combination of Points and Line Segments}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {Autonomous Vehicle Navigation, Computer Vision for Other Robotic Applications, Localization},
  abstract      = {Most approaches to stereo visual odometry reconstruct the motion based on the tracking of point features along a sequence of images. However, in low-textured scenes it is often difficult to encounter a large set of point features, or it may happen that they are not well distributed over the image, so that the behavior of these algorithms deteriorates. This paper proposes a probabilistic approach to stereo visual odometry based on the combination of both point and line segment that works robustly in a wide variety of scenarios. The camera motion is recovered through non-linear minimization of the projection errors of both point and line segment features. In order to effectively combine both types of features, their associated errors are weighted according to their covariance matrices, computed from the propagation of Gaussian distribution errors in the sensor measurements. The method, of course, is computationally more expensive that using only one type of feature, but still can run in real-time on a standard computer and provides interesting advantages, including a straightforward integration into any probabilistic framework commonly employed in mobile robotics.},
}

@InProceedings{bose2016icra,
  author        = {L. Bose and A. Richards},
  title         = {{Fast Depth Edge Detection and Edge Based RGB-D SLAM}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {SLAM, Mapping, Visual Tracking},
  abstract      = {This paper presents a method of occluding depth edge-detection targeted towards RGB-D video streams and explores the use of these and other edge features in RGB-D SLAM. The proposed depth edge-detection approach uses prior information obtained from the previous RGB-D video frame to determine which areas of the current depth image are likely to contain edges due to image similarity. By limiting the search for edges to these areas a significant amount of computation time is saved compared to searching the entire image. Pixels belonging to both the depth and colour edges of an RGB-D image can be back projected using the depth component to form 3D point clouds of edge points. Registration between such edge point clouds is achieved using ICP and we present a realtime RGB-D SLAM system utilizing such back projected edge features. Experimental results are presented demonstrating the performance of both the proposed depth edge-detection and SLAM system using publicly available datasets.},
}

@InProceedings{jaimez2016icra,
  author        = {M. Jaimez and J.G. Monroy and J. Gonzlez-Jimnez},
  title         = {{Planar Odometry from a Radial Laser Scanner. A Range Flow-Based Approach}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {Range Sensing, Localization},
  abstract      = {In this paper we present a fast and precise method to estimate the planar motion of a lidar from consecutive range scans. For every scanned point we formulate the range flow constraint equation in terms of the sensor velocity, and minimize a robust function of the resulting geometric constraints to obtain the motion estimate. Conversely to traditional approaches, this method does not search for correspondences but performs dense scan alignment based on the scan gradients, in the fashion of dense 3D visual odometry. The minimization problem is solved in a coarse-to-fine scheme to cope with large displacements, and a smooth filter based on the covariance of the estimate is employed to handle uncertainty in unconstraint scenarios (e.g. corridors). Simulated and real experiments have been performed to compare our approach with two prominent scan matchers and with wheel odometry. Quantitative and qualitative results demonstrate the superior performance of our approach which, along with its very low computational cost (0.9 milliseconds on a single CPU core), makes it suitable for those robotic applications that require planar odometry. For this purpose, we also provide the code so that the robotics community can benefit from it.},
}

@InProceedings{ma2016icra,
  author        = {L. Ma and C. Kerl and J. St{\"u}ckler and D. Cremers},
  title         = {{CPA SLAM: Consistent Plane-Model Alignment for Direct RGB-D SLAM}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {SLAM, RGB-D Perception, Semantic Scene Understanding},
  abstract      = {Planes are predominant features of man-made environments which have been exploited in many mapping approaches. In this paper, we propose a real-time capable RGB-D SLAM system that consistently integrates frame-tokeyframe and frame-to-plane alignment. Our method models the environment with a global plane model and besides direct image alignment it uses the planes for tracking and global graph optimization. This way, our method makes use of the dense image information available in keyframes for accurate short-term tracking. At the same time it uses a global model to reduce drift. Both components are integrated consistently in an expectation-maximization framework. In experiments, we demonstrate the benefits our approach and its state-of-the-art accuracy on challenging benchmarks.},
}

@InProceedings{mount2016icra,
  author        = {J. Mount and M.J. Milford},
  title         = {{2D Visual Place Recognition for Domestic Service Robots at Night}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {Localization, Service Robots, Visual-Based Navigation},
  abstract      = {Domestic service robots such as lawn mowing and vacuum cleaning robots are the most numerous consumer robots in existence today. While early versions employed random exploration, recent systems fielded by most of the major manufacturers have utilized range-based and visual sensors and user-placed beacons to enable robots to map and localize. However, active range and visual sensing solutions have the disadvantages of being intrusive, expensive, or only providing a 1D scan of the environment, while the requirement for beacon placement imposes other practical limitations. In this paper we present a passive and potentially cheap vision-based solution to 2D localization at night that combines easily obtainable day-time maps with low resolution contrast-normalized image matching algorithms, image sequence-based matching in two-dimensions, place match interpolation and recent advances in conventional low light camera technology. In a range of experiments over a domestic lawn and in a lounge room, we demonstrate that the proposed approach enables 2D localization at night, and analyse the effect on performance of varying odometry noise levels, place match interpolation and sequence matching length. Finally we benchmark the new low light camera technology and show how it can enable robust place recognition even in an environment lit only by a moonless sky, raising the tantalizing possibility of being able to apply all conventional vision algorithms, even in the darkest of nights.},
}

@InProceedings{radwan2016icra,
  author        = {N. Radwan and G.D. Tipaldi and L. Spinello and W. Burgard},
  title         = {{Do You See the Bakery? Leveraging Geo-Referenced Texts for Global Localization in Public Maps}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {Localization, Visual-Based Navigation, Recognition},
  abstract      = {Text is one of the richest sources of information in an urban environment. Although textual information is heavily relied on by humans for a majority of the daily tasks, its usage has not been completely exploited in the field of robotics. In this work, we propose a localization approach utilizing textual features in urban environments. Starting at an unknown location, equipped with an RGB-camera and a compass, our approach uses off-the-shelf text extraction methods to identify text labels in the vicinity. We then apply a probabilistic localization approach with specific sensor models to integrate multiple observations. An extensive evaluation with real-world data gathered in different cities reveals an improvement over GPS-based localization when using our method.},
}

@InProceedings{wojke2016icra,
  author        = {N. Wojke and D. Paulus},
  title         = {{Global Data Association for the Probability Hypothesis Density Filter Using Network Flows}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {Visual Tracking, Human detection & tracking, RGB-D Perception},
  abstract      = {The Probability Hypothesis Density (PHD) filter is an efficient formulation of multi-target state estimation that circumvents the combinatorial explosion of the multitarget posterior by operating on single-target space without maintaining target identities. In this paper, we propose a multi-target tracker based on the PHD filter that provides instantaneous state estimation and delayed decision on data association. For this purpose, we reformulate the PHD recursion in terms of single-target track hypotheses and solve a mincost flow network for trajectory estimation where measurement likelihoods and transition probabilities are based on multitarget state estimates. In this manner, the presented approach combines global data association with efficient multi-target filtering. We evaluate the approach on a publicly available pedestrian tracking dataset to present state estimation and data association capabilities.},
}

@InProceedings{chen2016icra-pfrr,
  author        = {M. Chen and E. Frazzoli and D. Hsu and W.S. Lee},
  title         = {{POMDP-lite for Robust Robot Planning under Uncertainty}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {Motion and Path Planning, AI Reasoning Methods},
  abstract      = {The partially observable Markov decision process (POMDP) provides a principled general model for planning under uncertainty. However, solving a general POMDP is computationally intractable in the worst case. This paper introduces POMDP-lite, a subclass of POMDPs in which the hidden state variables are constant or only change deterministically. We show that a POMDP-lite is equivalent to a set of fully observable Markov decision processes indexed by a hidden parameter and is useful for modeling a variety of interesting robotic tasks. We develop a simple model-based Bayesian reinforcement learning algorithm to solve POMDP-lite models. The algorithm performs well on large-scale POMDP-lite models with up to 1020 states and outperforms the state-of-the-art general-purpose POMDP algorithms. We further show that the algorithm is near-Bayesian-optimal under suitable conditions.},
}

@InProceedings{mendes2016icra,
  author        = {C.C.T. Mendes and V. Fremont and D.F. Wolf},
  title         = {{Exploiting Fully Convolutional Neural Networks for Fast Road Detection}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {Semantic Scene Understanding, Computer Vision for Transportation, Visual-Based Navigation},
  abstract      = {Road detection is a crucial task in autonomous navigation systems. It is responsible for delimiting the road area and hence the free and valid space for maneuvers. In this paper, we consider the visual road detection problem where, given an image, the objective is to classify every of its pixels into road or non-road. We address this task by proposing a convolutional neural network architecture. We are especially interested in a model that takes advantage of a large contextual window while maintaining a fast inference. We achieve this by using a Network-in-Network (NiN) architecture and by converting the model into a fully convolutional network after training. Experiments have been conducted to evaluate the effects of different contextual window sizes (the amount of contextual information) and also to evaluate the NiN aspect of the proposed architecture. Finally, we evaluated our approach using the KITTI road detection benchmark achieving results in line with other state-of-the-art methods while maintaining real-time inference. The benchmark results also reveal that the inference time of our approach is unique at this level of accuracy, being two orders of magnitude faster than other methods with similar performance.},
}

@InProceedings{yang2016icra,
  author        = {S. Yang and D. Maturana and S. Scherer},
  title         = {{Real-Time 3D Scene Layout from a Single Image Using Convolutional Neural Networks}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {Semantic Scene Understanding, Visual Learning, Autonomous Vehicle Navigation, CNN},
  abstract      = {We consider the problem of understanding the 3D layout of indoor corridor scenes from a single image in real time. Identifying obstacles such as walls is essential for robot navigation, but also challenging due to the diversity in structure, appearance and illumination of real-world corridor scenes. Many current single-image methods make Manhattanworld assumptions, and break down in environments that do not meet this mold. They also may require complicated handdesigned features for image segmentation or clear boundaries to form certain building models. In addition, most cannot run in real time. In this paper, we propose to combine machine learning with geometric modelling to build a simplified 3D model from a single image. We first employ a supervised Convolutional Neural Network (CNN) to provide a dense, but coarse, geometric class labelling of the scene. We then refine this labelling with a fully connected Conditional Random Field (CRF). Finally, we fit line segments along wall-ground boundaries and pop up a 3D model using geometric constraints. We assemble a dataset of 967 labelled corridor images. Our experiments on this dataset and another publicly available dataset show our method outperforms other single image scene understanding methods in pixelwise accuracy while labelling images at over 15 Hz.},
}

@InProceedings{sun2016icra,
  author        = {Y. Sun and D. Fox},
  title         = {{NEOL: Toward Never-Ending Object Learning for Robots}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {Recognition, Cognitive Human-Robot Interaction, Learning and Adaptive Systems},
  abstract      = {Learning to recognize objects based on names is a crucial capability for personal robots. Recent recognition methods successfully learn to recognize objects in a train-oncethen-test setting. Yet, these methods do not apply readily to robotic settings, where a robot might continuously encounter new objects and new names. In this work, we present a framework for Never-Ending Object Learning (NEOL). Our framework automatically learns to organize object names into a semantic hierarchy using crowdsourcing and background knowledge bases. It then uses the hierarchy to improve the consistency and efficiency of annotating objects. It also adapts information from additional image datasets to learn object classifiers from a very small number of training examples. We present experiments to test the performance of the adaptation method and demonstrate the full system in a never-ending object learning experiment.},
}

@InProceedings{ghafarianzadeh2016icra,
  author        = {M. Ghafarianzadeh and M. Blaschko and G. Sibley},
  title         = {{Efficient, Dense, Object-Based Segmentation from RGBD Video}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {Object detection, Segmentation, Categorization},
  abstract      = {Spatio-temporal cues offer a rich source of information for inferring structural and semantic scene properties. A particularly useful representation in computer vision is a spatiotemporal video segmentation. Together with motion, knowledge of depth can substantially improve superpixel segmentation. In this work we present a novel framework for spatio-temporal segmentation from RGBD video. The method employs both low-level (intensity, color) and high-level (deformable parts model) appearance features. Motion is incorporated through the use of optical flow to construct the temporal connections in the graph Laplacian. Depth cues are incorporated in the similarity metric to provide an informative cue for object boundaries at depth disparities. Nave application of spectral clustering to dense spatio-temporal graphs leads to a high computational cost that is typically addressed through the use of GPUs or computer clusters. By contrast, we build upon a recently proposed Nystrom approximation strategy for spatiotemporal clustering that enables computation on a single core. We further explore structured local connectivity patterns to give high performance at low computational cost. Also we propose a novel context-aware aggregation method that uses a deformable parts model to group the detected parts of the object as a single segment with an accurate boundary. Detailed experiments on the NYU Depth Dataset and TUM RGBD Dataset is performed to compare against previous large-scale graph-based spatiotemporal segmentation techniques which shows the substantial performance advantages of our framework.},
}

@InProceedings{tourani2016icra,
  author        = {S. Tourani and S. Mittal and A. Nagariya and V. Chari and M. Krishna},
  title         = {{Rolling Shutter and Motion Blur Removal for Depth Cameras}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {RGB-D Perception},
  abstract      = {Structured light range sensors (SLRS) like the Microsoft Kinect have electronic rolling shutters (ERS). The output of such a sensor while in motion is subject to significant motion blur (MB) and rolling shutter (RS) distortion. Most robotic literature still does not explicitly model this distortion, resulting in inaccurate camera motion estimation. In RGBD cameras, we show via experimentation that the distortion undergone by depth images is different from that of color images and provide a mathematical model for it. We propose an algorithm that rectifies for these RS and MB distortions. To assess the performance of the algorithm we conduct an extensive set of experiments for each step of the pipeline. We assess the performance of our algorithm by comparing the performance of the rectified images on scene-flow and camera pose estimation, and show that with our proposed rectification, the performance improvement is significant.},
}

@InProceedings{schlosser2016icra,
  author        = {J. Schlosser and C. Chow and Z. Kira},
  title         = {{Fusing LIDAR and Images for Pedestrian Detection Using Convolutional Neural Networks}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {Visual Learning, Sensor Fusion, Computer Vision for Other Robotic Applications, CNN},
  abstract      = {In this paper, we explore various aspects of fusing LIDAR and color imagery for pedestrian detection in the context of convolutional neural networks (CNNs), which have recently become state-of-art for many vision problems. We incorporate LIDAR by up-sampling the point cloud to a dense depth map and then extracting three features representing different aspects of the 3D scene. We then use those features as extra image channels. Specifically, we leverage recent work on HHA [9] (horizontal disparity, height above ground, and angle) representations, adapting the code to work on up-sampled LIDAR rather than Microsoft Kinect depth maps. We show, for the first time, that such a representation is applicable to up-sampled LIDAR data, despite its sparsity. Since CNNs learn a deep hierarchy of feature representations, we then explore the question: At what level of representation should we fuse this additional information with the original RGB image channels? We use the KITTI pedestrian detection dataset for our exploration. We first replicate the finding that region-CNNs (R-CNNs) [8] can outperform the original proposal mechanism using only RGB images, but only if fine-tuning is employed. Then, we show that: 1) using HHA features and RGB images performs better than RGB-only, even without any fine-tuning using large RGB web data, 2) fusing RGB and HHA achieves the strongest results if done late, but, under a parameter or computational budget, is best done at the early to middle layers of the hierarchical representation, which tend to represent midlevel features rather than low (e.g. edges) or high (e.g. object class decision) level features, 3) some of the less successful methods have the most parameters, indicating that increased classification accuracy is not simply a function of increased capacity in the neural network.},
}

@InProceedings{choudhury2016icra,
  author        = {S. Choudhury and J.D. Gammell and T. Barfoot and S. Srinivasa and S. Scherer},
  title         = {Regionally Accelerated Batch Informed Trees (RABIT*): A Framework to Integrate Local Information into Optimal Path Planning},
  booktitle     = icra,
  year          = 2016,
  keywords      = {Motion and Path Planning, Optimization and Optimal Control, Aerial Robotics},
  abstract      = {Sampling-based optimal planners, such as RRT*, almost-surely converge asymptotically to the optimal solution, but have provably slow convergence rates in high dimensions. This is because their commitment to finding the global optimum compels them to prioritize exploration of the entire problem domain even as its size grows exponentially. Optimization techniques, such as CHOMP, have fast convergence on these problems but only to local optima. This is because they are exploitative, prioritizing the immediate improvement of a path even though this may not find the global optimum of nonconvex cost functions. In this paper, we present a hybrid technique that integrates the benefits of both methods into a single search. A key insight is that applying local optimization to a subset of edges likely to improve the solution avoids the prohibitive cost of optimizing every edge in a global search. This is made possible by Batch Informed Trees (BIT*), an informed global technique that orders its search by potential solution quality. In our algorithm, Regionally Accelerated BIT* (RABIT*), we extend BIT* by using optimization to exploit local domain information and find alternative connections for edges in collision and accelerate the search. This improves search performance in problems with difficult-to-sample homotopy classes (e.g., narrow passages) while maintaining almost-sure asymptotic convergence to the global optimum. Our experiments on simulated random worlds and real data from an autonomous helicopter show that on certain difficult problems, RABIT* converges 1.8 times faster than BIT*. Qualitatively, in problems with difficult-to-sample homotopy classes, we show that RABIT* is able to efficiently transform paths to avoid obstacles.},
}

@InProceedings{concha2016icra,
  author        = {A. Concha and G. Loianno and V. Kumar and J. Civera},
  title         = {{Visual-Inertial Direct SLAM}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {SLAM, Mapping, Sensor Fusion},
  abstract      = {The so-called direct visual SLAM methods have shown a great potential in estimating a semidense or fully dense reconstruction of the scene, in contrast to the sparse reconstructions of the traditional feature-based algorithms. In this paper, we propose for the first time a direct, tightly-coupled formulation for the combination of visual and inertial data. Our algorithm runs in real-time on a standard CPU. The processing is split in three threads. The first thread runs at frame rate and estimates the camera motion by a joint non-linear optimization from visual and inertial data given a semidense map. The second one creates a semidense map of high-gradient areas only for camera tracking purposes. Finally, the third thread estimates a fully dense reconstruction of the scene at a lower frame rate. We have evaluated our algorithm in several real sequences with ground truth trajectory data, showing a state-of-the-art performance.},
}

@InProceedings{kendall2016icra,
  author        = {A. Kendall and R. Cipolla},
  title         = {{Modelling Uncertainty in Deep Learning for Camera Relocalization}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {Localization, SLAM, Probability and Statistical Methods},
  abstract      = {We present a robust and real-time monocular six degree of freedom visual relocalization system. We use a Bayesian convolutional neural network to regress the 6-DOF camera pose from a single RGB image. It is trained in an end-to-end manner with no need of additional engineering or graph optimisation. The algorithm can operate indoors and outdoors in real time, taking under 6ms to compute. It obtains approximately 2m and 6 accuracy for very large scale outdoor scenes and 0.5m and 10 accuracy indoors. Using a Bayesian convolutional neural network implementation we obtain an estimate of the models relocalization uncertainty and improve state of the art localization accuracy on a large scale outdoor dataset. We leverage the uncertainty measure to estimate metric relocalization error and to detect the presence or absence of the scene in the input image. We show that the models uncertainty is caused by images being dissimilar to the training dataset in either pose or appearance.},
}

@InProceedings{vivaldini2016icra,
  author        = {K.C.T. Vivaldini and V. Guizilini and M.D.C. Oliveira and T.H. Martinelli and F. Ramos and D.F. Wolf},
  title         = {{Route Planning for Active Classification with UAVs}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {Motion and Path Planning, Agricultural Automation},
  abstract      = {The mapping of agricultural crops by capturing images obtained with UAVs enables fast environmental monitoring and diagnosis in large areas. Airborne monitoring in agriculture can a substantially impacts on the identification of diseases and produce accurate information on affected areas. The problem can be formulated as a classification task on aerial images with significant opportunities to impact other fields. This paper presents an active learning method through route planning for improvements in the knowledge on visited areas and minimization uncertainties about the classification of diseases in crops. Binary Logistic Regression and Gaussian Process were used for the detection of pathologies and map interpolation, respectively. A Bayesian optimization strategy is also proposed for the planning of an informative trajectory, which resulted in a maximized search for affected areas in an initially unknown environment.},
}

@InProceedings{dietrich2016icra,
  author        = {V. Dietrich and D. Chen and K.M. Wurm and G.v. Wichert and P. Ennen},
  title         = {{Probabilistic Multi-Sensor Fusion Based on Signed Distance Functions}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {Sensor Fusion, RGB-D Perception, Mapping},
  abstract      = {In this paper, we present an approach for the probabilistic fusion of 3D sensor measurements. Our fusion algorithm is based on truncated signed distance functions. It explicitly considers the measurement noise by modeling the surface using random variables. Furthermore, our proposed surface model provides an explicit estimation of the spatial uncertainty. The approach can be implemented on a GPU to achieve a high update performance and enable online updates of the model. The approach was evaluated in simulation and using real sensor data. In our experiments, we confirmed that it accurately estimates surfaces from noisy sensor data and that it provides a corresponding estimate of the uncertainty. We could also show that the approach is able to fuse measurements from sensors with different noise characteristics.},
}

@InProceedings{hostettler2016icra,
  author        = {L.O. Hostettler and A. {\"O}zg{\"u}r and S. Lemaignan and P. Dillenbourg and F. Mondada},
  title         = {{Real-Time High-Accuracy 2D Localization with Structured Patterns}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {Localization},
  abstract      = {Building over algorithms previously developed for digital pens, this article introduces a novel 2D localization technique for mobile robots, based on simple printed patterns. This method combines high absolute accuracy (below 0.3mm), unlimited scalability, low computational requirements (the presented open-source implementation runs at above 45Hz on a low-cost microcontroller) and low cost (below 30 per device at prototype stage). The article first presents the underlying algorithms and localization pipeline. It then describes our reference hardware and software implementations, and finally evaluates the performance of this technique for mobile robots.},
}

@InProceedings{linegar2016icra,
  author        = {C. Linegar and W. Churchill and P. Newman},
  title         = {{Made to Measure: Bespoke Landmarks for 24-Hour, All-Weather Localisation with a Camera}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {Visual-Based Navigation, Localization, Computer Vision for Transportation},
  abstract      = {This paper is about camera-only localisation in challenging outdoor environments, where changes in lighting, weather and season cause traditional localisation systems to fail. Conventional approaches to the localisation problem rely on point-features such as SIFT, SURF or BRIEF to associate landmark observations in the live image with landmarks stored in the map; however, these features are brittle to the severe appearance change routinely encountered in outdoor environments. In this paper, we propose an alternative to traditional point-features: we train place-specific linear SVM classifiers to recognise distinctive elements in the environment. The core contribution of this paper is an unsupervised mining algorithm which operates on a single mapping dataset to extract distinct elements from the environment for localisation. We evaluate our system on 205 km of data collected from central Oxford over a period of six months in bright sun, night, rain, snow and at all times of the day. Our experiment consists of a comprehensive N-vs-N analysis on 22 laps of the approximately 10 km route in central Oxford. With our proposed system, the portion of the route where localisation fails is reduced by a factor of 6, from 33.3\% to 5.5\%.},
}

@InProceedings{chhaya2016icra,
  author        = {F. Chhaya and D.R. Narapureddy and S. Upadhyay and V. Chari and M.Z. Zia and M. Krishna},
  title         = {{Monocular Reconstruction of Vehicles: Combining SLAM with Shape Priors}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {Semantic Scene Understanding, SLAM, Visual Tracking},
  abstract      = {Reasoning about objects in images and videos using 3D representations is re-emerging as a popular paradigm in computer vision. Specifically, in the context of scene understanding for roads, 3D vehicle detection and tracking from monocular videos still needs a lot of attention to enable practical applications. Current approaches leverage two kinds of information to deal with the vehicle detection and tracking problem: (1) 3D representations (eg. wireframe models or voxel based or CAD models) for diverse vehicle skeletal structures learnt from data, and (2) classifiers trained to detect vehicles or vehicle parts in single images built on top of a basic feature extraction step. In this paper, we propose to extend current approaches in two ways. First, we extend detection to a multiple view setting. We show that leveraging information given by feature or part detectors in multiple images can lead to more accurate detection results than single image detection. Secondly, we show that given multiple images of a vehicle, we can also leverage 3D information from the scene generated using a unique structure from motion algorithm. This helps us localize the vehicle in 3D, and constrain the parameters of optimization for fitting the 3D model to image data. We show results on the KITTI dataset, and demonstrate superior results compared with recent state-of-theart methods, with upto 14.64 \% improvement in localization error.},
}

@InProceedings{bircher2016icra,
  author        = {A. Bircher and M. Kamel and K. Alexis and H. Oleynikova and R. Siegwart},
  title         = {Receding Horizon "Next-Best-View" Planner for 3D Exploration},
  booktitle     = icra,
  year          = 2016,
  keywords      = {Aerial Robotics, Autonomous Vehicle Navigation, Motion and Path Planning},
  abstract      = {This paper presents a novel path planning algorithm for the autonomous exploration of unknown space using aerial robotic platforms. The proposed planner employs a receding horizon nextbestview scheme: In an online computed random tree it finds the best branch, the quality of which is determined by the amount of unmapped space that can be explored. Only the first edge of this branch is executed at every planning step, while repetition of this procedure leads to complete exploration results. The proposed planner is capable of running online, onboard a robot with limited resources. Its high performance is evaluated in detailed simulation studies as well as in a challenging real world experiment using a rotorcraft micro aerial vehicle. Analysis on the computational complexity of the algorithm is provided and its good scaling properties enable the handling of large scale and complex problem setups.},
}

@InProceedings{alcantarilla2016icra,
  author        = {P.F. Alcantarilla and B. Stenger},
  title         = {{How Many Bits Do I Need for Matching Local Binary Descriptors?}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {Visual Learning, Recognition, SLAM},
  abstract      = {In this paper we provide novel insights about the performance and design of popular pairwise tests-based local binary descriptors with the aim of answering the question: How many bits are needed for matching local binary descriptors? We use the interpretation of binary descriptors as a Locality Sensitive Hashing (LSH) scheme for approximating Kendalls tau rank distance between image patches. Based on this understanding we compare local binary descriptors in terms of the number of bits that are required to achieve a certain performance in feature-based matching problems. Furthermore, we introduce a calibration method to automatically determine a suitable number of bits required in an image matching scenario. We provide a performance analysis in image matching and structure from motion benchmarks, showing calibration results in visual odometry and object recognition problems. Our results show that excellent performance can be achieved using a small fraction of the total number of bits from the whole descriptor, speeding-up matching and reducing storage requirements.},
}

@InProceedings{ursic2016icra,
  author        = {P. Ursic and A. Leonardis and D. Skocaj and M. Kristan},
  title         = {{Hierarchical Spatial Model for 2D Range Data Based Room Categorization}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {Range Sensing, Service Robots},
  abstract      = {The next generation service robots are expected to co-exist with humans in their homes. Such a mobile robot requires an efficient representation of space, which should be compact and expressive, for effective operation in real-world environments. In this paper we present a novel approach for 2D ground-plan-like laser-range-data-based room categorization that builds on a compositional hierarchical representation of space, and show how an additional abstraction layer, whose parts are formed by merging partial views of the environment followed by graph extraction, can achieve improved categorization performance. A new algorithm is presented that finds a dictionary of exemplar elements from a multi-category set, based on the affinity measure defined among pairs of elements. This algorithm is used for part selection in new layer construction. Room categorization experiments have been performed on a challenging publicly available dataset, which has been extended in this work. State-of-the-art results were obtained by achieving the most balanced performance over all categories.},
}

@InProceedings{otte2016icra,
  author        = {M.W. Otte and W. Silva and E.W. Frew},
  title         = {Any-Time Path-Planning: Time-Varying Wind Field + Moving Obstacles},
  booktitle     = icra,
  year          = 2016,
  keywords      = {Motion and Path Planning, Autonomous Vehicle Navigation, Aerial Robotics},
  abstract      = {We consider the problem of real-time pathplanning in a spatiotemporally varying wind-field with moving obstacles. We are provided with changing wind and obstacle predictions along a (D + 1)-dimensional space-time lattice. We present an Any-Time algorithm that quickly finds an -suboptimal solution (a path that is not longer than times the optimal time-length), and then improves and while planning time remains or until new wind/obstacle predictions trigger a restart. The factor comes from an -overestimate of the A*-like cost heuristic. is proportional to motion modeling error. Any-Time performance is achieved by: (1) improving the connectivity model of the environment from a discrete graph to a continuous cost-field (decreasing ); (2) using the established method of incrementally deflating . Our method was deployed as the global planner on a fixed-wing unmanned aircraft system that uses Doppler radar and atmospheric models for online realtime wind sensing and prediction. We compare its performance vs. other state-of-the-art methods in simulated environments.},
}

@InProceedings{zhang2016icra-bolf,
  author        = {Z. Zhang and H. Rebecq and C. Forster and D. Scaramuzza},
  title         = {{Benefit of Large Field-Of-View Cameras for Visual Odometry}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {Visual-Based Navigation, Omnidirectional Vision, SLAM},
  abstract      = {The transition of visual-odometry technology from research demonstrators to commercial applications naturally raises the question: what is the optimal camera for visionbased motion estimation? This question is crucial as the choice of camera has a tremendous impact on the robustness and accuracy of the employed visual odometry algorithm. While many properties of a camera (e.g. resolution, frame-rate, globalshutter/rolling-shutter) could be considered, in this work we focus on evaluating the impact of the camera field-of-view (FoV) and optics (i.e., fisheye or catadioptric) on the quality of the motion estimate. Since the motion-estimation performance depends highly on the geometry of the scene and the motion of the camera, we analyze two common operational environments in mobile robotics: an urban environment and an indoor scene. To confirm the theoretical observations, we implement a stateof-the-art VO pipeline that works with large FoV fisheye and catadioptric cameras. We evaluate the proposed VO pipeline in both synthetic and real experiments. The experiments point out that it is advantageous to use a large FoV camera (e.g., fisheye or catadioptric) for indoor scenes and a smaller FoV for urban canyon environments.},
}

@InProceedings{zeisl2016icra,
  author        = {B. Zeisl and M. Pollefeys},
  title         = {{Structure-Based Auto-Calibration of RGB-D Sensors}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {RGB-D Perception, Sensor Fusion, Mapping},
  abstract      = {The readily available image and depth data from commodity RGB-D sensors has had tremendous impact in the robotics and computer vision community recently. To jointly leverage both modalities, the depth and image measurements need to be registered. Typical calibration approaches make use of artificial landmarks and special calibration targets. However, this is not feasible if on-line (re-)calibration is necessary or the sensor setup is inaccessible, e.g., for already captured datasets. Instead of using specific calibration patterns, we propose to leverage a sparse environment model as geometric prior for the calibration. Structure-from-motion or SLAM can provide such a sparse 3D scene model, and hence our approach allows for self-calibration without the need for any manual interaction. We validate our hypothesis by introducing an optimization that jointly minimizes the alignment error between the sparse map and all recorded depth maps. Since the accuracy of depth measurements is known to degrade considerably with scene depth, we account for this distortion via a spatially varying correction term. The evaluation of our approach demonstrates that we are able to compute an accurate extrinsic and intrinsic calibration, which for example allows dense 3D modeling at improved precision.},
}

@InProceedings{scott2016icra,
  author        = {T. Scott and A. Morye and P. Pinies and L.M. Paz and I. Posner and P. Newman},
  title         = {{Choosing a Time and Place for Calibration of Lidar-Camera Systems}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {Calibration and Identification, Localization, Sensor Networks},
  abstract      = {We propose a calibration method that automatically estimates the extrinsic calibration between a sensor posegraph from natural scenes. The sensor pose-graph represents a system of sensors comprising of lidars and cameras, without sensor co-visibility constraints. The method addresses the fact that each scene contributes differently to the calibration problem by introducing a diligent scene selection scheme. The algorithm searches over all scenes to extract a subset of exemplars, whose joint optimisation yields progressively better calibration estimates. This non-parametric method requires no knowledge of the physical world, and continuously finds scenes that better constrain the optimisation parameters. We explain the theory, implement the method, and provide detailed performance analyses with experiments on real-world data.},
}

@InProceedings{wendel2016icra,
  author        = {A. Wendel and J.P. Underwood},
  title         = {{Self-Supervised Weed Detection in Vegetable Crops Using Ground Based Hyperspectral Imaging}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {Robotics in Agriculture and Forestry, Agricultural Automation, Field Robots},
  abstract      = {A critical step in treating or eradicating weed infestations amongst vegetable crops is the ability to accurately and reliably discriminate weeds from crops. In recent times, high spatial resolution hyperspectral imaging data from ground based platforms have shown particular promise in this application. Using spectral vegetation signatures to discriminate between crop and weed species has been demonstrated on several occasions in the literature over the past 15 years. A number of authors demonstrated successful per-pixel classification with accuracies of over 80\%. However, the vast majority of the related literature uses supervised methods, where training datasets have been manually compiled. In practice, static training data can be particularly susceptible to temporal variability due to physiological or environmental change. A self-supervised training method that leverages prior knowledge about seeding patterns in vegetable fields has recently been introduced in the context of RGB imaging, allowing the classifier to continually update weed appearance models as conditions change. This paper combines and extends these methods to provide a selfsupervised framework for hyperspectral crop/weed discrimination with prior knowledge of seeding patterns using an autonomous mobile ground vehicle. Experimental results in corn crop rows demonstrate the systems performance and limitations.},
}

@InProceedings{osep2016icra,
  author        = {A. Osep and A. Hermans and F. Engelmann and D. Klostermann and M. Mathias and B. Leibe},
  title         = {{Multi-Scale Object Candidates for Generic Object Tracking in Street Scenes}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {Visual Tracking, Computer Vision for Transportation},
  abstract      = {Most vision based systems for object tracking in urban environments focus on a limited number of important object categories such as cars or pedestrians, for which powerful detectors are available. However, practical driving scenarios contain many additional objects of interest, for which suitable detectors either do not yet exist or would be cumbersome to obtain. In this paper we propose a more general tracking approach which does not follow the often used tracking-bydetection principle. Instead, we investigate how far we can get by tracking unknown, generic objects in challenging street scenes. As such, we do not restrict ourselves to only tracking the most common categories, but are able to handle a large variety of static and moving objects. We evaluate our approach on the KITTI dataset and show competitive results for the annotated classes, even though we are not restricted to them.},
}

@InProceedings{pillai2016icra,
  author        = {S. Pillai and S. Ramalingam and J. Leonard},
  title         = {{High-Performance and Tunable Stereo Reconstruction}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {Computer Vision for Transportation, Mapping, RGB-D Perception},
  abstract      = {Traditional stereo algorithms have focused their efforts on reconstruction quality and have largely avoided prioritizing for run time performance. Robots, on the other hand, require quick maneuverability and effective computation to observe its immediate environment and perform tasks within it. In this work, we propose a high-performance and tunable stereo disparity estimation method, with a peak frame-rate of 120Hz (VGA resolution, on a single CPU-thread), that can potentially enable robots to quickly reconstruct their immediate surroundings and maneuver at high-speeds. Our key contribution is a disparity estimation algorithm that iteratively approximates the scene depth via a piece-wise planar mesh from stereo imagery, with a fast depth validation step for semidense reconstruction. The mesh is initially seeded with sparsely matched keypoints, and is recursively tessellated and refined as needed (via a resampling stage), to provide the desired stereo disparity accuracy. The inherent simplicity and speed of our approach, with the ability to tune it to a desired reconstruction quality and runtime performance makes it a compelling solution for applications in high-speed vehicles.},
}

@InProceedings{schneider2016icra,
  author        = {J. Schneider and C. Eling and L. Klingbeil and H. Kuhlmann and W. F{\"o}rstner and C. Stachniss},
  title         = {{Fast and Effective Online Pose Estimation and Mapping for UAVs}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {SLAM, Localization, Aerial Robotics},
  abstract      = {Online pose estimation and mapping in unknown environments is essential for most mobile robots. Especially autonomous unmanned aerial vehicles require good pose estimates at comparably high frequencies. In this paper, we propose an effective system for online pose and simultaneous map estimation designed for light-weight UAVs. Our system consists of two components: (1) real-time pose estimation combining RTK-GPS and IMU at 100 Hz and (2) an effective SLAM solution running at 10 Hz using image data from an omnidirectional multifisheye-camera system. The SLAM procedure combines spatial resection computed based on the map that is incrementally refined through bundle adjustment and combines the image data with raw GPS observations and IMU data on keyframes. The overall system yields a real-time, georeferenced pose at 100 Hz in GPS-friendly situations. Additionally, we obtain a precise pose and feature map at 10 Hz even in cases where the GPS is not observable or underconstrained. Our system has been implemented and thoroughly tested on a 5 kg copter and yields accurate and reliable pose estimation at high frequencies. We compare the point cloud obtained by our method with a model generated from georeferenced terrestrial laser scanner.},
  url           = {http://www.ipb.uni-bonn.de/wp-content/papercite-data/pdf/schneider16icra.pdf},
}

@InProceedings{guo2016icra,
  author        = {C. Guo and K. Sartipi and R. DuToit and G. Georgiou and R. Li and J. O'Leary and E. Nerurkar and J. Hesch and S. Roumeliotis},
  title         = {{Large-Scale Cooperative 3D Visual-Inertial Mapping in a Manhattan World}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {Mapping, SLAM, Sensor Fusion},
  abstract      = {In this paper, we address the problem of cooperative mapping (CM) using datasets collected by multiple users at different times, when the transformation between the users starting poses is unknown. Specifically, we formulate CM as a constrained optimization problem, where each users independently estimated trajectory and map are combined in a single map by imposing geometric constraints between commonly-observed point and line features. Furthermore, our formulation allows for modularity since new/old maps (or parts of them) can be easily added/removed with no impact on the remaining ones. Additionally, the proposed CM algorithm lends itself, for the most part, to parallel implementations, hence gaining in speed. Experimental results based on visual and inertial measurements collected from four users within two large buildings are used to assess the performance of the proposed CM algorithm.},
}

@InProceedings{liu2016icra,
  author        = {S. Liu and M. Watterson and S. Tang and V. Kumar},
  title         = {{High Speed Navigation for Quadrotors with Limited Onboard Sensing}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {Aerial Robotics, Collision Avoidance, Autonomous Vehicle Navigation},
  abstract      = {We address the problem of high speed autonomous navigation of quadrotor micro aerial vehicles with limited onboard sensing and computation. In particular, we propose a dual range planning horizon method to safely and quickly navigate quadrotors to specified goal locations in previously unknown and unstructured environments. In each planning epoch, a short-range planner uses a local map to generate a new trajectory. At the same time, a safe stopping policy is found. This allows the robot to come to an emergency halt when necessary. Our algorithm guarantees collision avoidance and demonstrates important advances in real-time planning. First, our novel short range planning method allows us to generate and re-plan trajectories that are dynamically feasible, comply with state and input constraints, and avoid obstacles in real-time. Further, previous planning algorithms abstract away the obstacle detection problem by assuming the instantaneous availability of geometric information about the environment. In contrast, our method addresses the challenge of using the raw sensor data to form a map and navigate in real-time. Finally, in addition to simulation examples, we provide physical experiments that demonstrate the entire algorithmic pipeline from obstacle detection to trajectory execution.},
}

@InProceedings{dequaire2016icra,
  author        = {J.M.M. Dequaire and C.H. Tong and W. Churchill and I. Posner},
  title         = {{Off the Beaten Track: Predicting Localisation Performance in Visual Teach and Repeat}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {Visual-Based Navigation, Localization, Autonomous Vehicle Navigation},
  abstract      = {This paper proposes an appearance-based approach to estimating localisation performance in the context of visual teach and repeat. Specifically, it aims to estimate the likely corridor around a taught trajectory within which a visionbased localisation system is still able to localise itself. In contrast to prior art, our system is able to predict this localisation envelope for trajectories in similar, yet geographically distant locations where no repeat runs have yet been performed. Thus, by characterising the localisation performance in one region, we are able to predict performance in another. To achieve this, we leverage a Gaussian Process regressor to estimate the likely number of feature matches for any keyframe in the teach run, based on a combination of trajectory properties such as curvature and an appearance model of the keyframe. Using data from real traversals, we demonstrate that our approach performs as well as prior art when it comes to interpolating localisation performance based on a number of repeat runs, while also performing well at generalising performance estimation to freshly taught trajectories.},
}

@InProceedings{lee2016icra-aeps,
  author        = {D. Lee and M. Campbell},
  title         = {{An Efficient Probabilistic Surface Normal Estimator}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {Object detection, Segmentation, Categorization, Recognition, Mapping},
  abstract      = {An efficient surface normal estimation method is presented. The new algorithm estimates surface normal direction for each cell in a grid based on the occupancy information (both occupied and empty) of the neighboring cells. This grid representation allows user-defined sizes and scaling with the environment, not the number of measurements. Recursive and batch formulations to obtain the posterior estimate are presented, and compared. A computationally efficient implementation is derived which provides consistent and accurate estimates as measurements become available. Both simulation and experimental results are shown, demonstrating comparable estimation performance to that of using Point Cloud Library, but with significantly reduced computation time.},
}

@InProceedings{amayo2016icra,
  author        = {P. Amayo and P. Pinies and L.M. Paz and P. Newman},
  title         = {{A Unified Representation for Application of Architectural Constraints in Large-Scale Mapping}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {SLAM, Mapping, Optimization and Optimal Control},
  abstract      = {This paper is about discovering and leveraging architectural constraints in large scale 3D reconstructions using laser. Our contribution is to offer a formulation of the problem which naturally and in a unified way, captures the variety of architectural constraints that can be discovered and applied in urban reconstructions. We focus in particular on the case of survey construction with a push broom laser + VO system. Here visual odometry is combined with vertical 2D scans to create a 3D picture of the environment. A key characteristic here is that the sensors pass/sweep swiftly through the environment such that elements of the scene are seen only briefly by cameras and scanned just once by the laser. These qualities make for a an ill-constrained optimisation problem which is greatly aided if architectural constraints can be discovered and appropriately applied. We demonstrate our approach in an end-to-end implementation which discovers salient architectural constraints and rejects false loop closures before invoking an optimisation to return a 3D model of the workspace. We evaluate the precision of this model by comparison to a ground truth provided by a 3rd party professional survey using highend (static) 3D laser scanners.},
}

@InProceedings{dube2016icra,
  author        = {R. Dub{\'e} and H. Sommer and A.R. Gawel and M. Bosse and R. Siegwart},
  title         = {{Non-Uniform Sampling Strategies for Continuous Correction Based Trajectory Estimation}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {SLAM, Localization, Field Robots},
  abstract      = {Sliding window estimation is widely used for online simultaneous localization and mapping. While increasing the sliding window size generally yields improved accuracy, it also comes at an increase in computational cost. In order to reduce this cost, we propose smarter non-uniform sampling of the trajectory representation over the sliding window. This nonuniform temporal resolution is possible with continuous-time representations that allow freely adjustable knots location. Four strategies for selecting the knots location are presented and evaluated based on a real data laser-odometry SLAM problem. The results clearly show that non-uniform distributions of knots can be superior to uniform distribution in terms of accuracy per computation time.},
}

@InProceedings{ivanov2016icra,
  author        = {A. Ivanov and M. Campbell},
  title         = {{An Efficient Robotic Exploration Planner with Probabilistic Guarantees}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {Motion and Path Planning, Optimization and Optimal Control, Autonomous Vehicle Navigation},
  abstract      = {Efficient robotic exploration of an unknown, sensor limited, global-information-deficient environment poses a unique challenge to path planning algorithms because no deterministic guarantees on path completion and mission success can be made. Integrated Exploration (IE), which strives to combine localization and exploration, must be solved in order to create an autonomous robotic system capable of long term operation in new and challenging environments. This paper formulates a probabilistic framework which allows the creation of exploration algorithms providing probabilistic guarantees of success. A novel connection is made between the Hamiltonian Path Problem and exploration. The Guaranteed Probabilistic Information Explorer (G-PIE) is developed for the IE problem, providing a probabilistic guarantee on path completion, and asymptotic optimality of exploration.},
}

@InProceedings{hoffman2016icra,
  author        = {J. Hoffman and S. Gupta and J. Leong and S. Guadarrama and T. Darrell},
  title         = {{Cross-Modal Adaptation for RGB-D Detection}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {RGB-D Perception, Computer Vision for Other Robotic Applications, Object detection, Segmentation, Categorization, CNN},
  abstract      = {In this paper owing paper, we present a framework for we propose a technique to adapt convolutional neural Our network (CNN) based object detectors ject detectors for robotic perception. trained on images to effectively leverage depth images at y robotics practitioners to RGB quickly (under test time to boost detection performance. Given labeled depth build a large-scale real-time perception images a detectors handful of we show how to createfor new oncategories we adapt an RGB object detector for thus a new category such that it can now use depth e internet image databases, allowing in addition to RGB images at test time to produce ng thousands images of available categories to more detections. em suitable for theaccurate particular robotic Our approach is built upon the observation layers of a CNN are largely task and ore, we show how to adaptthat theselower models category domain specific while higher layers are nment with just a fewagnostic in-situ and images. largelyevaluate task andthe category ng 2D benchmarks speed,specific while being domain agnostic. We operationalize this observation by proposing a mid-level y of our system. fusion of RGB and depth CNNs. Experimental evaluation on the challenging NYUD2 dataset shows that our proposed adaptation .},
}

@InProceedings{kaelbling2016icra,
  author        = {L. Kaelbling and T. Lozano-Perez},
  title         = {{Implicit Belief-Space Pre-Images for Hierarchical Planning and Execution}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {Manipulation Planning, AI Reasoning Methods, Mobile Manipulation},
  abstract      = {We present a method for planning and execution in very high-dimensional mixed discrete and continuous spaces in the presence of uncertainty using an implicit, factored approximation representation of pre-images and extend it to planning in belief space. We demonstrate the approach in a mobile-manipulation domain combining pushing with pick-andplace manipulation with error in sensing and manipulation. We show empirically that execution monitoring using pre-images improves computational efficiency over continual replanning, and that the hierarchical planning method it enables provides further efficiency improvements.},
}

@InProceedings{frost2016icra,
  author        = {D. Frost and O. K{\"a}hler and D. Murray},
  title         = {{Object-Aware Bundle Adjustment for Correcting Monocular Scale Drift}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {SLAM, Computer Vision for Other Robotic Applications, Visual Tracking},
  abstract      = {Without knowledge of the absolute baseline between images, the scale of a map from single-camera simultaneous localization and mapping system is subject to calamitous drift over time. We describe a monocular approach that in addition to point measurements also considers object detections to resolve this scale ambiguity and drift. By placing a prior on the size of the objects, the scale estimation can be seamlessly integrated into a bundle adjustment. When object observations are available, the local scale of the map is then determined jointly with the camera pose in local adjustments. Unlike many previous visual odometry methods, our approach does not impose restrictions such as approximately constant camera height or planar roadways, and is therefore applicable to a much wider range of applications. We evaluate our approach on the KITTI dataset and show that it reduces scale drift over long-range outdoor sequences with a total length of 40 km. Qualitative evaluation is also performed on video footage from a hand-held camera.},
}

@InProceedings{mccool2016icra,
  author        = {C.S. McCool and I. Sa and F. Dayoub and C. Lehnert and T. Perez and B. Upcroft},
  title         = {{Visual Detection of Occluded Crop: For Automated Harvesting}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {Computer Vision for Other Robotic Applications},
  abstract      = {This paper presents a novel crop detection system applied to the challenging task of field sweet pepper (capsicum) detection. The field-grown sweet pepper crop presents several challenges for robotic systems such as the high degree of occlusion and the fact that the crop can have a similar colour to the background (green on green). To overcome these issues, we propose a two-stage system that performs per-pixel segmentation followed by region detection. The output of the segmentation is used to search for highly probable regions and declares these to be sweet pepper. We propose the novel use of the local binary pattern (LBP) to perform crop segmentation. This feature improves the accuracy of crop segmentation from an AUC of 0.10, for previously proposed features, to 0.56. Using the LBP feature as the basis for our two-stage algorithm, we are able to detect 69.2\% of field grown sweet peppers in three sites. This is an impressive result given that the average detection accuracy of people viewing the same colour imagery is 66.8\%.},
}

@InProceedings{pothen2016icra,
  author        = {Z. Pothen and S. Nuske},
  title         = {{Texture-Based Fruit Detection Via Images Using the Smooth Patterns on the Fruit}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {Field Robots, Robotics in Agriculture and Forestry, Object detection, Segmentation, Categorization},
  abstract      = {This paper describes a keypoint detection algorithm to accurately detect round fruits in high resolution imagery. The significant challenge associated with round fruits such as grapes and apples is that the surface is smooth and lacks definition and contrasting features, the contours of the fruit may be partially occluded, and the color of the fruit often blends with background foliage. We propose a fruit detection algorithm that utilizes the gradual variation of intensity and gradient orientation on the surface of the fruit. Candidate fruit locations, or seed points are tested for both monotonically decreasing intensity and gradient orientation profiles. Candidate fruit locations that pass the initial filter are classified using modified histogram of oriented gradients combined with a pairwise intensity comparison texture descriptor and random forest classifier. We analyse the performance of the fruit detection algorithm on image datasets of grapes and apples using human labeled images as ground truth. Our method to detect candidate fruit locations is scale invariant, robust to partial occlusions and more accurate than existing methods. We achieve overall F1 accuracy score of 0.82 for grapes and 0.80 for apples. We demonstrate our method is more accurate than existing methods.},
}

@InProceedings{ataer-cansizoglu2016icra,
  author        = {E. Ataer-Cansizoglu and Y. Taguchi and S. Ramalingam},
  title         = {{Pinpoint SLAM: A Hybrid of 2D and 3D Simultaneous Localization and Mapping for RGB-D Sensors}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {SLAM},
  abstract      = {Conventional SLAM systems with an RGB-D sensor use depth measurements only in a limited depth range due to hardware limitation and noise of the sensor, ignoring regions that are too far or too close from the sensor. Such systems introduce registration errors especially in scenes with large depth variations. In this paper, we present a novel RGB-D SLAM system that makes use of both 2D and 3D measurements. Our system first extracts keypoints from RGB images and generates 2D and 3D point features from the keypoints with invalid and valid depth values, respectively. It then establishes 3D-to-3D, 2D-to-3D, and 2D-to-2D point correspondences among frames. For the 2D-to-3D point correspondences, we use the rays defined by the 2D point features to pinpoint the corresponding 3D point features, generating longer-range constraints than using only 3D-to-3D correspondences. For the 2D-to-2D point correspondences, we triangulate the rays to generate 3D points that are used as 3D point features in the subsequent process. We use the hybrid correspondences in both online SLAM and offline postprocessing: the online SLAM focuses more on the speed by computing correspondences among consecutive frames for real-time operations, while the offline postprocessing generates more correspondences among all the frames for higher accuracy. The results on RGB-D SLAM benchmarks show that the online SLAM provides higher accuracy than conventional SLAM systems, while the postprocessing further improves the accuracy.},
}

@InProceedings{suenderhauf2016icra,
  author        = {N. S{\"u}nderhauf and F. Dayoub and S.M. McMahon and B. Talbot and R. Schulz and G. Wyeth and P. Corke and B. Upcroft and M.J. Milford},
  title         = {{Place Categorization and Semantic Mapping on a Mobile Robot}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {Semantic Scene Understanding, Mapping},
  abstract      = {In this paper we focus on the challenging problem of place categorization and semantic mapping on a robot without environment-specific training. Motivated by their ongoing success in various visual recognition tasks, we build our system upon a state-of-the-art convolutional network. We overcome its closed-set limitations by complementing the network with a series of one-vs-all classifiers that can learn to recognize new semantic classes online. Prior domain knowledge is incorporated by embedding the classification system into a Bayesian filter framework that also ensures temporal coherence. We evaluate the classification accuracy of the system on a robot that maps a variety of places on our campus in real-time. We show how semantic information can boost robotic object detection performance and how the semantic map can be used to modulate the robots behaviour during navigation tasks. The system is made available to the community as a ROS module.},
}

@InProceedings{phillips2016icra,
  author        = {S. Phillips and A. Jaegle and K. Daniilidis},
  title         = {{Fast, Robust, Continuous Monocular Egomotion Computation}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {Visual-Based Navigation, Computer Vision for Other Robotic Applications},
  abstract      = {We propose robust methods for estimating camera egomotion in noisy, real-world monocular image sequences in the general case of unknown observer rotation and translation with two views and a small baseline. This is a difficult problem because of the nonconvex cost function of the perspective camera motion equation and because of non-Gaussian noise arising from noisy optical flow estimates and scene non-rigidity. To address this problem, we introduce the expected residual likelihood method (ERL), which estimates confidence weights for noisy optical flow data using likelihood distributions of the residuals of the flow field under a range of counterfactual model parameters. We show that ERL is effective at identifying outliers and recovering appropriate confidence weights in many settings. We compare ERL to a novel formulation of the perspective camera motion equation using a lifted kernel, a recently proposed optimization framework for joint parameter and confidence weight estimation with good empirical properties. We incorporate these strategies into a motion estimation pipeline that avoids falling into local minima. We find that ERL outperforms the lifted kernel method and baseline monocular egomotion estimation strategies on the challenging KITTI dataset, while adding almost no runtime cost over baseline egomotion methods.},
}

@InProceedings{bargoti2016icra,
  author        = {S. Bargoti and J.P. Underwood},
  title         = {{Image Classification with Orchard Metadata}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {Robotics in Agriculture and Forestry, Semantic Scene Understanding, Object detection, Segmentation, Categorization},
  abstract      = {Low cost and easy to use monocular vision systems are able to capture large scale, dense data in orchards, to facilitate precision agriculture applications. Accurate image parsing is required for this purpose, however, operating in natural outdoor conditions makes this a complex task due to the undesirable intra-class variations caused by changes in illumination, pose and tree types, etc. Typically these variations are difficult to explicitly model and discriminative classifiers strive to be invariant to them. However, given the presence of structure, in both the orchard and how the data was obtained, a subset of these factors of variations can correlate with readily available metadata, including extrinsic experimental information such as the sun incidence angle, position within farm, etc. This paper presents a method to incorporate such metadata to aid scene parsing based on a multi-scale Multi-Layered Perceptron (MLP) architecture. Experimental results are shown for pixel segmentation over data collected at an apple orchard, leading to fruit detection and yield estimation. The results show a consistent improvement in segmentation accuracy with the inclusion of metadata under different network complexities, training configurations and evaluation metrics.},
}

@InProceedings{hayne2016icra,
  author        = {R. Hayne and R. Luo and D. Berenson},
  title         = {{Considering Avoidance and Consistency in Motion Planning for Human-Robot Manipulation in a Shared Workspace}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {Manipulation Planning, Physical Human-Robot Interaction},
  abstract      = {This paper presents an approach to formulating the cost function for a motion planner intended for human-robot collaboration on manipulation tasks in a shared workspace. To be effective for human-robot collaboration a robot should plan its motion so that it is both safe and efficient. To achieve this, we propose two factors to consider in the cost function for the robots motion planner: (1) Avoidance of the workspace previously-occupied by the human, so that the motion is as safe as possible, and (2) Consistency of the robots motion, so that the motion is as predictable as possible for the human and they can perform their task without focusing undue attention on the robot. Our experiments in simulation and a human-robot workspace sharing study compare a cost function that uses only the first factor and a combined cost that uses both factors vs. a baseline method that is perfectly consistent but does not account for the humans previous motion. We find that using either cost function we outperform the baseline method in terms of task success rate without degrading the task completion time. The best task success rate is achieved with the cost function that includes both the avoidance and consistency terms.},
}

@InProceedings{cicco2016icra,
  author        = {M. {Di Cicco} and B. {Della Corte} and G. Grisetti},
  title         = {{Unsupervised Calibration of Wheeled Mobile Platforms}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {Calibration and Identification, Kinematics},
  abstract      = {This paper describes an unsupervised approach to retrieve the kinematic parameters of a wheeled mobile robot. The robot chooses which action to take in order to minimize the uncertainty in the parameter estimate and to fully explore the parameter space. Our method explores the effects of a set of elementary motion on the platform to dynamically select the best action and to stop the process when the estimate can be no further improved. We tested our approach both in simulation and with real robots. Our method is reported to obtain in shorter time parameter estimates that are statistically more accurate than the ones obtained by steering the robot on predefined patterns.},
}

@InProceedings{li2016icra-ltg3,
  author        = {J. Li and D.P. Meger and G. Dudek},
  title         = {{Learning to Generalize 3D Spatial Relationships}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {Semantic Scene Understanding, RGB-D Perception, Visual Learning},
  abstract      = {This paper presents an approach to learn meaningful spatial relationships in an unsupervised fashion from the distribution of 3D object poses in the real world. Our approach begins by extracting an over-complete set of features to describe the relative geometry of two objects. Each relationship type is modeled using a relevance-weighted distance over this feature space. This effectively ignores irrelevant feature dimensions. Our algorithm RANSEM for determining subsets of data that share a relationship as well as the model to describe each relationship is based on robust sample-based clustering. This approach combines the search for consistent groups of data with the extraction of models that precisely capture the geometry of those groups. An iterative refinement scheme has shown to be an effective approach for finding concepts of differing degrees of geometric specificity. Our results show that the models learned by our approach correlate strongly with the English labels that have been given by a human annotator to a set of validation data drawn from the NYUv2 real-world Kinect dataset, demonstrating that these concepts can be automatically acquired given sufficient experience. Additionally, the results of our method significantly out-perform K-means, a standard baseline for unsupervised cluster extraction.},
}

@InProceedings{kim2016icra,
  author        = {J. Kim and C.D.C. Lerma and I. Reid},
  title         = {{Direct Semi-dense SLAM for Rolling Shutter Cameras}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {SLAM},
  abstract      = {In this paper, we present a monocular Direct and Semi-dense SLAM (Simultaneous Localization And Mapping) system for rolling shutter cameras. In a rolling shutter camera, the pose is different for each row of each image, and this yields poor pose estimates and poor structure estimates when using a state-of-the-art semi-dense direct method designed for global shutter cameras. To address this issue in tracking, we model the smooth and continuous camera trajectory using a B-spline curve of degree k1 for poses in the Lie algebra, se(3). We solve for the camera poses at each row-time by a direct optimisation of photometric error as a function of the control points of the spline. Likewise for mapping, we develop generalised epipolar geometry for the rolling shutter case and solve for point depths using photometric error. Although each of these issues has been previously tackled, to the best of our knowledge ours is the first full solution to monocular, direct (feature-less) SLAM. We benchmark our method for pose accuracy and map accuracy against the state-of-the-art semi-dense SLAM system, LSDSLAM, demonstrating the improved efficacy of our approach when using rolling shutter cameras via synthetic sequences with known ground-truth and real sequences.},
}

@InProceedings{paull2016icra,
  author        = {L. Paull and G. Huang and J. Leonard},
  title         = {{A Unified Resource-Constrained Framework for Graph SLAM}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {SLAM, Mapping, Localization},
  abstract      = {Graphical methods have proven an extremely useful tool employed by the mobile robotics community to frame estimation problems. Incremental solvers are able to process incoming sensor data and produce maximum a posteriori (MAP) estimates in realtime by exploiting the natural sparsity within the graph for reasonable-sized problems. However, to enable truly longterm operation in prior unknown environments requires algorithms whose computation, memory, and bandwidth (in the case of distributed systems) requirements scale constantly with time and environment size. Some recent approaches have addressed this problem through a two-step process - first the variables selected for removal are marginalized which induces density, and then the result is sparsified to maintain computational efficiency. Previous literature generally addresses only one of these two components. In this work, we attempt to explicitly connect all of the aforementioned resource constraint requirements by considering the node removal and sparsification pipeline in its entirety. We formulate the node selection problem as a minimization problem over the penalty to be paid in the resulting sparsification. As a result, we produce node subset selection strategies that are optimal in terms of minimizing the impact, in terms of Kullback-Liebler divergence (KLD), of approximating the dense distribution by a sparse one. We then show that one instantiation of this problem yields a computationally tractable formulation. Finally, we evaluate the method on standard datasets and show that the KLD is minimized as compared to other commonly-used heuristic node selection techniques.},
}

@InProceedings{kim2016icra-pfgs,
  author        = {S. Kim and M. Likhachev},
  title         = {{Planning for Grasp Selection of Partially Occluded Objects}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {Motion and Path Planning, Manipulation Planning, Grasping},
  abstract      = {In a cluttered scene, an object is often occluded by other objects, and a robot cannot figure out what the object is and perceive its pose exactly. We assume that the robot is equipped with a depth sensor and given a database of 3D object models and their grasping poses, but yet there is uncertainty about objects class and pose. In this paper, we study the problem of how to predict the class and pose of an occluded object by carefully taking a sequence of observations. To find the best sequence of viewpoints by the robot, we construct hypotheses of the states of the target and occluding objects, and update our belief state as new observations come in. Every time selecting the next robot pose, we greedily choose the one that is expected to reduce the uncertainty the most. Based on the theoretical analysis of adaptive submodular maximization problems, this process is guaranteed to find a near-optimal sequence of robot poses in terms of observation and traverse costs. To validate the proposed method, we present simulation and robot experiments using a PR2.},
}

@InProceedings{karasev2016icra,
  author        = {V. Karasev and A. Ayvaci and B. Heisele and S. Soatto},
  title         = {{Intent-Aware Long-Term Prediction of Pedestrian Motion}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {Autonomous Vehicle Navigation, Motion and Path Planning},
  abstract      = {We present a method to predict long-term motion of pedestrians, modeling their behavior as jump-Markov processes with their goal a hidden variable. Assuming approximately rational behavior, and incorporating environmental constraints and biases, including time-varying ones imposed by traffic lights, we model intent as a policy in a Markov decision process framework. We infer pedestrian state using a Rao-Blackwellized filter, and intent by planning according to a stochastic policy, reflecting individual preferences in aiming at the same goal.},
}

@InProceedings{khosoussi2016icra,
  author        = {K. Khosoussi and S. Huang and G. Dissanayake},
  title         = {{Tree-Connectivity: Evaluating the Graphical Structure of SLAM}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {SLAM},
  abstract      = {Simultaneous localization and mapping (SLAM) in robotics, and a number of related problems that arise in sensor networks are instances of estimation problems over weighted graphs. This paper studies the relation between the graphical representation of such problems and estimationtheoretic concepts such as the Cramer-Rao lower bound (CRLB) and D-optimality. We prove that the weighted number of spanning trees, as a graph connectivity metric, is closely related to the determinant of CRLB. This metric can be efficiently computed for large graphs by exploiting the sparse structure of underlying estimation problems. Our analysis is validated using experiments with publicly available pose-graph SLAM datasets.},
}

@InProceedings{lehnert2016icra,
  author        = {C. Lehnert and I. Sa and C.S. McCool and B. Upcroft and T. Perez},
  title         = {{Sweet Pepper Pose Detection and Grasping for Automated Crop Harvesting}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {Robotics in Agriculture and Forestry, Computer Vision for Other Robotic Applications},
  abstract      = {This paper presents a method for estimating the 6DOF pose of sweet-pepper (capsicum) crops for autonomous harvesting via a robotic manipulator. The method uses the Kinect Fusion algorithm to robustly fuse RGB-D data from an eye-in-hand camera combined with a colour segmentation and clustering step to extract an accurate representation of the crop. The 6DOF pose of the sweet peppers is then estimated via a nonlinear least squares optimisation by fitting a superellipsoid to the segmented sweet pepper. The performance of the method is demonstrated on a real 6DOF manipulator with a custom gripper. The method is shown to estimate the 6DOF pose successfully enabling the manipulator to grasp sweet peppers for a range of different orientations. The results obtained improve largely on the performance of grasping when compared to a naive approach, which does not estimate the orientation of the crop.},
}

@InProceedings{isler2016icra,
  author        = {S.R. Isler and R. Sabzevari and J. Delmerico and D. Scaramuzza},
  title         = {{An Information Gain Formulation for Active Volumetric 3D Reconstruction}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {Autonomous Agents, Computational Geometry, Computer Vision for Automation},
  abstract      = {We consider the problem of next-best view selection for volumetric reconstruction of an object by a mobile robot equipped with a camera. Based on a probabilistic volumetric map that is built in real time, the robot can quantify the expected information gain from a set of discrete candidate views. We propose and evaluate several formulations to quantify this information gain for the volumetric reconstruction task, including visibility likelihood and the likelihood of seeing new parts of the object. These metrics are combined with the cost of robot movement in utility functions. The next best view is selected by optimizing these functions, aiming to maximize the likelihood of discovering new parts of the object. We evaluate the functions with simulated and real world experiments within a modular software system that is adaptable to other robotic platforms and reconstruction problems. We release our implementation open source.},
}

@InProceedings{bormann2016icra,
  author        = {R. Bormann and F. Jordan and W. LI and J. Hampp and M. Haegele},
  title         = {{Room Segmentation: Survey, Implementation, and Analysis}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {Mapping, Localization},
  abstract      = {The division of floor plans or navigation maps into single rooms or similarly meaningful semantic units is central to numerous tasks in robotics such as topological mapping, semantic mapping, place categorization, human-robotinteraction, or automatized professional cleaning. Although many map partitioning algorithms have been proposed for various applications there is a lack of comparative studies on these different algorithms. This paper surveys the literature on room segmentation and provides four publicly available implementations of popular methods, which target the semantic mapping domain and are tuned to yield segmentations into complete rooms. In an attempt to provide new users of such technologies guidance in the choice of map segmentation algorithm, those methods are compared qualitatively and quantitatively using several criteria. The evaluation is based on a novel compilation of 20 challenging floor plans.},
}

@InProceedings{li2016icra-lsim,
  author        = {Z. Li and V. Isler},
  title         = {{Large Scale Image Mosaic Construction for Agricultural Applications}},
  booktitle     = icra,
  year          = 2016,
  keywords      = {Computer Vision for Other Robotic Applications, Robotics in Agriculture and Forestry},
  abstract      = {We present a novel technique for stitching images including those obtained from aerial vehicles flying at low altitudes. Existing image stitching/mosaicking methods rely on inter-image homography computation based on a planar scene assumption. This assumption holds when images are taken from high-altitudes (hence the depth variation is negligible). It is often violated when flying at low altitudes. Further, to avoid scale and resolution changes, existing methods rely on primarily translational motion at fixed altitudes. Our method removes these limitations and performs well even when aerial images are taken from low altitudes by an aerial vehicle performing complex motions. It starts by extracting the ground geometry from a sparse reconstruction of the scene obtained from a small fraction of the input images. Next, it selects the best image (from the entire sequence) for each location on the ground using a novel camera selection criterion. This image is then independently rectified to obtain the corresponding portion of the mosaic. Therefore, the technique avoids performing costly joint-optimization over the entire sequence. It is validated using challenging input sequences motivated by agricultural applications.},
}

@InProceedings{dymczyk2016iros,
  author        = {M.T. Dymczyk and T. Schneider and I. Gilitschenski and R. Siegwart and E. Stumm},
  title         = {{Erasing Bad Memories: Agent-Side Summarization for Long-Term Mapping}},
  booktitle     = iros,
  year          = 2016,
  keywords      = {Mapping, Visual Navigation, Localization},
  abstract      = {Precisely estimating the pose of an agent in a global reference frame is a crucial goal that unlocks a multitude of robotic applications, including autonomous navigation and collaboration. In order to achieve this, current state-ofthe-art localization approaches collect data provided by one or more agents and create a single, consistent localization map, maintained over time. However, with the introduction of lengthier sorties and the growing size of the environments, data transfers between the backend server where the global map is stored and the agents are becoming prohibitively large. While some existing methods partially address this issue by building compact summary maps, the data transfer from the agents to the backend can still easily become unmanageable. In this paper, we propose a method that is designed to reduce the amount of data that needs to be transferred from the agent to the backend, functioning in large-scale, multisession mapping scenarios. Our approach is based upon a landmark selection method that exploits information coming from multiple, possibly weak and correlated, landmark utility predictors; fused using learned feature coefficients. Such a selection yields a drastic reduction in data transfer while maintaining localization performance and the ability to efficiently summarize environments over time. We evaluate our approach on a data set that was autonomously collected in a dynamic indoor environment over a period of several months.},
}

@InProceedings{buerki2016iros,
  author        = {M. B{\"u}rki and I. Gilitschenski and E. Stumm and R. Siegwart and J. Nieto},
  title         = {{Appearance-Based Landmark Selection for Efficient Long-Term Visual Localization}},
  booktitle     = iros,
  year          = 2016,
  keywords      = {Visual Navigation, Localization, Distributed Robot Systems},
  abstract      = {In this paper, we present an online landmark selection method for distributed long-term visual localization systems in bandwidth-constrained environments. Sharing a common map for online localization provides a fleet of autonomous vehicles with the possibility to maintain and access a consistent map source, and therefore reduce redundancy while increasing efficiency. However, connectivity over a mobile network imposes strict bandwidth constraints and thus the need to minimize the amount of exchanged data. The wide range of varying appearance conditions encountered during long-term visual localization offers the potential to reduce data usage by extracting only those visual cues which are relevant at the given time. Motivated by this, we propose an unsupervised method of adaptively selecting landmarks according to how likely these landmarks are to be observable under the prevailing appearance condition. The ranking function this selection is based upon exploits landmark co-observability statistics collected in past traversals through the mapped area. Evaluation is performed over different outdoor environments, large time-scales and varying appearance conditions, including the extreme transition from day-time to night-time, demonstrating that with our appearance-dependent selection method, we can significantly reduce the amount of landmarks used for localization while maintaining or even improving the localization performance.},
}

@InProceedings{shakeri2016iros,
  author        = {M. Shakeri and H. Zhang},
  title         = {{Illumination Invariant Representation of Natural Images for Visual Place Recognition}},
  booktitle     = iros,
  year          = 2016,
  keywords      = {Recognition, Localization, Visual Navigation},
  abstract      = {Illumination changes are a typical problem for many outdoor long-term applications such as visual place recognition. Keypoints may fail to match between images taken at the same location but different times of the day. Although recently some methods are presented for creating shadowfree image representations, all of them have the limitation in terms of dealing with night images and non-Planckian source of lighting. In this paper we present a new method for creating illumination invariant image representation using a combination of two existing methods based on natural image statistics that address the issue of illumination invariance. Unlike previous attempts at solving the problem of illumination invariant representation, the proposed method does not assume the ideal narrow-band color camera nor a calibration step for each environment. We evaluate our method on real datasets to establish its accuracy and efficiency. Experimental results show that our method outperforms competing methods for illumination invariant image representation.},
}

@InProceedings{caselitz2016iros,
  author        = {T. Caselitz and B. Steder and M. Ruhnke and W. Burgard},
  title         = {{Monocular Camera Localization in 3D LiDAR Maps}},
  booktitle     = iros,
  year          = 2016,
  keywords      = {Localization, Visual Navigation, Range Sensing},
  abstract      = {Localizing a camera in a given map is essential for vision-based navigation. In contrast to common methods for visual localization that use maps acquired with cameras, we propose a novel approach, which tracks the pose of monocular camera with respect to a given 3D LiDAR map. We employ a visual odometry system based on local bundle adjustment to reconstruct a sparse set of 3D points from image features. These points are continuously matched against the map to track the camera pose in an online fashion. Our approach to visual localization has several advantages. Since it only relies on matching geometry, it is robust to changes in the photometric appearance of the environment. Utilizing panoramic LiDAR maps additionally provides viewpoint invariance. Yet lowcost and lightweight camera sensors are used for tracking. We present real-world experiments demonstrating that our method accurately estimates the 6-DoF camera pose over long trajectories and under varying conditions.},
}

@InProceedings{paton2016iros,
  author        = {M. Paton and K.A. MacTavish and M. Warren and T. Barfoot},
  title         = {Bridging the Appearance Gap: Multi-Experience Localization for Long-Term Visual Teach & Repeat},
  booktitle     = iros,
  year          = 2016,
  keywords      = {Localization, Mapping, Field Robots},
  abstract      = {Vision-based, route-following algorithms enable autonomous robots to repeat manually taught paths over long distances using inexpensive vision sensors. However, these methods struggle with long-term, outdoor operation due to the challenges of environmental appearance change caused by lighting, weather, and seasons. While techniques exist to address appearance change by using multiple experiences over different environmental conditions, they either provide topological-only localization, require several manually taught experiences in different conditions, or require extensive offline mapping to produce metric localization. For real-world use, we would like to localize metrically to a single manually taught route and gather additional visual experiences during autonomous operations. Accordingly, we propose a novel multi-experience localization (MEL) algorithm developed specifically for routefollowing applications; it provides continuous, six-degree-offreedom (6DoF) localization with relative uncertainty to a privileged (manually taught) path using several experiences simultaneously. We validate our algorithm through two experiments: i) an offline performance analysis on a 9km subset of a challenging 27km route-traversal dataset and ii) an online field trial where we demonstrate autonomy on a small 250m loop over the course of a sunny day. Both exhibit significant appearance change due to lighting variation. Through these experiments we show that safe localization can be achieved by bridging the appearance gap.},
}

@InProceedings{tanaka2016iros,
  author        = {K. Tanaka},
  title         = {{Self-Localization from Images with Small Overlap}},
  booktitle     = iros,
  year          = 2016,
  keywords      = {Localization, Robot Vision, Performance Evaluation and Benchmarking, CNN},
  abstract      = {With the recent success of visual features from deep convolutional neural networks (DCNN) in visual robot selflocalization, it has become important and practical to address more general self-localization scenarios. In this paper, we address the scenario of self-localization from images with small overlap. We explicitly introduce a localization difficulty index as a decreasing function of view overlap between query and relevant database images and investigate performance versus difficulty for challenging cross-view self-localization tasks. We then reformulate the self-localization as a scalable bag-ofvisual-features (BoVF) scene retrieval and present an efficient solution called PCA-NBNN, aiming to facilitate fast and yet discriminative correspondence between partially overlapping images. The proposed approach adopts recent findings in discriminativity preserving encoding of DCNN features using principal component analysis (PCA) and cross-domain scene matching using naive Bayes nearest neighbor distance metric (NBNN). We experimentally demonstrate that the proposed PCA-NBNN framework frequently achieves comparable results to previous DCNN features and that the BoVF model is significantly more efficient. We further address an important alternative scenario of self-localization from images with NO overlap and report the result.},
}

@InProceedings{fontana2016iros,
  author        = {S. Fontana and G. Agamennoni and R. Siegwart and D.G. Sorrenti},
  title         = {{Point Clouds Registration with Probabilistic Data Association}},
  booktitle     = iros,
  year          = 2016,
  keywords      = {Mapping, Field Robots},
  abstract      = {Although Point Clouds Registration is a very well studied problem, with many different solutions, most of the approaches in the literature aims at aligning two dense point clouds. Instead, we tackle the problem of aligning a dense point cloud with a sparse one: a problem that has to be solved, for example, to merge maps produced by different sensors, such as a vision-based sensor and laser scanner or two different laser-based sensors. The most used approach to point clouds registration, Iterative Closest Point (ICP), is also applicable to this sub-problem. We propose an improvement over the standard ICP data association policy and we called it Probabilistic Data Association. It was derived applying statistical inference techniques on a fully probabilistic model. In our proposal, each point in the source point cloud is associated with a set of points in the target point cloud; each association is then weighted so that the weights form a probability distribution. The result is an algorithm similar to ICP but more robust w.r.t. noise and outliers. While we designed our approach to deal with the problem of dense-sparse registration, it can be successfully applied also to standard point clouds registration.},
}

@InProceedings{schlegel2016iros,
  author        = {D. Schlegel and G. Grisetti},
  title         = {{Visual Localization and Loop Closing Using Decision Trees and Binary Features}},
  booktitle     = iros,
  year          = 2016,
  keywords      = {Recognition, SLAM, Visual Navigation},
  abstract      = {In this paper we present an approach for efficiently retrieving the most similar image, based on pointto-point correspondences, within a sequence that has been acquired through continuous camera movement. Our approach is entailed to the use of standardized binary feature descriptors and exploits the temporal form of the input data to dynamically adapt the search structure. While being straightforward to implement, our method exhibits very fast response times and its Precision/Recall rates compete with state of the art approaches. Our claims are supported by multiple large scale experiments on publicly available datasets. Index Terms Place recognition, Localization, Robot Vision},
}

@InProceedings{arroyo2016iros,
  author        = {R. Arroyo and P.F. Alcantarilla and L.M. Bergasa and E. Romera},
  title         = {{Fusion and Binarization of CNN Features for Robust Topological Localization across Seasons}},
  booktitle     = iros,
  year          = 2016,
  keywords      = {Localization, Visual Navigation, Robot Vision, CNN},
  abstract      = {The extreme variability in the appearance of a place across the four seasons of the year is one of the most challenging problems in life-long visual topological localization for mobile robotic systems and intelligent vehicles. Traditional solutions to this problem are based on the description of images using hand-crafted features, which have been shown to offer moderate invariance against seasonal changes. In this paper, we present a new proposal focused on automatically learned descriptors, which are processed by means of a technique recently popularized in the computer vision community: Convolutional Neural Networks (CNNs). The novelty of our approach relies on fusing the image information from multiple convolutional layers at several levels and granularities. In addition, we compress the redundant data of CNN features into a tractable number of bits for efficient and robust place recognition. The final descriptor is reduced by applying simple compression and binarization techniques for fast matching using the Hamming distance. An exhaustive experimental evaluation confirms the improved performance of our proposal (CNN-VTL) with respect to state-of-the-art methods over varied long-term datasets recorded across seasons.},
}

@InProceedings{serafin2016iros,
  author        = {J. Serafin and E. Olson and G. Grisetti},
  title         = {{Fast and Robust 3D Feature Extraction from Sparse Point Clouds}},
  booktitle     = iros,
  year          = 2016,
  keywords      = {Localization, SLAM, Navigation},
  abstract      = {Matching 3D point clouds, a critical operation in map building and localization, is difficult with Velodyne-type sensors due to the sparse and non-uniform point clouds that they produce. Standard methods from dense 3D point clouds are generally not effective. In this paper, we describe a featurebased approach using Principal Components Analysis (PCA) of neighborhoods of points, which results in mathematically principled line and plane features. The key contribution in this work is to show how this type of feature extraction can be done efficiently and robustly even on non-uniformly sampled point clouds. The resulting detector runs in real-time and can be easily tuned to have a low false positive rate, simplifying data association. We evaluate the performance of our algorithm on an autonomous car at the MCity Test Facility using a Velodyne HDL-32E, and we compare our results against the state-of-theart NARF keypoint detector.},
}

@InProceedings{he2016iros,
  author        = {L. He and X. Wang and H. Zhang},
  title         = {{M2DP: A Novel 3D Point Cloud Descriptor and Its Application in Loop Closure Detection}},
  booktitle     = iros,
  year          = 2016,
  keywords      = {Visual Navigation, SLAM},
  abstract      = {In this paper, we present a novel global descriptor M2DP for 3D point clouds, and apply it to the problem of loop closure detection. In M2DP, we project a 3D point cloud to multiple 2D planes and generate a density signature for points for each of the planes. We then use the left and right singular vectors of these signatures as the descriptor of the 3D point cloud. Our experimental results show that the proposed algorithm outperforms state-of-the-art global 3D descriptors in both accuracy and efficiency.},
}

@InProceedings{kallasi2016iros,
  author        = {F. Kallasi and D.L. Rizzini},
  title         = {{Efficient Loop Closure Based on FALKO LIDAR Features for Online Robot Localization and Mapping}},
  booktitle     = iros,
  year          = 2016,
  keywords      = {Range Sensing, Localization, Mapping},
  abstract      = {Keypoint features detection from measurements enables efficient localization and map estimation through the compact representation and recognition of locations. The keypoint detector FALKO has been proposed to detect stable points in laser scans for localization and mapping tasks. In this paper, we present novel loop closure methods based on FALKO keypoints and compare their performance in online localization and mapping problems. The pose graph formulation is adopted, where each pose is associated to a local map of keypoints extracted from the corresponding laser scan. Loops in the graph are detected by matching local maps in two steps. First, the candidate matching scans are selected by comparing the scan signatures obtained from the keypoints of each scan. Second, the transformation between two scans is obtained by pairing and aligning the respective keypoint sets. Experiments with standard benchmark datasets assess the performance of FALKO and of the proposed loop closure algorithms in both offline and online localization and map estimation.},
}

@InProceedings{spangenberg2016iros,
  author        = {R. Spangenberg and D. Goehring and R. Rojas},
  title         = {{Pole-Based Localization for Autonomous Vehicles in Urban Scenarios}},
  booktitle     = iros,
  year          = 2016,
  keywords      = {Localization, Robot Vision, Mapping},
  abstract      = {Localization is a key capability for autonomous vehicles especially in urban scenarios. We propose the use of pole-like landmarks as primary features in these environments, as they are distinct, long-term stable and can be detected reliably with a stereo camera system. Furthermore, the resulting map representation is memory efficient, allowing for easy storage and on-line updates. The localization is performed in real-time by a stereo camera system as a main sensor, using vehicle odometry and an off-the-shelf GPS as secondary information sources. Localization is performed by a particle filter approach, coupled with an Kalman filter for robustness and sensor fusion. This leads to a lateral accuracy below 20 cm in various urban test areas. The system has been included in our autonomous test vehicle and successfully demonstrated the full loop from mapping to autonomous driving.},
}

@InProceedings{skinner2016iros,
  author        = {J.R. Skinner and S. Garg and N. S{\"u}nderhauf and P. Corke and B. Upcroft and M.J. Milford},
  title         = {{High-Fidelity Simulation for Evaluating Robotic Vision Performance}},
  booktitle     = iros,
  year          = 2016,
  keywords      = {Animation and Simulation, Robot Vision, Recognition},
  abstract      = {Robotic vision, unlike computer vision, typically involves processing a stream of images from a camera with time varying pose operating in an environment with time varying lighting conditions and moving objects. Repeating robotic vision experiments under identical conditions is often impossible, making it difficult to compare different algorithms. For machine learning applications a critical bottleneck is the limited amount of real world image data that can be captured and labelled for both training and testing purposes. In this paper we investigate the use of a photo-realistic simulation tool to address these challenges, in three specific domains: robust place recognition, visual SLAM and object recognition. For the first two problems we generate images from a complex 3D environment with systematically varying camera paths, camera viewpoints and lighting conditions. For the first time we are able to systematically characterise the performance of these algorithms as paths and lighting conditions change. In particular, we are able to systematically generate varying camera viewpoint datasets that would be difficult or impossible to generate in the real world. We also compare algorithm results for a camera in a real environment and a simulated camera in a simulation model of that real environment. Finally, for the object recognition domain, we generate labelled image data and characterise the viewpoint dependency of a current convolution neural network in performing object recognition. Together these results provide a multi-domain demonstration of the beneficial properties of using simulation to characterise and analyse a wide range of robotic vision algorithms.},
}

@Article{canelhas2016ral,
  author        = {D.R. Canelhas and T. Stoyanov and A.J. Lilienthal},
  title         = {{From Feature Detection in Truncated Signed Distance Fields to Sparse Stable Scene Graphs}},
  journal       = ral,
  year          = 2016,
  keywords      = {Mapping, Recognition, RGB-D Perception},
  abstract      = {With the increased availability of GPUs and multicore CPUs, volumetric map representations are an increasingly viable option for robotic applications. A particularly important representation is the truncated signed distance field (TSDF) that is at the core of recent advances in dense 3D mapping. However, there is relatively little literature exploring the characteristics of 3D feature detection in volumetric representations. In this paper we evaluate the performance of features extracted directly from a 3D TSDF representation. We compare the repeatability of Integral invariant features, specifically designed for volumetric images, to the 3D extensions of Harris and Shi & Tomasi corners. We also study the impact of different methods for obtaining gradients for their computation. We motivate our study with an example application for building sparse stable scene graphs, and present an efficient GPU-parallel algorithm to obtain the graphs, made possible by the combination of TSDF and 3D feature points. Our findings show that while the 3D extensions of 2D corner-detection perform as expected, integral invariants have shortcomings when applied to discrete TSDFs. We conclude with a discussion of the cause for these points of failure that sheds light on possible mitigation strategies.},
}

@Article{patten2016ral,
  author        = {T. Patten and M. Zillich and R. Fitch and M. Vincze and S. Sukkarieh},
  title         = {{Viewpoint Evaluation for Online 3D Active Object Classification}},
  journal       = ral,
  year          = 2016,
  keywords      = {Object detection, Segmentation, Categorization, Semantic Scene Understanding, RGB-D Perception},
  abstract      = {We present an end-to-end method for active object classification in cluttered scenes from RGB-D data. Our algorithms predict the quality of future viewpoints in the form of entropy using both class and pose. Occlusions are explicitly modelled in predicting the visible regions of objects, which modulates the corresponding discriminatory value of a given view. We implement a one-step greedy planner and demonstrate our method online using a mobile robot. We also analyse the performance of our method compared to similar strategies in simulated execution using the Willow Garage dataset. Results show that our active method usefully reduces the number of views required to accurately classify objects in clutter as compared to traditional passive perception.},
}

@Article{neubert2016ral,
  author        = {P. Neubert and P. Protzel},
  title         = {{Beyond Holistic Descriptors, Keypoints and Fixed Patches: Multiscale Superpixel Grids for Place Recognition in Changing Environments}},
  journal       = ral,
  year          = 2016,
  keywords      = {Localization, Visual-Based Navigation, Recognition},
  abstract      = {Vision-based place recognition in environments subject to severe appearance changes due to day-night cycles, changing weather or seasons is a challenging task. Existing methods typically exploit image sequences, holistic descriptors and/or training data. Each of these approaches limits the practical applicability, e.g. to constant viewpoints for usage of holistic image descriptors. Recently, the combination of local region detectors and descriptors based on Convolutional Neural Networks showed to be a promising approach to overcome these limitations. However, established region detectors, for example keypoint detectors, showed severe problems to provide repetitive landmarks despite dramatically changed appearance of the environment. Thus, they are typically replaced by holistic image descriptors or fixedly arranged patches - both known to be sensitive towards viewpoint changes. In this paper, we present a novel local region detector, SP-Grid, that is particularly suited for the combination of severe appearance and viewpoint changes. It is based on multi-scale image oversegmentations and is designed to combine the advantages of keypoints and fixed image patches by starting from an initial grid-like arrangement and subsequently adapting to the image content. The gridlike arrangement showed to be beneficial in the presence of severe appearance changes and the adaptation to the image content increases the robustness towards viewpoint changes. The experimental evaluation will show the benefit compared to existing local region detectors and holistic image descriptors.},
}

@Article{jiang2016ral,
  author        = {C. Jiang and D.P. Paudel and Y. Fougerolle and D. Fofi and C. Demonceaux},
  title         = {{Static-Map and Dynamic Object Reconstruction in Outdoor Scenes Using 3D Motion Segmentation}},
  journal       = ral,
  year          = 2016,
  keywords      = {Mapping, Motion and Path Planning, SLAM},
  abstract      = {This paper aims to build the static-map of a dynamic scene using a mobile robot equipped with 3D sensors. The sought static-map consists of only the static scene parts, which has a vital role in scene understanding and landmark based navigation. Building static-map requires the categorization of moving and static objects. In this work, we propose a Sparse Subspace Clustering-based Motion Segmentation method that categories the static scene parts and the multiple moving objects using their 3D motion trajectories. Our motion segmentation method uses the raw trajectory data, allowing the objects to move in direct 3D space, without any projection model assumption or whatsoever. We also propose a complete pipeline for static-map building which estimates the inter-frame motion parameters by exploiting the minimal 3-point Random Sample Consensus algorithm on the feature correspondences only from the static scene parts. The proposed method has been especially designed and tested for large scene in real outdoor environments. On one hand, our 3D Motion Segmentation approach outperforms its 2D based counterparts, for extensive experiments on KITTI dataset. On the other hand, separately reconstructed static-maps and moving objects for various dynamic scenes are very satisfactory.},
}

@Article{wolf2016ral,
  author        = {D. Wolf and J. Prankl and M. Vincze},
  title         = {{Enhancing Semantic Segmentation for Robotics: The Power of 3D Entangled Forests}},
  journal       = ral,
  year          = 2016,
  keywords      = {Semantic Scene Understanding, RGB-D Perception, Computer Vision for Automation},
  abstract      = {We present a novel, fast and compact method to improve semantic segmentation of 3D point clouds, which is able to learn and exploit common contextual relations between observed structures and objects. Introducing 3D Entangled Forests (3DEF), we extend the concept of entangled features for decision trees to 3D point clouds, enabling the classifier not only to learn which labels are likely to occur close to each other, but also in which specific geometric configuration. Operating on a plane-based representation of a point cloud, our method does not require a final smoothing step and achieves state-of-the-art results on the NYU Depth Dataset in a single inference step. This compactness in turn allows for fast processing times, a crucial factor to consider for online applications on robotic platforms. In a thorough evaluation, we demonstrate the expressiveness of our new 3D entangled feature set and the importance of spatial context in the scope of semantic segmentation.},
}

@Article{vysotska2016ral,
  author        = {O. Vysotska and C. Stachniss},
  title         = {{Lazy Data Association for Image Sequences Matching under Substantial Appearance Changes}},
  journal       = ral,
  year          = 2016,
  keywords      = {Localization, Computer Vision for Transportation},
  abstract      = {Localization is an essential capability for mobile robots and the ability to localize in changing environments is key to robust outdoor navigation. Robots operating over extended periods of time should be able to handle substantial appearance changes such as those occurring over seasons or under different weather conditions. In this paper, we investigate the problem of efficiently coping with seasonal appearance changes in online localization. We propose a lazy data association approach for matching streams of incoming images to a reference image sequence in an online fashion. We present a search heuristic to quickly find matches between the current image sequence and a database using a data association graph. Our experiments conducted under substantial seasonal changes suggest that our approach can efficiently match image sequences while requiring a comparably small number of image to image comparisons.},
}

@Article{vitzrabin2016ral,
  author        = {E. Vitzrabin and Y. Edan},
  title         = {{Changing Task Objectives for Improved Sweet Pepper Detection for Robotic Harvesting}},
  journal       = ral,
  year          = 2016,
  keywords      = {Robotics in Agriculture and Forestry, Object detection, Segmentation, Categorization, Sensor Fusion},
  abstract      = {This paper presents a method to improve detection for robotic harvesting by changing the task objective in real time in an adaptive thresholding algorithm. The adaptive thresholding algorithm includes three main parts: 3D adaptive thresholding, object detection, and fusion. Optimal local 3D thresholds that were previously determined according to changing illumination conditions were expanded in this research to include also changing task objectives. The task objectives describe the relationships between False Positive Rate, True Positive Rate, and accuracy in the location. The first task objective aims to maximize detection and minimize false alarms so as to ensure the arm is directed only towards real fruits. The second task objective focuses on high accuracy in the detection. Intensive evaluations were conducted on databases which contained 240 images acquired in the field with various artificial illumination setups. The difference between the two tasks objectives was on average 0.09 in detection rates and 0.66 cm in the accuracy. Robotic experiments resulted in 26.6\% difference in pepper grasping success rate with two different task objectives indicating the importance of changing the task objectives for the fruit detection task.},
}

@Article{schor2016ral,
  author        = {N. Schor and A. Bechar and T. Ignat and A. Dombrovsky and Y. Elad and S. Berman},
  title         = {{Robotic Disease Detection in Greenhouses: Combined Detection of Powdery Mildew and Tomato Spotted Wilt Virus}},
  journal       = ral,
  year          = 2016,
  keywords      = {Agricultural Automation, Computer Vision for Automation},
  abstract      = {Robotic systems for disease detection in greenhouses are expected to improve disease control, increase yield, and reduce pesticide application. We present a robotic detection system for combined detection of two major threats of greenhouse bell peppers: Powdery mildew (PM) and Tomato spotted wilt virus (TSWV). The system is based on a manipulator which facilitates reaching multiple detection poses. Several detection algorithms are developed based on principal component analysis (PCA) and the coefficient of variation (CV). Tests ascertain the system can successfully detect the plant and reach the detection pose required for PM (along the side of the plant), yet it has difficulties in reaching the TSWV detection pose (above the plant). Increasing manipulator workvolume is expected to solve this issue. For TSWV, PCA-based classification with leaf vein removal, achieved the highest classification accuracy (90\%) while the accuracy of the CV methods was also high (85\%, 87\%). For PM, PCA-based pixellevel classification was high (95.2\%) while leaf condition classification accuracy was low (64.3\%) since it was determined based on the upper side of the leaf while disease symptoms start on its lower side. Exposure of the lower side of the leaf during detection is expected to improve PM condition detection.},
}

@Article{santos2016ral,
  author        = {J.M. Santos and T. Krajn{\'i}k and J.P. Fentanes and T. Duckett},
  title         = {{Lifelong Information-Driven Exploration to Complete and Refine 4D Spatio-Temporal Maps}},
  journal       = ral,
  year          = 2016,
  keywords      = {Mapping, Service Robots},
  abstract      = {This paper presents an exploration method that allows mobile robots to build and maintain spatio-temporal models of changing environments. The assumption of a perpetuallychanging world adds a temporal dimension to the exploration problem, making spatio-temporal exploration a never-ending, life-long learning process. We address the problem by application of information-theoretic exploration methods to spatiotemporal models that represent the uncertainty of environment states as probabilistic functions of time. This allows to predict the potential information gain to be obtained by observing a particular area at a given time, and consequently, to decide which locations to visit and the best times to go there. To validate the approach, a mobile robot was deployed continuously over 5 consecutive business days in a busy office environment. The results indicate that the robots ability to spot environmental changes improved as it refined its knowledge of the world dynamics. Index Terms mobile robotics, spatio-temporal exploration},
}

@Article{kaiser2016ral,
  author        = {J. Kaiser and A. Martinelli and F. Fontana and D. Scaramuzza},
  title         = {{Simultaneous State Initialization and Gyroscope Bias Calibration in Visual Inertial Aided Navigation}},
  journal       = ral,
  year          = 2016,
  keywords      = {Sensor Fusion, Localization, Visual-Based Navigation},
  abstract      = {State of the art approaches for visual-inertial sensor fusion use filter-based or optimization-based algorithms. Due to the nonlinearity of the system, a poor initialization can have a dramatic impact on the performance of these estimation methods. Recently, a closed-form solution providing such an initialization was derived in [1]. That solution determines the velocity (angular and linear) of a monocular camera in metric units by only using inertial measurements and image features acquired in a short time interval. In this paper, we study the impact of noisy sensors on the performance of this closed-form solution. We show that the gyroscope bias, not accounted for in [1], significantly affects the performance of the method. Therefore, we introduce a new method to automatically estimate this bias. Compared to the original method, the new approach now models the gyroscope bias and is robust to it. The performance of the proposed approach is successfully demonstrated on real data from a quadrotor MAV. Index Terms Sensor Fusion, Localization, Visual-Based Navigation},
}

@Article{kallasi2016ral,
  author        = {F. Kallasi and D.L. Rizzini and S. Caselli},
  title         = {{Fast Keypoint Features from Laser Scanner for Robot Localization and Mapping}},
  journal       = ral,
  year          = 2016,
  keywords      = {Range Sensing, Mapping},
  abstract      = {Detecting features in sensor measurements and distinguishing among them is an important capability for robot localization and navigation. Despite the wide diffusion of range finders, there are few works on keypoint features for 2D LIDAR and there is potential for improvement over the existing methods. This paper proposes two novel keypoint detectors for the stable detection of interest points in laser measurements and two descriptors for robust associations. The features defined by combining keypoints and descriptors allow stable and efficient place recognition. Experiments with standard benchmark datasets assess the performance of the detectors and descriptors investigated. One of the proposed features, termed FALKO-BSC, achieves higher repeatability score and similar descriptor performance compared with the FLIRT state-of-the-art feature. FALKO-BSC is also shown to enable effective localization.},
}

@Article{li2016ral,
  author        = {Z. Li and V. Isler},
  title         = {{Large Scale Image Mosaic Construction for Agricultural Applications}},
  journal       = ral,
  year          = 2016,
  keywords      = {Computer Vision for Other Robotic Applications, Robotics in Agriculture and Forestry},
  abstract      = {We present a novel technique for stitching images including those obtained from aerial vehicles flying at low altitudes. Existing image stitching/mosaicking methods rely on inter-image homography computation based on a planar scene assumption. This assumption holds when images are taken from high-altitudes (hence the depth variation is negligible). It is often violated when flying at low altitudes. Further, to avoid scale and resolution changes, existing methods rely on primarily translational motion at fixed altitudes. Our method removes these limitations and performs well even when aerial images are taken from low altitudes by an aerial vehicle performing complex motions. It starts by extracting the ground geometry from a sparse reconstruction of the scene obtained from a small fraction of the input images. Next, it selects the best image (from the entire sequence) for each location on the ground using a novel camera selection criterion. This image is then independently rectified to obtain the corresponding portion of the mosaic. Therefore, the technique avoids performing costly joint-optimization over the entire sequence. It is validated using challenging input sequences motivated by agricultural applications.},
}

@Article{khomutenko2016ral,
  author        = {B. Khomutenko and G. Garcia and P. Martinet},
  title         = {{An Enhanced Unified Camera Model}},
  journal       = ral,
  year          = 2016,
  keywords      = {Calibration and Identification, Omnidirectional Vision, Computer Vision for Automation},
  abstract      = {This paper describes a novel projection model based on the so-called unified projection model. The new model applies to catadioptric systems and wide-angle fish-eye cameras, it does not require additional mapping to model distortions, and it takes just two projection parameters more than a simple pinhole model to represent radial distortion (one parameter more than the unified model). Here we provide a study of different mathematical aspects of the model, its application limits, and explicit closed-form inversion. The latter allows to apply all the notions of epipolar geometry with no difficulties. Also we introduce a concept of projection surface, which is a useful notion to study and compare different projection models with radial distortion. Using developed software, several different lenses were calibrated using the proposed model, and in all cases sub-pixel precision was achieved.},
}

@Article{davis2016ral,
  author        = {B. Davis and I. Karamouzas and S.J. Guy},
  title         = {{C-OPT: Coverage-Aware Trajectory Optimization under Uncertainty}},
  journal       = ral,
  year          = 2016,
  keywords      = {Motion and Path Planning, Collision Avoidance, Reactive and Sensor-Based Planning},
  abstract      = {We introduce a new problem of continuous, coverage-aware trajectory optimization under localization and sensing uncertainty. In this problem, the goal is to plan a path from a start state to a goal state that maximizes the coverage of a user-specified region while minimizing the control costs of the robot and the probability of collision with the environment. We present a principled method for quantifying the coverage sensing uncertainty of the robot. We use this sensing uncertainty along with the uncertainty in robot localization to develop C-OPT, a coverage-optimization algorithm which optimizes trajectories over belief-space to find locally optimal coverage paths. We highlight the applicability of our approach in multiple simulated scenarios inspired by surveillance, UAV crop analysis, and search-and-rescue tasks. We also present a case study on a physical, differential-drive robot. We also provide quantitative and qualitative analysis of the paths generated by our approach.},
}

@Article{johnson2016ral,
  author        = {A. Johnson and J. King and S. Srinivasa},
  title         = {{Convergent Planning}},
  journal       = ral,
  year          = 2016,
  keywords      = {Motion and Path Planning, Manipulation Planning, Field Robots},
  abstract      = {We propose a number of divergence metrics to quantify the robustness of a trajectory to state uncertainty for under-actuated or under-sensed systems. These metrics are inspired by contraction analysis and we demonstrate their use to guide randomized planners towards more convergent trajectories through three extensions to the kinodynamic RRT. The first strictly thresholds action selection based on these metrics, forcing the planner to find a solution that lies within a contraction region over which all initial conditions converge exponentially to a single trajectory. However, finding such a monotonically contracting plan is not always possible. Thus, we propose a second method that relaxes these strict requirements to find convergent (i.e. low-divergence) plans. The third algorithm uses these metrics for post-planning path selection. Two examples test the ability of these metrics to lead the planners to more robust trajectories: a mobile robot climbing a hill and a manipulator rearranging objects on a table.},
}

@Article{osswald2016ral,
  author        = {S. Osswald and M. Bennewitz and W. Burgard and C. Stachniss},
  title         = {{Speeding-Up Robot Exploration by Exploiting Background Information}},
  journal       = ral,
  year          = 2016,
  keywords      = {Mapping, Motion and Path Planning},
  abstract      = {The ability to autonomously learn a model of an environment is an important capability of a mobile robot. In this paper, we investigate the problem of exploring a scene given background information in form of a topo-metric graph of the environment. Our method is relevant for several real-world applications in which the rough structure of the environment is known beforehand. We present an approach that exploits such background information and enables a robot to cover the environment with its sensors faster compared to a greedy exploration system without this information. We implemented our exploration system in ROS and evaluated it in different environments. As the experimental results demonstrate, our proposed method significantly reduces the overall trajectory length needed to cover the environment with the robots sensors and thus yields a more efficient exploration strategy compared to state-of-the-art greedy exploration, if the additional information is available.},
}

@Article{schneider2016ral,
  author        = {J. Schneider and C. Stachniss and W. F{\"o}rstner},
  title         = {{On the Accuracy of Dense Fisheye Stereo}},
  journal       = ral,
  year          = 2016,
  keywords      = {Range Sensing, Mapping, Computer Vision for Other Robotic Applications},
  abstract      = {Fisheye cameras offer a large field of view, which is important for several robotics applications as a larger field of view allows for covering a large area with a single image. In contrast to classical cameras, however, fisheye cameras cannot be approximated well using the pinhole camera model and this renders the computation of depth information from fisheye stereo image pairs more complicated. In this work, we analyze the combination of an epipolar rectification model for fisheye stereo cameras with existing dense methods. This has the advantage that existing dense stereo systems can be applied as a black-box even with cameras that have field of view of more than 180 deg to obtain dense disparity information. We thoroughly investigate the accuracy potential of such fisheye stereo systems using image data from our UAV. The empirical analysis is based on image pairs of a calibrated fisheye stereo camera system and two state-of-the-art algorithms for dense stereo applied to adequately rectified image pairs from fisheye stereo cameras. The canonical stochastic model for sensor points assumes homogeneous uncertainty and we generalize this model based on an empirical analysis using a test scene consisting of mutually orthogonal planes. We show (1) that the combination of adequately rectified fisheye image pairs and dense methods provides dense 3D point clouds at 6-7 Hz on our autonomous multi-copter UAV, (2) that the uncertainty of points depends on their angular distance from the optical axis, (3) how to estimate the variance component as a function of that distance, and (4) how the improved stochastic model improves the accuracy of the scene points.},
  url           = {http://www.ipb.uni-bonn.de/wp-content/papercite-data/pdf/schneider16ral.pdf},
}

@Article{fulhammer2016ral,
  author        = {T. Fulhammer and R. Ambrus and C. Burbridge and M. Zillich and J. Folkesson and N. Hawes and P. Jensfelt and M. Vincze},
  title         = {{Autonomous Learning of Object Models on a Mobile Robot}},
  journal       = ral,
  year          = 2016,
  keywords      = {Visual Learning, Object detection, Segmentation, Categorization, Autonomous Agents},
  abstract      = {In this article we present and evaluate a system which allows a mobile robot to autonomously detect, model and re-recognize objects in everyday environments. Whilst other systems have demonstrated one of these elements, to our knowledge we present the first system which is capable of doing all of these things, all without human interaction, in normal indoor scenes. Our system detects objects to learn by modelling the static part of the environment and extracting dynamic elements. It then creates and executes a view plan around a dynamic element to gather additional views for learning. Finally these views are fused to create an object model. The performance of the system is evaluated on publicly available datasets as well as on data collected by the robot in both controlled and uncontrolled scenarios.},
}

@Article{adarve2016ral,
  author        = {J.D. Adarve and R. Mahony},
  title         = {{A Filter Formulation for Computing Real Time Optical Flow}},
  journal       = ral,
  year          = 2016,
  keywords      = {Computer Vision for Transportation, Visual Tracking, Computer Vision for Other Robotic Applications},
  abstract      = {Dense optical flow is a crucial visual cue for obstacle avoidance and motion control for robotic systems functioning in complex unstructured environments. In order to compute image motion for modern highly dynamic robots, it is necessary to use high speed vision systems. To perceive small and thin objects such as tree branches, fence poles, and other similar objects, high resolution vision systems must be used. The data stream from a high-resolution, high-speed vision system will saturate the onboard data bus and overload the onboard processor of almost all existing robotic systems. To implement such a vision system, we believe it is necessary to process the data in situ at the camera using dedicated processing hardware, a vision processing paradigm that we term Rapid Embedded Vision (REV) systems. Even then, the task of processing multiple hundred hertz of dense high-resolution images is not possible using classical algorithms and classical computing hardware. A new processing paradigm that actively exploits the high frame rate of the image sequences, and high performance computing hardware such as GPU or FPGA must be employed. This paper proposes a filtering algorithm for the computation of dense optical flow fields in real time. The filter is designed as a pyramidal structure of update and propagation loops, where an optical flow state is constantly refined with new image data from the camera. The computational properties of the proposed algorithm makes it well suited for implementation in GPU and FPGA systems. We present results from a GPU implementation achieving frame rates in the order of 800 Hz at VGA resolution. Experimental validation and comparison is provided for both synthetic and real life high speed video sequences at frame rates of 300 Hz and 1016 544 pixel resolution.},
}

@Article{costante2016ral,
  author        = {G. Costante and M. Mancini and P. Valigi and T.A. Ciarfuglia},
  title         = {{Exploring Representation Learning with CNNs for Frame to Frame Ego-Motion Estimation}},
  journal       = ral,
  year          = 2016,
  keywords      = {Visual Learning, Visual-Based Navigation, CNN},
  abstract      = {Visual Ego-Motion Estimation, or briefly Visual Odometry (VO), is one of the key building blocks of modern SLAM systems. In the last decade, impressive results have been demonstrated in the context of visual navigation, reaching very high localization performance. However, all ego-motion estimation systems require careful parameter tuning procedures for the specific environment they have to work in. Furthermore, even in ideal scenarios, most state-of-the-art approaches fail to handle image anomalies and imperfections, which results in less robust estimates. VO systems that rely on geometrical approaches extract sparse or dense features and match them to perform Frame to Frame (F2F) motion estimation. However, images contain much more information that can be used to further improve the F2F estimation. To learn new feature representation a very successful approach is to use deep Convolutional Neural Networks. Inspired by recent advances in Deep Networks and by previous work on learning methods applied to VO, we explore the use of Convolutional Neural Networks to learn both the best visual features and the best estimator for the task of visual Ego-Motion Estimation. With experiments on publicly available datasets we show that our approach is robust with respect to blur, luminance and contrast anomalies and outperforms most state-of-the-art approaches even in nominal conditions.},
}

@Article{delmerico2017ral,
  author        = {J. Delmerico and E. Mueggler and J. Nitsch and D. Scaramuzza},
  title         = {{Active Autonomous Aerial Exploration for Ground Robot Path Planning}},
  journal       = ral,
  year          = 2017,
  keywords      = {Search and Rescue Robots, Motion and Path Planning, Visual-Based Navigation},
  abstract      = {We address the problem of planning a path for a ground robot through unknown terrain, using observations from a flying robot. In search and rescue missions, which are our target scenarios, the time from arrival at the disaster site to the delivery of aid is critically important. Previous works required exhaustive exploration before path planning, which is time-consuming but eventually leads to an optimal path for the ground robot. Instead, we propose active exploration of the environment, where the flying robot chooses regions to map in a way that optimizes the overall response time of the system, which is the combined time for the air and ground robots to execute their missions. In our approach, we estimate terrain classes throughout our terrain map, and we also add elevation information in areas where the active exploration algorithm has chosen to perform 3D reconstruction. This terrain information is used to estimate feasible and efficient paths for the ground robot. By exploring the environment actively, we achieve superior response times compared to both exhaustive and greedy exploration strategies. We demonstrate the performance and capabilities of the proposed system in simulated and real-world outdoor experiments. To the best of our knowledge, this is the first work to address ground robot path planning using active aerial exploration.},
}

@Article{ambrus2017ral,
  author        = {R. Ambrus and S. Claici and A.J. Wendt},
  title         = {{Automatic Room Segmentation from Unstructured 3D Data of Indoor Environments}},
  journal       = ral,
  year          = 2017,
  keywords      = {Mapping, Semantic Scene Understanding, RGB-D Perception},
  abstract      = {We present an automatic approach for the task of reconstructing a 2D floor plan from unstructured point clouds of building interiors. Our approach emphasizes accurate and robust detection of building structural elements, and unlike previous approaches does not require prior knowledge of scanning device poses. The reconstruction task is formulated as a multiclass labeling problem that we approach using energy minimization. We use intuitive priors to define the costs for the energy minimization problem, and rely on accurate wall and opening detection algorithms to ensure robustness. We provide detailed experimental evaluation results, both qualitative and quantitative, against state of the art methods and labeled ground truth data.},
}

@Article{jadidi2017ral,
  author        = {M.G. Jadidi and J.V. Miro and G. Dissanayake},
  title         = {{Warped Gaussian Processes Occupancy Mapping with Uncertain Inputs}},
  journal       = ral,
  year          = 2017,
  keywords      = {Mapping},
  abstract      = {In this paper, we study extensions to the Gaussian Processes (GPs) continuous occupancy mapping problem. There are two classes of occupancy mapping problems that we particularly investigate. The first problem is related to mapping under pose uncertainty and how to propagate pose estimation uncertainty into the map inference. We develop expected kernel and expected sub-map notions to deal with uncertain inputs. In the second problem, we account for the complication of the robots perception noise using Warped Gaussian Processes (WGPs). This approach allows for non-Gaussian noise in the observation space and captures the possible nonlinearity in that space better than standard GPs. The developed techniques can be applied separately or concurrently to a standard GP occupancy mapping problem. According to our experimental results, although taking into account pose uncertainty leads, as expected, to more uncertain maps, by modeling the nonlinearities present in the observation space WGPs can improve the map quality.},
}

@Article{mur-artal2017ral,
  author        = {R. Mur-Artal and J.D. Tardos},
  title         = {{Visual-Inertial Monocular SLAM with Map Reuse}},
  journal       = ral,
  year          = 2017,
  keywords      = {SLAM, Sensor Fusion, Visual-Based Navigation},
  abstract      = {In recent years there have been excellent results in Visual-Inertial Odometry techniques, which aim to compute the incremental motion of the sensor with high accuracy and robustness. However these approaches lack the capability to close loops, and trajectory estimation accumulates drift even if the sensor is continually revisiting the same place. In this work we present a novel tightly-coupled Visual-Inertial Simultaneous Localization and Mapping system that is able to close loops and reuse its map to achieve zero-drift localization in already mapped areas. While our approach can be applied to any camera configuration, we address here the most general problem of a monocular camera, with its well-known scale ambiguity. We also propose a novel IMU initialization method, which computes the scale, the gravity direction, the velocity, and gyroscope and accelerometer biases, in a few seconds with high accuracy. We test our system in the 11 sequences of a recent micro-aerial vehicle public dataset achieving a typical scale factor error of 1\% and centimeter precision. We compare to the state-of-the-art in visual-inertial odometry in sequences with revisiting, proving the better accuracy of our method due to map reuse and no drift accumulation. Index Terms SLAM, Sensor Fusion, Visual-Based Navigation},
}

@Article{sa2017ral,
  author        = {I. Sa and C. Lehnert and A. English and C.S. McCool and F. Dayoub and B. Upcroft and T. Perez},
  title         = {{Peduncle Detection of Sweet Pepper for Autonomous Crop Harvesting - Combined Colour and 3D Information}},
  journal       = ral,
  year          = 2017,
  keywords      = {Agricultural Automation, Robotics in Agriculture and Forestry, RGB-D Perception},
  abstract      = {This paper presents a 3D visual detection method for the challenging task of detecting peduncles of sweet peppers (Capsicum annuum) in the field. Cutting the peduncle cleanly is one of the most difficult stages of the harvesting process, where the peduncle is the part of the crop that attaches it to the main stem of the plant. Accurate peduncle detection in 3D space is therefore a vital step in reliable autonomous harvesting of sweet peppers, as this can lead to precise cutting while avoiding damage to the surrounding plant. This paper makes use of both colour and geometry information acquired from an RGB-D sensor and utilises a supervised-learning approach for the peduncle detection task. The performance of the proposed method is demonstrated and evaluated using qualitative and quantitative results (the Area-Under-the-Curve (AUC) of the detection precision-recall curve). We are able to achieve an AUC of 0.71 for peduncle detection on field-grown sweet peppers. We release a set of manually annotated 3D sweet pepper and peduncle images to assist the research community in performing further research on this topic.},
}

@Article{daudelin2017ral,
  author        = {J.J. Daudelin and M. Campbell},
  title         = {{An Adaptable, Probabilistic, Next Best View Algorithm for Reconstruction of Unknown 3D Objects}},
  journal       = ral,
  year          = 2017,
  keywords      = {Autonomous Agents, Probability and Statistical Methods, Motion and Path Planning},
  abstract      = {Autonomous mobile robots perform many tasks, such as grasping and inspection, that may require complete models of 3D objects in the environment. If little or no knowledge about an object is known a priori, the robot must take sensor measurements from strategically determined viewpoints in order to reconstruct a 3D model of the object. We propose an autonomous object reconstruction approach for mobile robots that is very general, with no assumptions about object shape or size, such as a bounding box or predetermined set of candidate viewpoints. A probabilistic, volumetric method for determining the optimal next best view is developed based on a partial model of a 3D object of unknown shape and size. The proposed method integrates an object probability characteristic to determine sensor views that incrementally reconstruct a 3D model of the object. Experiments in simulation and on a real world robot validate the work and compare it to the state of the art.},
}

@Article{lehnert2017ral,
  author        = {C. Lehnert and A. English and C.S. McCool and A.M.W. Tow and T. Perez},
  title         = {{Autonomous Sweet Pepper Harvesting for Protected Cropping Systems}},
  journal       = ral,
  year          = 2017,
  keywords      = {Agricultural Automation, Dexterous Manipulation, Mechanism Design of Manipulators},
  abstract      = {In this paper we present a new robotic harvester (Harvey) that can autonomously harvest sweet pepper in protected cropping environments. Our approach combines effective vision algorithms with a novel end-effector design to enable successful harvesting of sweet peppers. Initial field trials in protected cropping environments, with two cultivar, demonstrate the efficacy of this approach achieving a 46\% success rate for unmodified crop, and 58\% for modified crop. Furthermore, for the more favourable cultivar we were also able to detach 90\% of sweet peppers, indicating that improvements in the grasping success rate would result in greatly improved harvesting performance.},
}

@Article{chen2017ral,
  author        = {S.W. Chen and S. Skandan and S. Dcunha and J. Das and C. Qu and C.J. Taylor and V. Kumar},
  title         = {{Counting Apples and Oranges with Deep Learning: A Data Driven Approach}},
  journal       = ral,
  year          = 2017,
  keywords      = {Agricultural Automation, Object detection, Segmentation, Categorization, Visual Learning},
  abstract      = {This paper describes a fruit counting pipeline based on deep learning that accurately counts fruit in unstructured environments. Obtaining reliable fruit counts is challenging because of variations in appearance due to illumination changes and occlusions from foliage and neighboring fruits. We propose a novel approach that uses deep learning to map from input images to total fruit counts. The pipeline utilizes a custom crowd-sourcing platform to quickly label large data sets. A blob detector based on a fully convolutional network extracts candidate regions in the images. A counting algorithm based on a second convolutional network then estimates the number of fruit in each region. Finally, a linear regression model maps that fruit count estimate to a final fruit count. We analyze the performance of the pipeline on two distinct data sets of oranges in daylight, and green apples at night, utilizing human generated labels as ground truth. We also show that the pipeline has a short training time and performs well with a limited data set size. Our method generalizes across both data sets and is able to perform well even on highly occluded fruits that are challenging for human labelers to annotate.},
}

@Article{bonanni2017ral,
  author        = {T.M. Bonanni and B.D. Corte and G. Grisetti},
  title         = {{3D Map Merging}},
  journal       = ral,
  year          = 2017,
  keywords      = {Mapping, SLAM, Localization},
  abstract      = {In this paper, we propose an approach for merging 3D maps represented as pose graphs of point clouds. Our method can effectively deal with typical distortions affecting SLAM-generated maps. Traditional map merging techniques that use a single rigid body transformation to relate the reference frames of different maps. Instead, our approach achieves more accurate results by eliminating the inconsistencies resulting from distortions affecting the inputs, and can succeed in those situations where traditional approaches fail for substantial deformations. The core idea behind our solution is to localize the robot in a reference map by using the data from another map as observations. We validated our approach on publicly available datasets, and provide quantitative results that confirm its effectiveness on challenging instances of the merging problem.},
}

@Article{deray2017ral,
  author        = {J. Deray and J. Sol and J. Andrade-Cetto},
  title         = {{Word Ordering and Document Adjacency for Large Loop Closure Detection in 2D Laser Maps}},
  journal       = ral,
  year          = 2017,
  keywords      = {Localization, Recognition, SLAM},
  abstract      = {We address in this paper the problem of loop closure detection for laser-based simultaneous localization and mapping (SLAM) of very large areas. Consistent with the state of the art, the map is encoded as a graph of poses, and to cope with very large mapping capabilities, loop closures are asserted by comparing the features extracted from a query laser scan against a previously acquired corpus of scan features using a bag-of-words (BoW) scheme. Two contributions are here presented. First, to benefit from the graph topology, feature frequency scores in the BoW are computed not only for each individual scan but also from neighboring scans in the SLAM graph. This has the effect of enforcing neighbor relational information during document matching. Secondly, a weak geometric check that takes into account feature ordering and occlusions is introduced that substantially improves loop closure detection performance. The two contributions are evaluated both separately and jointly on four common SLAM datasets, and are shown to improve the state-of-the-art performance both in terms of precision and recall in most of the cases. Moreover, our current implementation is designed to work at nearly frame rate, allowing loop closure query resolution at nearly 22 Hz for the best case scenario and 2 Hz for the worst case scenario.},
}

@Article{schmidt2017ral,
  author        = {T. Schmidt and R. Newcombe and D. Fox},
  title         = {{Self-Supervised Learning of Dense Visual Descriptors}},
  journal       = ral,
  year          = 2017,
  keywords      = {Visual Learning, RGB-D Perception, Recognition},
  abstract      = {Robust estimation of correspondences between image pixels is an important problem in robotics, with applications in tracking, mapping, and recognition of objects, environments, and other agents. Correspondence estimation has long been the domain of hand-engineered features, but more recently deep learning techniques have provided powerful tools for learning features from raw data. The drawback of the latter approach is that a vast amount of (labelled, typically) training data is required for learning. This paper advocates a new approach to learning visual descriptors for dense correspondence estimation in which we harness the power of a strong 3D generative model to automatically label correspondences in RGB-D video data. A fully-convolutional network is trained using a contrastive loss to produce viewpoint- and lighting-invariant descriptors. As a proof of concept, we collected two datasets: the first depicts the upper torso and head of the same person in widely varied settings, and the second depicts an office as seen on multiple days with objects re-arranged within. Our datasets focus on re-visitation of the same objects and environments, and we show that by training the CNN only from local tracking data, our learned visual descriptor generalizes towards identifying non-labelled correspondences across videos. We furthermore show that our approach to descriptor learning can be used to achieve state-of-the-art single-frame localization results on the MSR 7-scenes dataset without using any labels identifying correspondences between separate videos of the same scenes at training time.},
}

@Article{mccool2017ral,
  author        = {C.S. McCool and T. Perez and B. Upcroft},
  title         = {{Mixtures of Lightweight Deep Convolutional Neural Networks: Applied to Agricultural Robotics}},
  journal       = ral,
  year          = 2017,
  keywords      = {Computer Vision for Automation, Recognition, Agricultural Automation, CNN},
  abstract      = {We propose a novel approach for training deep convolutional neural networks (DCNNs) that allows us to tradeoff complexity and accuracy to learn lightweight models suitable for robotic platforms such as AgBot II (which performs automated weed management). Our approach consists of three stages, the first is to adapt a pre-trained model to the task at hand. This provides state-of-the-art performance but at the cost of high computational complexity resulting in a low frame rate of just 0.12 frames per second (fps). Second, we use the adapted model and employ model compression techniques to learn a lightweight DCNN that is less accurate but has two orders of magnitude fewer parameters. Third, K lightweight models are combined as a mixture model to further enhance the performance of the lightweight models. Applied to the challenging task of weed segmentation, we improve accuracy from 85.9\%, using a traditional approach, to 93.9\% by adapting a complicated pre-trained DCNN with 25M parameters (Inception-v3). The downside to this adapted model, Adapted-IV3, is that it can only process 0.12fps. To make this approach fast while still retaining accuracy, we learn lightweight DCNNs which when combined can achieve accuracy greater than 90\% while using considerably fewer parameters capable of processing between 1.07 and 1.83 frames per second, up to an order of magnitude faster and up to an order of magnitude fewer parameters.},
}

@Article{alismail2017ral,
  author        = {H. Alismail and M. Kaess and B. Browning and S. Lucey},
  title         = {{Direct Visual Odometry in Low Light Using Binary Descriptors}},
  journal       = ral,
  year          = 2017,
  keywords      = {Visual-Based Navigation, SLAM, Mining Robotics},
  abstract      = {Feature descriptors are powerful tools for photometrically and geometrically invariant image matching. To date, however, their use has been tied to sparse interest point detection, which is susceptible to noise under adverse imaging conditions. In this work, we propose to use binary feature descriptors in a direct tracking framework without relying on sparse interest points. This novel combination of feature descriptors and direct tracking is shown to achieve robust and efficient visual odometry with applications to poorly lit subterranean environments.},
}

@InProceedings{nardi2016iros,
  author        = {L. Nardi and C. Stachniss},
  title         = {{Experience-Based Path Planning for Mobile Robots Exploiting User Preferences}},
  booktitle     = iros,
  year          = 2016,
  keywords      = {Planning, Navigation, Mobile Robots},
  url           = {http://www.ipb.uni-bonn.de/pdfs/nardi16iros.pdf},
}

@Article{wang2014arxiv,
  title         = {{Hashing for similarity search: A survey}},
  author        = {J. Wang, and H.T. Shen and J. Song and J. Ji},
  journal       = arxiv,
  year          = {2014},
  volume        = {abs/1408.2927},
  keywords      = {Hashing, Locality Sensitive Hashing},
  url           = {https://arxiv.org/pdf/1408.2927},
}

@article{dellacorte2018icra,
  author    = {B. {Della Corte} and I. Bogoslavskyi and C. Stachniss and G. Grisetti},
  title     = {A General Framework for Flexible Multi-Cue Photometric Point Cloud
               Registration},
  journal   = icra,
  year      = {2018},
}

@Article{weiss2008nips,
  title         = {{Spectral Hashing}},
  author        = {Y. Weiss and A. Torralba and R. Fergus},
  doi           = {10.1.1.141.7577},
  isbn          = {9781605609492},
  journal       = nipsjournal,
  keywords      = {Hashing, Spectral Clustering},
  number        = {1},
  pages         = {1--8},
  url           = {https://people.csail.mit.edu/torralba/publications/spectralhashing.pdf},
  year          = 2008,
}

@Article{sunderhauf2015rss,
  author        = {N. S{\"u}nderhauf and S. Shirazi and A. Jacobson and F. Dayoub and E. Pepperell and B. Upcroft and M. Milford},
  keywords      = {Place Recognition, CNN, Image Features},
  title         = {{Place Recognition with ConvNet Landmarks : Viewpoint-Robust, Condition-Robust, Training-Free}},
  journal       = rss,
  url           = {https://eprints.qut.edu.au/84931/1/rss15_placeRec.pdf},
  year          = 2015,
}

@Article{roy1999nips,
  title         = {{Coastal navigation: Mobile robot navigation with uncertainty in dynamic environments}},
  author        = {N. Roy and W. Burgard and D. Fox and S. Thrun},
  doi           = {10.1109/ROBOT.1999.769927},
  journal       = nipsjournal,
  pages         = {1043--1049},
  volume        = {12},
  keywords      = {Navigation, Planning, Coastal Navigation, POMDP},
  year          = 1999,
  url           = {http://www.ipb.uni-bonn.de/pdfs/roy1999nips.pdf},
}

@InProceedings{newman2009rss,
  author        = {M. Cummins AND P. Newman},
  title         = {Highly scalable appearance-only {SLAM} - {FAB-MAP} 2.0},
  booktitle     = rss,
  year          = 2009,
  keywords      = {SLAM, Localization, Visual Navigation, Place Recognition},
}

@Article{davison2007pami,
  author        = {A.J. Davison and I.D. Reid and N.D. Molton and O. Stasse},
  title         = {{MonoSLAM: Real-time single camera SLAM}},
  journal       = pami,
  year          = 2007,
  volume        = {29},
  keywords      = {SLAM, Visual SLAM},
}

@Article{agrawal2008tro,
  title         = {{FrameSLAM: From Bundle Adjustment to Real-Time Visual Mapping}},
  journal       = tro,
  volume        = {24},
  number        = {5},
  year          = 2008,
  author        = {M. Agrawal and K. Konolige},
  keywords      = {SLAM, Visual SLAM},
}

@InProceedings{glover2010icra,
  author        = {A.J. Glover and W.P. Maddern and M. Milford and G.F. Wyeth},
  booktitle     = icra,
  pages         = {3507-3512},
  title         = {{FAB-MAP} + {RatSLAM}: Appearance-based SLAM for multiple times of day.},
  year          = 2010,
  keywords      = {SLAM, Localization, Visual Navigation, Place Recognition},
}

@Article{bay2008cviu,
  author        = {H. Bay and A. Ess and T. Tuytelaars and L. Van Gool},
  title         = {Speeded-Up Robust Features ({SURF})},
  journal       = cviu,
  volume        = {110},
  number        = {3},
  year          = 2008,
  pages         = {346--359},
  keywords      = {Image Features},
}

@InProceedings{naseer2014aaai,
  title         = {{Robust Visual Robot Localization Across Seasons using Network Flows}},
  author        = {T. Naseer and L. Spinello and W. Burgard and C. Stachniss},
  booktitle     = aaai,
  year          = 2014,
  url           = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/naseer14aaai.pdf},
  keywords      = {Localization, Visual Navigation, Place Recognition},
}

@Article{lowe2004ijcv,
  author        = {D.G. Lowe},
  title         = {{Distinctive Image Features from Scale-Invariant Keypoints}},
  journal       = ijcv,
  volume        = {60},
  number        = {2},
  year          = 2004,
  issn          = {0920-5691},
  pages         = {91--110},
  numpages      = {20},
  keywords      = {Localization, Image Features},
}

@InProceedings{biber2005rss,
  author        = {P. Biber and T. Duckett},
  booktitle     = rss,
  pages         = {17-24},
  title         = {{Dynamic Maps for Long-Term Operation of Mobile Service Robots}},
  keywords      = {Localization, Changing Environments},
  year          = 2005,
}

@Article{furgale2010jfr,
  author        = {P.T. Furgale and T.D. Barfoot},
  journal       = jfr,
  pages         = {534-560},
  title         = {{Visual teach and repeat for long-range rover autonomy}},
  keywords      = {Localization, Visual Navigation, Place Recognition},
  volume        = {27},
  year          = {2010},
}

@InProceedings{milford2012icra,
  author        = {M. Milford and G.F. Wyeth},
  booktitle     = icra,
  title         = {{SeqSLAM: Visual route-based navigation for sunny summer days and stormy winter nights.}},
  year          = {2012},
  keywords      = {Localization, Visual Navigation, Place Recognition},
}

@InProceedings{neubert2013ecmr,
  author        = {P. Neubert and N. Sunderhauf and P. Protzel},
  title         = {{Appearance Change Prediction for Long-Term Navigation Across Seasons}},
  booktitle     = ecmr,
  keywords      = {Localization, Visual Navigation, Place Recognition},
  year          = {2013},
}

@Article{churchill2013ijrr,
  author        = {W. Churchill and P. Newman},
  journal       = ijrr,
  keywords      = {Localization, Visual Navigation},
  pdf           = {http://www.robots.ox.ac.uk/~mobile/Papers/2013IJRR_Churchill.pdf},
  title         = {{Experience-Based Navigation for Long-Term Localisation}},
  year          = 2013,
}

@InProceedings{churchill2012icra,
  author        = {W. Churchill and P. Newman},
  booktitle     = icra,
  title         = {{Practice Makes Perfect? Managing and Leveraging Visual Experiences for Lifelong Navigation}},
  keywords      = {Localization, Visual Navigation},
  year          = 2012,
}

@InProceedings{johns2013icra,
  author        = {E. Johns and G.-Z. Yang},
  booktitle     = icra,
  title         = {{Feature Co-occurrence Maps: Appearance-Based Localisation Throughout the Day}},
  keywords      = {Localization},
  year          = 2013,
}

@Article{valgren2010jras,
  author        = {C. Valgren and A.J. Lilienthal },
  title         = {{SIFT, SURF \& Seasons: Appearance-Based Long-Term Localization in Outdoor Environments}},
  journal       = jras,
  number        = 2,
  year          = 2010,
  volume        = {85},
  pages         = {149-156},
  keywords      = {Localization, Image Features},
}

@Article{zeiler2013arxiv,
  author        = {M.D. Zeiler and R. Fergus},
  title         = {{Visualizing and Understanding Convolutional Networks}},
  journal       = arxiv,
  volume        = {abs/1311.2901},
  year          = 2013,
  keywords      = {CNN},
  url           = {http://arxiv.org/pdf/1311.2901},
  abstract      = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky \etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets. },
}

@Article{manikandasriram2017arxiv,
  author        = {S.R. Manikandasriram and C. Anderson and R. Vasudevan and M. Johnson-Roberson},
  title         = {{Failing to Learn: Autonomously Identifying Perception Failures for Self-driving Cars}},
  journal       = arxiv,
  volume        = {1707.00051},
  year          = 2017,
  keywords      = {Autonomous Driving, Classification, CNN},
  url           = {http://arxiv.org/pdf/1707.00051},
  abstract      = {One of the major open challenges in self-driving cars is the ability to detect cars and pedestrians to safely navigate in the world. Deep learning-based object detector approaches have enabled great advances in using camera imagery to detect and classify objects. But for a safety critical application such as autonomous driving, the error rates of the current state-of-the-art are still too high to enable safe operation. Moreover, our characterization of object detector performance is primarily limited to testing on prerecorded datasets. Errors that occur on novel data go undetected without additional human labels. In this paper, we propose an automated method to identify mistakes made by object detectors without ground truth labels. We show that inconsistencies in object detector output between a pair of similar images can be used to identify false negatives(e.g. missed detections). In particular, we study two distinct cues - temporal and stereo inconsistencies - using data that is readily available on most autonomous vehicles. Our method can be used with any camera-based object detector and we evaluate the technique on several sets of real world data. The proposed method achieves over 97\% precision in automatically identifying missed detections produced by one of the leading state-of-the-art object detectors in the literature. We also release a new tracking dataset with over 100 sequences totaling more than 80,000 labeled images from a game engine to facilitate further research. },
}

@Article{cadena2016tro,
  author        = {C. Cadena and L. Carlone and H. Carrillo and Y. Latif and D. Scaramuzza and J. Neira and I. Reid and J.J. Leonard},
  title         = {{Past, Present, and Future of Simultaneous Localization And Mapping: Towards the Robust-Perception Age}},
  journal       = tro,
  volume        = 32,
  issue         = 6,
  year          = 2016,
  pages         = {1309-1332},
  keywords      = {SLAM, Survey},
  url           = {http://arxiv.org/pdf/1606.05830v4},
}

@Article{jung2017ijgi,
  author        = {J. Jung and C. Stachniss and C. Kim},
  title         = {{Automatic room segmentation of 3D laser data using morphological processing}},
  journal       = ijgi,
  year          = 2017,
}

@Article{chebrolu2017ijrr,
  author        = {N. Chebrolu and P. Lottes and A. Schaefer and W. Winterhalter and W. Burgard and C. Stachniss},
  title         = {{Agricultural Robot Dataset for Plant Classification, Localization and Mapping on Sugar Beet Fields}},
  journal       = ijrr,
  year          = 2017,
}

@Article{merfels2017pfg,
  author        = {C. Merfels and C. Stachniss},
  title         = {{Sensor Fusion for Self-Localisation of Automated Vehicles}},
  journal       = pfg,
  year          = 2017,
  doi           = {10.1007/s41064-017-0008-1},
}

@Article{bogoslavskyi2017pfg,
  author        = {I. Bogoslavskyi and C. Stachniss},
  title         = {{Efficient Online Segmentation for Sparse 3D Laser Scans}},
  journal       = pfg,
  year          = 2017,
  pages         = {41-52},
  doi           = {10.1007/s41064-016-0003},
}

@Article{vysotska2017pfg,
  author        = {O. Vysotska and C. Stachniss},
  title         = {{Improving SLAM by Exploiting Building Information from Publicly Available Maps and Localization Priors}},
  journal       = pfg,
  year          = 2017,
  volume        = 85,
  number        = 1,
  pages         = {53-65},
  doi           = {10.1007/s41064-017-0006-3},
}

@InProceedings{beekmans2017egu,
  author        = {Ch. Beekmans and J. Schneider and T. Laebe and M. Lennefer and C. Stachniss and C. Simmer},
  title         = {{3D-Cloud Morphology and Motion from Dense Stereo for Fisheye Cameras}},
  booktitle     = {In Proc.~of the Europ.~Geosciences Union General Assembly (EGU)},
  year          = 2017,
}

@InProceedings{liebisch2017earsel,
  author        = {F. Liebisch and M. Popovic and J. Pfeifer and R. Khanna and P. Lottes and C. Stachniss and A. Pretto and S. In Kyu and J. Nieto and R. Siegwart and A. Walter},
  title         = {{Automatic UAV-based field inspection campaigns for weeding in row crops}},
  booktitle     = {In Proc.~of the 10th EARSeL SIG Imaging Spectroscopy Workshop},
  year          = 2017,
}

@Article{beekmans2016acp,
  author        = {Ch. Beekmans and J. Schneider and T. Laebe and M. Lennefer and C. Stachniss and C. Simmer},
  title         = {{Cloud Photogrammetry with Dense Stereo for Fisheye Cameras}},
  journal       = {Atmospheric Chemistry and Physics},
  volume        = 16,
  pages         = {14231-14248},
  number        = {22},
  abstract      = {We present a novel approach for dense 3-D cloud reconstruction above an area of 1010km2 using two hemispheric sky imagers with fisheye lenses in a stereo setup. We examine an epipolar rectification model designed for fisheye cameras, which allows the use of efficient out-of-the-box dense matching algorithms designed for classical pinhole-type cameras to search for correspondence information at every pixel. The resulting dense point cloud allows to recover a detailed and more complete cloud morphology compared to previous approaches that employed sparse feature-based stereo or assumed geometric constraints on the cloud field. Our approach is very efficient and can be fully automated. From the obtained 3-D shapes, cloud dynamics, size, motion, type and spacing can be derived, and used for radiation closure under cloudy conditions, for example. Fisheye lenses follow a different projection function than classical pinhole-type cameras and provide a large field of view with a single image. However, the computation of dense 3-D information is more complicated and standard implementations for dense 3-D stereo reconstruction cannot be easily applied. Together with an appropriate camera calibration, which includes internal camera geometry, global position and orientation of the stereo camera pair, we use the correspondence information from the stereo matching for dense 3-D stereo reconstruction of clouds located around the cameras. We implement and evaluate the proposed approach using real world data and present two case studies. In the first case, we validate the quality and accuracy of the method by comparing the stereo reconstruction of a stratocumulus layer with reflectivity observations measured by a cloud radar and the cloud-base height estimated from a Lidar-ceilometer. The second case analyzes a rapid cumulus evolution in the presence of strong wind shear.},
  doi           = {10.5194/acp-16-14231-2016},
  url           = {http://www.ipb.uni-bonn.de/pdfs/beekmans16acp.pdf},
  year          = 2016,
}

@InProceedings{schneider2016dvw,
  author        = {J. Schneider and C. Stachniss and W. F\"orstner},
  title         = {{Dichtes Stereo mit Fisheye-Kameras}},
  booktitle     = {UAV 2016 -- Vermessung mit unbemannten Flugsystemen},
  year          = 2016,
  volume        = {82},
  series        = {Schriftenreihe des DVW},
  pages         = {247-264},
  publisher     = {Wi{\ss}ner Verlag},
  note          = {Invited.},
}

@InProceedings{merfels2016iros,
  title         = {{Pose Fusion with Chain Pose Graphs for Automated Driving}},
  author        = {Ch. Mefels and C. Stachniss},
  booktitle     = iros,
  year          = 2016,
  url           = {http://www.ipb.uni-bonn.de/pdfs/merfels16iros.pdf},
}

@Article{lottes2016jfr,
  author        = {P. Lottes and M. Hoeferlin and S. Sanders and C. Stachniss},
  title         = {{Effective Vision-Based Classification for Separating Sugar Beets and Weeds for Precision Farming}},
  journal       = jfr,
  year          = 2016,
}

@Article{abdo2016ijrr,
  author        = {N. Abdo and C. Stachniss and L. Spinello and W. Burgard},
  title         = {{Organizing Objects by Predicting User Preferences Through Collaborative Filtering}},
  journal       = ijrr,
  year          = 2016,
  url           = {http://arxiv.org/pdf/1512.06362},
}

@InBook{siedentop2016lnib,
  author        = {C. Siedentop and V. Laukhart and B. Krastev and D. Kasper and A. Wenden and G. Breuel and C. Stachniss},
  chapter       = {Autonomous Parking Using Previous Paths},
  editor        = {T. Schulze and B. M{\"u}ller and G. Meyer},
  title         = {Advanced Microsystems for Automotive Applications 2015: Smart Systems for Green and Automated Driving. Lecture Notes in Mobility.},
  year          = 2016,
  publisher     = springer,
  pages         = {3-14},
  doi           = {10.1007/978-3-319-20855-8_1},
}

@InProceedings{siedentop2015fas,
  author        = {C. Siedentop and R. Heinze and D. Kasper and G. Breuel and C. Stachniss},
  title         = {{Path-Planning for Autonomous Parking with Dubins Curves}},
  booktitle     = {Proc.~of the Workshop Fahrerassistenzsysteme},
  year          = {2015},
}

@Article{abdo2015arxiv,
  author        = {N. Abdo and C. Stachniss and L. Spinello and W. Burgard},
  title         = {{Collaborative Filtering for Predicting User Preferences for Organizing Objects}},
  journal       = arxiv,
  volume        = {abs/1512.06362},
  year          = 2015,
  url           = {http://arxiv.org/pdf/1512.06362},
}

@InProceedings{liebisch2016wslw,
  author        = {F. Liebisch and J. Pfeifer and R. Khanna and P. Lottes and C. Stachniss and T. Falck and S. Sander and R. Siegwart and A. Walter and E. Galceran},
  title         = {{Flourish -- A robotic approach for automation in crop management}},
  booktitle     = {In Proc.~of the Workshop f\"ur Computer-Bildanalyse und unbemannte autonom fliegende Systeme in der Landwirtschaft},
  year          = 2016,
}

@InProceedings{schubert2016isprsannals,
  author        = {T. Schubert and S. Wenzel and R. Roscher and C. Stachniss},
  title         = {{Investigation of Latent Traces Using Infrared Reflectance Hyperspectral Imaging}},
  booktitle     = isprsannals,
  year          = 2016,
}

@Article{perea2016jras,
  author        = {D. Perea Str{\"o}m and I. Bogoslavskyi and C. Stachniss},
  title         = {{Robust Exploration and Homing for Autonomous Robots}},
  journal       = jras,
  year          = 2016,
}

@InBook{stachniss2016handbookphoto,
  author        = {C. Stachniss},
  alteditor     = {C. Heipke},
  title         = {Springer Handbuch der Geod{\"a}sie},
  chapter       = {Simultaneous Localization and Mapping},
  publisher     = springer,
  note          = {In German, invited.},
  doi           = {10.1007/978-3-662-46900-2_49-2},
  year          = 2016,
  abstract      = {Dieses Kapitel gibt eine Einfhrung in die Kartenerstellung und gleichzeitige Lokalisierung mobiler Sensorplattformen. Die gemeinsame Lsung dieser beiden Probleme ist eine Voraussetzung fr die Realisierung vieler technischer Systeme von leichten Fluggerten ber autonome Roboter bis hin zu mobilen Kameras. Als Simultaneous Localization and Mapping bezeichnet man die Aufgabe, die Trajektorie samt Orientierungsinformation einer sich bewegenden Plattform aus Beobachtungen zu schtzen und gleichzeitig eine Karte der Umgebung zu erstellen. Diese Aufgabe ist in vielen realen Systemen von entscheidender Bedeutung: einerseits stellen hochgenaue Karten mitunter einen Wert an sich fr den Benutzer oder eine spezielle Anwendung dar, andererseits bentigen beispielsweise autonome Roboter ein solches Modell, um zielgerichtet selbststndig navigieren zu knnen. Das Simultaneous Localization and Mapping Problem, beziehungsweise Teilprobleme davon, werden, je nach verwendeter Sensorik, auch als Bndelausgleichung, Structure from Motion oder SLAM bezeichnet. In diesem Kapitel werden wir verschiedene Anstze vorstellen, mit denen man das SLAM Problem adressieren kann. Dies beinhaltet neben dem klassischen Verfahren mittels Ausgleichung, welches offline auf allen Daten operiert, auch Filtertechniken wie den Kalman-Filter und den Partikel-Filter, die zu den Onlineverfahren zhlen. Bei der Verwendung der Kleinsten-Quadrate Methode sowie beim Kalman-Filter wird meist eine Normalverteilung beziehungsweise eine unimodale Verteilung ber die Positionen der 3D-Punkte in der Umgebung und die Orientierung des Sensors geschtzt. Im Gegensatz dazu arbeitet der Partikel-Filter nichtparametrisch und kann multiple Hypothesen ber mgliche Datenassoziationen parallel schtzen. Neben den einzusetzenden Schtzverfahren wird auch skizziert, wie SLAM Systeme mit unterschiedlichen Sensoren realisiert werden knnen.},
}

@InProceedings{naseer2015iros,
  author        = {T. Naseer and M. Ruhnke and L. Spinello and C. Stachniss and W. Burgard},
  title         = {{Robust Visual SLAM Across Seasons}},
  booktitle     = iros,
  year          = 2015,
}

@InProceedings{perea2015icra,
  author        = {D. Perea Str{\"o}m and F. Nenci and C. Stachniss},
  title         = {{Predictive Exploration Considering Previously Mapped Environments}},
  booktitle     = icra,
  year          = 2015,
}

@InProceedings{abdo2015icra,
  author        = {N. Abdo and C. Stachniss and L. Spinello and W.Burgard},
  title         = {Robot, Organize my Shelves! Tidying up Objects by Predicting User Preferences},
  booktitle     = icra,
  year          = 2015,
}

@InProceedings{bogoslavskyi2015icra,
  author        = {I. Bogoslavskyi and L. Spinello and W. Burgard and C. Stachniss},
  title         = {{Where to Park? Minimizing the Expected Time to Find a Parking Space}},
  booktitle     = icra,
  year          = 2015,
}

@InProceedings{vysotska2015icra,
  author        = {O. Vysotska and T. Naseer and L. Spinello and W. Burgard and C. Stachniss},
  title         = {{Efficient and Effective Matching of Image Sequences Under Substantial Appearance Changes Exploiting GPS Prior}},
  booktitle     = icra,
  year          = 2015,
  url           = {http://ais.informatik.uni-freiburg.de/publications/papers/vysotska15icra.pdf},
}

@InProceedings{vysotska2015icraws,
  author        = {O. Vysotska and C. Stachniss},
  title         = {{Lazy Sequences Matching Under Substantial Appearance Changes}},
  booktitle     = {Workshop on Visual Place Recognition in Changing Environments at the IEEE Int.~Conf.~on Robotics \& Automation},
  year          = 2015,
}

@InProceedings{ziparo2014icomosga,
  author        = {V.A. Ziparo and G. Castelli and L. Van Gool and G. Grisetti and B. Leibe and M. Proesmans and C. Stachniss},
  title         = {{The ROVINA Project. Robots for Exploration, Digital Preservation and Visualization of Archeological sites}},
  booktitle     = {Proc.~of the 18th ICOMOS General Assembly and Scientific Symposium ``Heritage and Landscape as Human Values''},
  year          = 2014,
}

@InProceedings{nenci2014iros,
  title         = {{Effective Compression of Range Data Streams for Remote Robot Operations using H.264}},
  author        = {F. Nenci and L. Spinello and C. Stachniss},
  booktitle     = iros,
  address       = {Chicago, USA},
  year          = 2014,
  url           = {http://www2.informatik.uni-freiburg.de/~stachnis/pdf/nenci14iros.pdf},
}

@InProceedings{vysotska2014iros,
  title         = {{Automatic Channel Selection and Neural Signal Estimation across Channels of Neural Probes}},
  author        = {O. Vysotska and B. Frank and I. Ulbert and O. Paul and P. Ruther and C. Stachniss and W. Burgard},
  booktitle     = iros,
  address       = {Chicago, USA},
  year          = 2014,
  doi           = {10.1109/IROS.2014.6942748},
  url           = {http://ais.informatik.uni-freiburg.de/publications/papers/vysotska14iros.pdf},
  keywords      = {Neurorobotics, Brain Machine Interfaces},
}

@Article{agarwal2014ram,
  author        = {Pratik Agarwal and Wolfram Burgard and Cyrill Stachniss},
  title         = {{A Survey of Geodetic Approaches to Mapping and the Relationship to Graph-Based SLAM}},
  journal       = ram,
  year          = 2014,
  doi           = {10.1109/MRA.2014.2322282},
  url           = {http://ais.informatik.uni-freiburg.de/publications/papers/agarwal-geodetic.pdf},
}

@Article{frank2014ras,
  author        = {B. Frank and C. Stachniss and R. Schmedding and M. Teschner and W. Burgard},
  title         = {{Learning object deformation models for robot motion planning}},
  journal       = jras,
  year          = 2014,
  volume        = {62},
  number        = {8},
  pages         = {1153-1174},
  doi           = {10.1016/j.robot.2014.04.005},
  url           = {http://ais.informatik.uni-freiburg.de/publications/papers/frank14ras.pdf},
}

@InProceedings{abdo2014icra,
  title         = {{Inferring What to Imitate in Manipulation Actions by Using a Recommender System}},
  author        = {N. Abdo and L. Spinello and W. Burgard and C. Stachniss},
  booktitle     = icra,
  address       = {Hong Kong, China},
  year          = 2014,
}

@InProceedings{agarwal2014icra-eaod,
  author        = {P. Agarwal and G. Grisetti and G.D. Tipaldi and L. Spinello and W. Burgard and C. Stachniss},
  title         = {{Experimental Analysis of Dynamic Covariance Scaling for Robust Map Optimization Under Bad Initial Estimates}},
  booktitle     = icra,
  address       = {Hong Kong, China},
  year          = 2014,
  doi           = {10.1109/ICRA.2014.6907383},
  url           = {http://ais.informatik.uni-freiburg.de/publications/papers/agarwal2014bicra.pdf},
}

@InProceedings{agarwal2014icra-habg,
  author        = {P. Agarwal and W. Burgard and C. Stachniss},
  title         = {Helmert's and Bowie's Geodetic Mapping Methods and Their Relation to Graph-Based SLAM},
  booktitle     = icra,
  address       = {Hong Kong, China},
  year          = 2014,
  doi           = {10.1109/ICRA.2014.6907382},
  url           = {http://ais.informatik.uni-freiburg.de/publications/papers/agarwal2014aicra.pdf},
}

@InProceedings{mazuran2014icra,
  author        = {M. Mazuran and G.D. Tipaldi and L. Spinello and W. Burgard and C. Stachniss},
  title         = {{A Statistical Measure for Map Consistency in SLAM}},
  booktitle     = icra,
  address       = {Hong Kong, China},
  year          = 2014,
  doi           = {10.1109/ICRA.2014.6907387},
  url           = {http://ais.informatik.uni-freiburg.de/publications/papers/mazuran14icra1.pdf},
}

@InProceedings{osswald2014icra,
  author        = {S. O{\ss}wald and H. Kretzschmar and W. Burgard and C. Stachniss},
  title         = {{Learning to Give Route Directions from Human Demonstrations}},
  booktitle     = icra,
  address       = {Hong Kong, China},
  year          = 2014,
  doi           = {10.1109/ICRA.2014.6907334},
  url           = {http://ais.informatik.uni-freiburg.de/publications/papers/osswald14icra.pdf},
  abstract      = {For several applications, robots and other computer systems must provide route descriptions to humans. These descriptions should be natural and intuitive for the human users. In this paper, we present an algorithm that learns how to provide good route descriptions from a corpus of human-written directions. Using inverse reinforcement learning, our algorithm learns how to select the information for the description depending on the context of the route segment. The algorithm then uses the learned policy to generate directions that imitate the style of the descriptions provided by humans, thus taking into account personal as well as cultural preferences and special requirements of the particular user group providing the learning demonstrations. We evaluate our approach in a user study and show that the directions generated by our policy sound similar to human-given directions and substantially more natural than directions provided by commercial web services.},
}

@InProceedings{ito2014icra,
  author        = {S. Ito and F. Endres and M. Kuderer and G.D. Tipaldi and C. Stachniss and W. Burgard},
  title         = {{W-RGB-D: Floor-Plan-Based Indoor Global Localization Using a Depth Camera and WiFi}},
  booktitle     = icra,
  address       = {Hong Kong, China},
  year          = 2014,
  doi           = {10.1109/ICRA.2014.6906890},
  url           = {http://ais.informatik.uni-freiburg.de/publications/papers/ito14icra.pdf},
}

@Article{stachniss2012fnt,
  author        = {C. Stachniss and W. Burgard},
  title         = {{Particle Filters for Robot Navigation}},
  journal       = fntr,
  year          = 2012,
  volume        = 3,
  number        = 4,
  pages         = {211-282},
  note          = {Published 2014},
  doi           = {10.1561/2300000013},
}

@InProceedings{bogoslavskyi2013ecmr,
  author        = {I. Bogoslavskyi and O. Vysotska and J. Serafin and G. Grisetti and C. Stachniss},
  title         = {{Efficient Traversability Analysis for Mobile Robots using the Kinect Sensor}},
  booktitle     = ecmr,
  address       = {Barcelona, Spain},
  year          = 2013,
  url           = {http://ais.informatik.uni-freiburg.de/publications/papers/bogoslavskyi13ecmr.pdf},
}

@Article{burgard2013forschung,
  author        = {W. Burgard and C. Stachniss},
  title         = {{G}estatten, {O}belix!},
  journal       = {Forschung -- Das {M}agazin der {D}eutschen {F}orschungsgemeinschaft},
  volume        = 1,
  year          = 2013,
  note          = {In German, invited},
  url           = {http://ais.informatik.uni-freiburg.de/publications/papers/forschung_2013_01-pg4-9.pdf},
}

@Article{hornung2013ar,
  author        = {A. Hornung and K.M. Wurm and M. Bennewitz and C. Stachniss and W. Burgard},
  title         = {{OctoMap}: An Efficient Probabilistic 3D Mapping Framework Based on Octrees},
  journal       = ar,
  year          = 2013,
  volume        = 34,
  issue         = 3,
  pages         = {189-206},
  url           = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/hornung13auro.pdf},
}

@Article{maier2013ijhr,
  author        = {D. Maier and C. Stachniss and M. Bennewitz},
  title         = {{Vision-Based Humanoid Navigation Using Self-Supervised Obstacle Detection}},
  journal       = ijhr,
  year          = 2013,
}

@Article{wurm2013ar,
  author        = {K.M. Wurm and C. Dornhege and B. Nebel and W. Burgard and C. Stachniss},
  title         = {{Coordinating Heterogeneous Teams of Robots using Temporal Symbolic Planning}},
  journal       = ar,
  year          = 2013,
  url           = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/wurm13auro.pdf},
  volume        = {34},
  issue         = {4},
}

@Article{wurm2013ras,
  author        = {K.M. Wurm and H. Kretzschmar and R. K{\"u}mmerle and C. Stachniss and W. Burgard},
  title         = {{Identifying Vegetation from Laser Data in Structured Outdoor Environments}},
  journal       = jras,
  year          = 2013,
  doi           = {10.1016/j.robot.2012.10.003},
  url           = {http://ais.informatik.uni-freiburg.de/publications/papers/wurm13ras.pdf},
}

@InProceedings{abdo2013icra,
  author        = {N. Abdo and H. Kretzschmar and L. Spinello and C. Stachniss},
  title         = {{Learning Manipulation Actions from a Few Demonstrations}},
  booktitle     = icra,
  address       = {Karlsruhe, Germany},
  year          = 2013,
  doi           = {10.1109/ICRA.2013.6630734},
  url           = {http://ais.informatik.uni-freiburg.de/publications/papers/abdo13icra.pdf},
}

@InProceedings{kuemmerle2013icra,
  author        = {R. K\"ummerle and M. Ruhnke and B. Steder and C. Stachniss and W. Burgard},
  title         = {{A Navigation System for Robots Operating in Crowded Urban Environments}},
  booktitle     = icra,
  address       = {Karlsruhe, Germany},
  year          = 2013,
  url           = {http://ais.informatik.uni-freiburg.de/publications/papers/kuemmerle13icra.pdf},
  abstract      = {Over the past years, there has been a tremendous progress in the area of robot navigation. Most of the systems developed thus far, however, are restricted to indoor scenarios, non-urban outdoor environments, or road usage with cars. Urban areas introduce numerous challenges to autonomous mobile robots as they are highly complex and in addition to that dynamic. In this paper, we present a navigation system for pedestrian-like autonomous navigation with mobile robots in city environments. We describe different components including a SLAM system for dealing with huge maps of city centers, a planning approach for inferring feasible paths taking also into account the traversability and type of terrain, and a method for accurate localization in dynamic environments. The navigation system has been implemented and tested in several large-scale field tests in which the robot Obelix managed to autonomously navigate from our university campus over a 3.3 km long route to the city center of Freiburg.},
}

@InProceedings{agarwal2013icra,
  author        = {P. Agarwal and G.D. Tipaldi and L. Spinello and C. Stachniss and W. Burgard},
  title         = {{Robust Map Optimization using Dynamic Covariance Scaling}},
  booktitle     = icra,
  address       = {Karlsruhe, Germany},
  year          = 2013,
  doi           = {10.1109/ICRA.2013.6630557},
  url           = {http://ais.informatik.uni-freiburg.de/publications/papers/agarwalicra13_DCS.pdf},
}

@InProceedings{agarwal2013icraws,
  title         = {{Dynamic Covariance Scaling for Robust Robotic Mapping}},
  author        = {P. Agarwal and G.D. Tipaldi and L. Spinello and C. Stachniss and W. Burgard},
  booktitle     = {{ICRA Workshop on robust and Multimodal Inference in Factor Graphs}},
  address       = {Karlsruhe, Germany},
  year          = 2013,
  url           = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/agarwal13icraws.pdf},
}

@InBook{stachniss2016handbook-slamchapter,
  author        = {C. Stachniss and J. Leonard and S. Thrun},
  editor        = {B. Siciliano and O. Khatib},
  title         = {{Springer Handbook of Robotics, 2nd edition}},
  chapter       = {Chapt.~46: Simultaneous Localization and Mapping},
  publisher     = springer,
  year          = 2016,
  abstract      = {This chapter provides a comprehensive introduction into one of the key enabling technologies of mobile robot navigation: simultaneous localization and mapping, or in short SLAM. SLAM addresses the problem of acquiring a spatial map of an environment while simultaneously localizing the robot relative to this model. The SLAM problem is generally regarded as one of the most important problems in the pursuit of building truly autonomous mobile robots. It is of great practical importance; if a robust, general-purpose solution to SLAM can be found, then many new applications of mobile robotics will become possible.},
}

@InProceedings{grisetti2012rich,
  author        = {G. Grisetti and L. Iocchi and B. Leibe and V.A. Ziparo and C. Stachniss},
  title         = {{Digitization of Inaccessible Archeological Sites with Autonomous Mobile Robots}},
  booktitle     = {Conf.~on Robotics Innovation for Cultural Heritage},
  year          = 2012,
  notes         = {Extended abstract},
}

@InProceedings{abdo2012tampra,
  author        = {N. Abdo and H. Kretzschmar and C. Stachniss},
  title         = {{From Low-Level Trajectory Demonstrations to Symbolic Actions for Planning}},
  booktitle     = {Proc.~of the ICAPS Workshop on Combining Task and Motion Planning for Real-World Applications (TAMPRA)},
  year          = 2012,
  url           = {http://ais.informatik.uni-freiburg.de/publications/papers/abdo12tampra.pdf},
}

@InProceedings{joho2012rss,
  author        = {D. Joho and G.D. Tipaldi and N. Engelhard and C. Stachniss and W. Burgard},
  title         = {Nonparametric {B}ayesian Models for Unsupervised Scene Analysis and Reconstruction},
  booktitle     = rss,
  year          = 2012,
  url           = {http://ais.informatik.uni-freiburg.de/publications/papers/joho12rss.pdf},
}

@Article{kretzschmar2012ijrr,
  author        = {H. Kretzschmar and C. Stachniss},
  title         = {Information-Theoretic Pose Graph Compression for Laser-based {SLAM}},
  journal       = ijrr,
  year          = 2012,
  volume        = 31,
  issue         = 11,
  pages         = {1219--1230},
}

@InProceedings{roewekaemper2012iros,
  author        = {J. Roewekaemper and C. Sprunk and G.D. Tipaldi and C. Stachniss and P. Pfaff and W. Burgard},
  title         = {{On the Position Accuracy of Mobile Robot Localization based on Particle Filters combined with Scan Matching}},
  booktitle     = iros,
  year          = 2012,
  url           = {http://ais.informatik.uni-freiburg.de/publications/papers/roewekaemper12iros.pdf},
}

@InProceedings{spinello2012rssws,
  author        = {L. Spinello and C. Stachniss and W. Burgard},
  title         = {{Scene in the Loop: Towards Adaptation-by-Tracking in RGB-D Data}},
  booktitle     = {Proc.~of the RSS Workshop RGB-D: Advanced Reasoning with Depth Cameras},
  year          = 2012,
  url           = {http://ais.informatik.uni-freiburg.de/publications/papers/spinello12rssws.pdf},
}

@InProceedings{bennewitz2011humanoids,
  author        = {M. Bennewitz and D. Maier and A. Hornung and C. Stachniss},
  title         = {{Integrated Perception and Navigation in Complex Indoor Environments}},
  booktitle     = {Proc.~of the IEEE-RAS Int.~Conf.~on Humanoid Robots (HUMANOIDS)},
  year          = 2011,
  note          = {Invited presentation at the workshop on Humanoid service robot navigation in crowded and dynamic environments},
}

@Article{sturm2011jair,
  author        = {J. Sturm and C. Stachniss and W. Burgard},
  title         = {{A Probabilistic Framework for Learning Kinematic Models of Articulated Objects}},
  journal       = jair,
  pages         = {477--526},
  volume        = 41,
  year          = 2011,
  url           = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/sturm11jair.pdf},
}

@InProceedings{burgard2010irosws,
  author        = {W. Burgard and K.M. Wurm and M. Bennewitz and C. Stachniss and A. Hornung and R.B. Rusu and K. Konolige},
  title         = {{Modeling the World Around Us: An Efficient 3D Representation for Personal Robotics}},
  booktitle     = {Workshop on Defining and Solving Realistic Perception Problems in Personal Robotics at the IEEE/RSJ Int.~Conf.~on Intelligent Robots and Systems},
  address       = {Taipei, Taiwan},
  year          = 2010,
}

@InProceedings{becker2011irosws,
  author        = {J. Becker and C. Bersch and D. Pangercic and B. Pitzer and T. R\"uhr and B. Sankaran and J. Sturm and C. Stachniss and M. Beetz and W. Burgard},
  title         = {{Mobile Manipulation of Kitchen Containers}},
  booktitle     = {Proc.~of the IROS'11 Workshop on Results, Challenges and Lessons Learned in Advancing Robots with a Common Platform},
  address       = {San Francisco, CA, USA},
  year          = 2011,
  url           = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/becker11irosws.pdf},
}

@InProceedings{kuemmerle2011arso,
  author        = {R. K\"ummerle and G. Grisetti and C. Stachniss and W. Burgard},
  title         = {{Simultaneous Parameter Calibration, Localization, and Mapping for Robust Service Robotics}},
  booktitle     = {Proc.~of the IEEE Workshop on Advanced Robotics and its Social Impacts},
  address       = {Half-Moon Bay, CA, USA},
  year          = 2011,
  url           = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/kuemmerle11arso.pdf},
  abstract      = {Modern service robots are designed to be deployed by end-users and not to be monitored by experts during operation. Most service robotics applications require reliable navigation capabilities of the robot. The calibration parameters of a mobile robot play a substantial role in navigation tasks. Often these parameters are subject to variations that depend either on environmental changes or on the wear of the devices. In this paper, we propose an approach to simultaneously estimate a map of the environment, the position of the on-board sensors of the robot, and its kinematic parameters. Our method requires no prior knowledge about the environment and relies only on a rough initial guess of the platform parameters. The proposed approach performs on-line estimation of the parameters and it is able to adapt to non-stationary changes of the configuration. Our approach has been implemented and is used on the EUROPA robot, a service robot operating in urban environments. In addition to that, we tested our approach in simulated environments and on a wide range of real world data using different types of robotic platforms.},
}

@Article{kuemmerle2014jfr,
  author        = {K{\"u}mmerle, R. and Ruhnke, M. and Steder, B. and Stachniss, C. and Burgard, W.},
  title         = {{Autonomous Robot Navigation in Highly Populated Pedestrian Zones}},
  journal       = jfr,
  year          = 2014,
  doi           = {10.1002/rob.21534},
  url           = {http://ais.informatik.uni-freiburg.de/publications/papers/kuemmerle14jfr.pdf},
}

@InProceedings{stachniss2011isrr,
  author        = {C. Stachniss and H. Kretzschmar},
  title         = {Pose Graph Compression for Laser-based {SLAM}},
  booktitle     = isrr,
  address       = {Flagstaff, AZ, USA},
  year          = 2011,
  note          = {Invited publication},
}

@InProceedings{kretzschmar2011iros,
  author        = {H. Kretzschmar and C. Stachniss and G. Grisetti},
  title         = {Efficient Information-Theoretic Graph Pruning for Graph-based {SLAM} with Laser Range Finders},
  booktitle     = iros,
  address       = {San Francisco, CA, USA},
  year          = 2011,
}

@InProceedings{wurm2011iros,
  title         = {{Hierarchies of Octrees for Efficient 3D Mapping}},
  author        = {K.M. Wurm and D. Hennes and D. Holz and R.B. Rusu and C. Stachniss and K. Konolige and W. Burgard},
  booktitle     = iros,
  address       = {San Francisco, CA, USA},
  year          = 2011,
}

@InProceedings{ziegler2011iros,
  author        = {J. Ziegler and H. Kretzschmar and C. Stachniss and G. Grisetti and W. Burgard},
  title         = {{Accurate Human Motion Capture in Large Areas by Combining IMU- and Laser-based People Tracking}},
  booktitle     = iros,
  address       = {San Francisco, CA, USA},
  year          = 2011,
}

@InProceedings{frank2011iros,
  author        = {B. Frank and C. Stachniss and N. Abdo and W. Burgard},
  title         = {{Efficient Motion Planning for Manipulation Robots in Environments with Deformable Objects}},
  booktitle     = iros,
  address       = {San Francisco, CA, USA},
  year          = 2011,
}

@InProceedings{frank2011pamr,
  author        = {B. Frank and C. Stachniss and N. Abdo and W. Burgard},
  title         = {{Using Gaussian Process Regression for Efficient Motion Planning in Environments with Deformable Objects}},
  booktitle     = {Proc. of the AAAI-11 Workshop on Automated Action Planning for Autonomous Mobile Robots (PAMR)},
  address       = {San Francisco, CA, USA},
  year          = 2011,
}

@InProceedings{maier2011icra,
  author        = {D. Maier and M. Bennewitz and C. Stachniss},
  title         = {{Self-supervised Obstacle Detection for Humanoid Navigation Using Monocular Vision and Sparse Laser Data}},
  booktitle     = icra,
  address       = {Shanghai, China},
  year          = 2011,
}

@InBook{asadi2011isfmo,
  author        = {S. Asadi and M. Reggente and C. Stachniss and C. Plagemann and A.J. Lilienthal},
  title         = {{Intelligent Systems for Machine Olfaction: Tools and Methodologies}},
  editor        = {E.L. Hines and M.S. Leeson},
  chapter       = {Statistical Gas Distribution Modelling using Kernel Methods},
  publisher     = {{IGI} {G}lobal},
  pages         = {153-179},
  year          = 2011,
}

@InProceedings{hornung2010erlars,
  author        = {A. Hornung and M.Bennewitz and C. Stachniss and H. Strasdat and S. O\ss{}wald and W. Burgard},
  title         = {{Learning Adaptive Navigation Strategies for Resource-Constrained Systems}},
  booktitle     = {Proc.~of the Int.~Workshop on Evolutionary and Reinforcement Learning for Autonomous Robot Systems},
  address       = {Lisbon, Portugal},
  year          = 2010,
}

@Article{grisetti2010titsmag,
  author        = {G. Grisetti and R. K{\"u}mmerle and C. Stachniss and W. Burgard},
  title         = {A Tutorial on Graph-based {SLAM}},
  journal       = titsmag,
  year          = 2010,
  pages         = {31--43},
  volume        = 2,
  issue         = 4,
}

@InProceedings{sturm2010rssws,
  author        = {J. Sturm and K. Konolige and C. Stachniss and W. Burgard},
  title         = {{3D Pose Estimation, Tracking and Model Learning of Articulated Objects from Dense Depth Video using Projected Texture Stereo}},
  booktitle     = {Proc.~of the Workshop RGB-D: Advanced Reasoning with Depth Cameras at Robotics: Science and Systems (RSS)},
  address       = {Zaragoza, Spain},
  year          = 2010,
}

@InProceedings{frank2010rssws,
  author        = {B. Frank and R. Schmedding and C. Stachniss and M. Teschner and W. Burgard},
  title         = {{Learning Deformable Object Models for Mobile Robot Path Planning using Depth Cameras and a Manipulation Robot}},
  booktitle     = {Proc.~of the Workshop RGB-D: Advanced Reasoning with Depth Cameras at Robotics: Science and Systems (RSS)},
  address       = {Zaragoza, Spain},
  year          = 2010,
}

@InProceedings{wurm2010iros,
  title         = {{Coordinated Exploration with Marsupial Teams of Robots using Temporal Symbolic Planning}},
  author        = {K.M. Wurm and C. Dornhege and P. Eyerich and C. Stachniss and B. Nebel and W. Burgard},
  booktitle     = iros,
  year          = 2010,
  address       = {Taipei, Taiwan},
}

@InProceedings{sturm2010iros,
  title         = {{Robustly Operating Articulated Objects based on Experience }},
  author        = {J. Sturm and A. Jain and C. Stachniss and C.C. Kemp and W. Burgard},
  booktitle     = iros,
  year          = 2010,
  address       = {Taipei, Taiwan},
}

@InProceedings{frank2010iros,
  title         = {{Learning the Elasticity Parameters of Deformable Objects with a Manipulation Robot}},
  author        = {B. Frank and R. Schmedding and C. Stachniss and M. Teschner and W. Burgard},
  booktitle     = iros,
  year          = 2010,
  address       = {Taipei, Taiwan},
}

@InProceedings{grisetti2010icra,
  author        = {G. Grisetti and R. K{\"u}mmerle and C. Stachniss and U. Frese and C. Hertzberg},
  title         = {{Hierarchical Optimization on Manifolds for Online 2D and 3D Mapping}},
  booktitle     = icra,
  address       = {Anchorage, Alaska},
  year          = 2010,
  url           = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/grisetti10icra.pdf},
}

@InProceedings{karg2010icra,
  author        = {M. Karg and K.M. Wurm and C. Stachniss and K. Dietmayer and W. Burgard},
  title         = {Consistent Mapping of Multistory Buildings by Introducing Global Constraints to Graph-based {SLAM}},
  booktitle     = icra,
  address       = {Anchorage, Alaska},
  year          = 2010,
}

@InProceedings{sturm2010icra,
  author        = {J. Sturm and K. Konolige and C. Stachniss and W. Burgard},
  title         = {{Vision-based Detection for Learning Articulation Models of Cabinet Doors and Drawers in Household Environments}},
  booktitle     = icra,
  address       = {Anchorage, Alaska},
  year          = 2010,
}

@InProceedings{wurm2010icraws,
  author        = {K.M. Wurm and A. Hornung and M. Bennewitz and C. Stachniss and W. Burgard},
  title         = {{OctoMap}: A Probabilistic, Flexible, and Compact {3D} Map Representation for Robotic Systems},
  booktitle     = {Proc. of the ICRA 2010 Workshop on Best Practice in 3D Perception and Modeling for Mobile Manipulation},
  year          = 2010,
  address       = {Anchorage, AK, USA},
  url           = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/wurm10icraws.pdf},
}

@PhDThesis{stachniss2009habil,
  author        = {C. Stachniss},
  title         = {{Spatial Modeling and Robot Navigation}},
  type          = {Habilitation},
  year          = 2009,
  school        = {University of Freiburg, Department of Computer Science},
  url           = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/stachniss-habil.pdf},
}

@InBook{mueller2010cogsysbook,
  author        = {J. M\"{u}ller and C. Stachniss and K.O. Arras and W. Burgard},
  chapter       = {Socially Inspired Motion Planning for Mobile Robots in Populated Environments},
  title         = {{Cognitive Systems}},
  series        = {Cognitive Systems Monographs},
  publisher     = springer,
  year          = 2010,
}

@Book{stachniss2009springerbook,
  author        = {C. Stachniss},
  title         = {{Robotic Mapping and Exploration}},
  publisher     = springer,
  year          = 2009,
  volume        = 55,
  series        = springerstaradvanced,
  isbn          = {978-3-642-01096-5},
}

@Article{plagemann2010ras,
  title         = {{A Nonparametric Learning Approach to Range Sensing from Omnidirectional Vision}},
  author        = {C. Plagemann and C. Stachniss and J. Hess and F. Endres and N. Franklin},
  journal       = jras,
  volume        = 58,
  issue         = 6,
  pages         = {762--772},
  year          = 2010,
}

@Article{kretzschmar2010ki,
  title         = {Lifelong Map Learning for Graph-based {SLAM} in Static Environments},
  author        = {H. Kretzschmar and G. Grisetti and C. Stachniss},
  journal       = {{KI} -- {K}\"unstliche {I}ntelligenz (German {AI} Magazine)},
  volume        = 24,
  issue         = 3,
  pages         = {199--206},
  year          = 2010,
}

@Article{wurm2010ras,
  title         = {{Bridging the Gap Between Feature- and Grid-based SLAM}},
  author        = {K.M. Wurm and C. Stachniss and G. Grisetti},
  journal       = jras,
  volume        = {58},
  number        = {2},
  pages         = {140 - 148},
  year          = {2010},
  issn          = {0921-8890},
  doi           = {10.1016/j.robot.2009.09.009},
  url           = {http://ais.informatik.uni-freiburg.de/publications/papers/wurm10ras.pdf},
}

@InProceedings{sturm2009ijcai,
  author        = {J. Sturm and V. Predeap and C. Stachniss and C. Plagemann and K. Konolige and W. Burgard},
  title         = {{Learning Kinematic Models for Articulated Objects}},
  booktitle     = ijcai,
  year          = 2009,
  address       = {Pasadena, CA, USA},
}

@InProceedings{sturm2009snowbird,
  author        = {J. Sturm and C. Stachniss and V. Predeap and C. Plagemann and K. Konolige and W. Burgard},
  title         = {{Learning Kinematic Models for Articulated Objects}},
  booktitle     = {Online Proc. of the Learning Workshop (Snowbird)},
  year          = 2009,
  address       = {Clearwater, FL, USA},
}

@InProceedings{endres2009rss,
  author        = {F. Endres and C. Plagemann and C. Stachniss and W. Burgard},
  title         = {{Scene Analysis using Latent Dirichlet Allocation}},
  booktitle     = rss,
  year          = 2009,
  address       = {Seattle, WA, USA},
}

@InProceedings{endres2009rssws,
  author        = {F. Endres and J. Hess and N. Franklin and C. Plagemann and C. Stachniss and W. Burgard},
  title         = {{Estimating Range Information from Monocular Vision}},
  booktitle     = {Workshop Regression in Robotics - Approaches and Applications at Robotics: Science and Systems (RSS)},
  year          = 2009,
  address       = {Seattle, WA, USA},
}

@InProceedings{sturm2009rssws,
  author        = {J. Sturm and C. Stachniss and V. Predeap and C. Plagemann and K. Konolige and W. Burgard},
  title         = {{Towards Understanding Articulated Objects}},
  booktitle     = {Workshop Integrating Mobility and Manipulation at Robotics: Science and Systems (RSS)},
  year          = 2009,
  address       = {Seattle, WA, USA},
}

@Article{kuemmerle2009ar,
  title         = {On measuring the accuracy of {SLAM} algorithms},
  author        = {R. K{\"u}mmerle and B. Steder and C. Dornhege and M. Ruhnke and G. Grisetti and C. Stachniss and A. Kleiner},
  journal       = ar,
  year          = 2009,
  volume        = 27,
  issue         = 4,
  pages         = {387ff},
}

@InProceedings{burgard2009iros,
  title         = {Trajectory-based Comparison of {SLAM} Algorithms},
  author        = {W. Burgard and C. Stachniss and G. Grisetti and B. Steder and R. K\"ummerle and C. Dornhege and M. Ruhnke and A. Kleiner and J.D. Tard\'os},
  booktitle     = iros,
  year          = 2009,
  address       = {St. Louis, MO, USA},
}

@InProceedings{wurm2009iros,
  author        = {K.M. Wurm and R. K{\"u}mmerle and Stachniss, C. and Burgard, W.},
  title         = {{Improving Robot Navigation in Structured Outdoor Environments by Identifying Vegetation from Laser Data}},
  booktitle     = iros,
  year          = 2009,
  address       = {St. Louis, MO, USA},
}

@InProceedings{schneider2009iros,
  author        = {A. Schneider and J. Sturm C. Stachniss and M. Reisert and H. Burkhardt and W. Burgard},
  title         = {{Object Identification with Tactile Sensors Using Bag-of-Features}},
  booktitle     = iros,
  year          = 2009,
  address       = {St. Louis, MO, USA},
}

@Article{stachniss2009ar,
  title         = {{Gas Distribution Modeling using Sparse Gaussian Process Mixtures}},
  author        = {C. Stachniss and C. Plagemann and A.J. Lilienthal},
  journal       = ar,
  year          = 2009,
  volume        = 26,
  issue         = 2,
  pages         = {187ff},
}

@InProceedings{bennewitz2009icra,
  author        = {M. Bennewitz and C. Stachniss and S. Behnke and W. Burgard},
  title         = {{Utilizing Reflection Properties of Surfaces to Improve Mobile Robot Localization}},
  booktitle     = icra,
  year          = 2009,
  address       = {Kobe, Japan},
}

@InProceedings{strasdat2009icra,
  author        = {H. Strasdat and C. Stachniss and W. Burgard},
  title         = {{Which Landmark is Useful? Learning Selection Policies for Navigation in Unknown Environments}},
  booktitle     = icra,
  year          = 2009,
  address       = {Kobe, Japan},
}

@InProceedings{frank2009icra,
  author        = {B. Frank and C. Stachniss and R. Schmedding and W. Burgard and M. Teschner},
  title         = {{Real-world Robot Navigation amongst Deformable Obstacles}},
  booktitle     = icra,
  year          = 2009,
  address       = {Kobe, Japan},
}

@InProceedings{eppner2009icra,
  author        = {C. Eppner and J. Sturm and M. Bennewitz and C. Stachniss and W. Burgard},
  title         = {{Imitation Learning with Generalized Task Descriptions}},
  booktitle     = icra,
  year          = 2009,
  address       = {Kobe, Japan},
}

@Article{grisetti2009tits,
  author        = {G. Grisetti and C. Stachniss and W. Burgard},
  title         = {{Non-linear Constraint Network Optimization for Efficient Map Learning}},
  journal       = tits,
  year          = 2009,
  volume        = 10,
  number        = 3,
  pages         = {428--439},
  url           = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/grisetti09its.pdf},
}

@InProceedings{kretzschmar2008iros,
  author        = {H. Kretzschmar and C. Stachniss and C. Plagemann and W. Burgard},
  title         = {{Estimating Landmark Locations from Geo-Referenced Photographs}},
  booktitle     = iros,
  year          = 2008,
  address       = {Nice, France},
}

@Article{sprunk2016auro,
  author        = {C. Sprunk and B. Lau and P. Pfaff and W. Burgard},
  title         = {{An Accurate and Efficient Navigation System for Omnidirectional Robots in Industrial Environments}},
  journal       = ar,
  pages         = {1--21},
  year          = 2016,
  doi           = {10.1007/s10514-016-9557-1},
  url           = {http://ais.informatik.uni-freiburg.de/publications/papers/sprunk16auro.pdf},
  issn          = {1573-7527},
}

@Article{kretzschmar2016ijrr,
  author        = {H. Kretzschmar and M. Spies and C. Sprunk and W. Burgard},
  title         = {{Socially Compliant Mobile Robot Navigation via Inverse Reinforcement Learning}},
  journal       = ijrr,
  year          = 2016,
  doi           = {10.1177/0278364915619772},
  url           = {http://ais.informatik.uni-freiburg.de/publications/papers/kretzschmar16ijrr.pdf},
}

@InProceedings{pfaff2008iros,
  author        = {P. Pfaff and C. Stachniss and C. Plagemann and W. Burgard},
  title         = {{Efficiently Learning High-dimensional Observation Models for Monte-Carlo Localization using Gaussian Mixtures}},
  booktitle     = iros,
  year          = 2008,
  doi           = {10.1109/IROS.2008.4650711},
  url           = {http://www2.informatik.uni-freiburg.de/~stachnis/pdf/pfaff08iros.pdf},
  address       = {Nice, France},
  abstract      = {Whereas probabilistic approaches are a powerful tool for mobile robot localization, they heavily rely on the proper definition of the so-called observation model which defines the likelihood of an observation given the position and orientation of the robot and the map of the environment. Most of the sensor models for range sensors proposed in the past either consider the individual beam measurements independently or apply uni-modal models to represent the likelihood function. In this paper we present an approach that learns place-dependent sensor models for entire range scans using Gaussian mixture models. To deal with the high dimensionality of the measurement space, we utilize principle component analysis for dimensionality reduction. In practical experiments carried out with data obtained from a real robot we demonstrate that our model substantially outperforms existing and popular sensor models.},
}

@InProceedings{wurm2008iros,
  author        = {K.M. Wurm and C. Stachniss and W. Burgard},
  title         = {{Coordinated Multi-Robot Exploration using a Segmentation of the Environment}},
  booktitle     = iros,
  year          = 2008,
  address       = {Nice, France},
  doi           = {10.1109/IROS.2008.4650734},
  url           = {http://ais.informatik.uni-freiburg.de/publications/papers/wurm08iros.pdf},
}

@InProceedings{stachniss2008rss,
  author        = {C. Stachniss and C. Plagemann and A.J. Lilienthal and W. Burgard},
  title         = {{Gas Distribution Modeling using Sparse Gaussian Process Mixture Models}},
  booktitle     = rss,
  year          = 2008,
  address       = {Zurich, Switzerland},
  url           = {http://ais.informatik.uni-freiburg.de/publications/papers/stachniss08rss.pdf},
}

@InProceedings{mueller2008cogsys,
  author        = {M\"uller, J. and Stachniss, C. and Arras, K.O. and Burgard, W.},
  title         = {{Socially Inspired Motion Planning for Mobile Robots in Populated Environments}},
  booktitle     = cogsys,
  year          = 2008,
  url           = {http://ais.informatik.uni-freiburg.de/publications/papers/mueller08cogsys.pdf},
}

@Article{steder2008tro,
  author        = {B. Steder and G. Grisetti and C. Stachniss and W. Burgard},
  title         = {Visual {SLAM} for Flying Vehicles},
  journal       = tro,
  year          = 2008,
  volume        = {24},
  number        = {8},
  pages         = {1088--1093},
  doi           = {10.1109/TRO.2008.2004521},
  url           = {http://ais.informatik.uni-freiburg.de/publications/papers/steder08tro.pdf},
}

@InProceedings{stachniss2008icra,
  author        = {C. Stachniss and M. Bennewitz and G. Grisetti and S. Behnke and W. Burgard},
  title         = {{How to Learn Accurate Grid Maps with a Humanoid}},
  booktitle     = icra,
  address       = {Pasadena, CA, USA},
  year          = 2008,
  doi           = {10.1109/ROBOT.2008.4543697},
  url           = {http://ais.informatik.uni-freiburg.de/publications/papers/stachniss08icra.pdf},
}

@InProceedings{plagemann2008icra,
  author        = {C. Plagemann and F. Endres and J. Hess and C. Stachniss and W. Burgard},
  title         = {{Monocular Range Sensing: A Non-Parametric Learning Approach}},
  booktitle     = icra,
  address       = {Pasadena, CA, USA},
  year          = 2008,
  doi           = {10.1109/ROBOT.2008.4543324},
  url           = {http://www2.informatik.uni-freiburg.de/~stachnis/pdf/plagemann08icra.pdf},
  abstract      = {For many applications, mobile robots need to estimate the geometry of their local surrounding area. To do so, proximity sensor such as laser range finders or sonars are typically employed. Cameras are a cheap and lightweight alternative to such sensors, but do not offer proximity information directly. In this paper, we present a novel approach to learning the relationship between range measurements and visual features extracted from a single monocular camera image. As the learning engine, we apply Gaussian processes, a non- parametric learning technique that not only yields the most likely range prediction corresponding to a certain visual input but also the predictive uncertainty. This information, in turn, can be utilized in an extended grid-based mapping scheme to update a model of the environment more gently where the predictions are unreliable. In practical experiments carried out with a mobile robot equipped with an omnidirectional camera system in different environments, we show that our system is able to predict range scans accurate enough to construct maps of the environment.},
}

@InProceedings{grisetti2008icra,
  author        = {G. Grisetti and D. Lordi Rizzini and C. Stachniss and E. Olson and W. Burgard},
  title         = {{Online Constraint Network Optimization for Efficient Maximum Likelihood Map Learning}},
  booktitle     = icra,
  address       = {Pasadena, CA, USA},
  year          = 2008,
  doi           = {10.1109/ROBOT.2008.4543481},
  url           = {http://ais.informatik.uni-freiburg.de/publications/papers/grisetti08icra.pdf},
}

@InProceedings{frank2008icra,
  author        = {B. Frank and M. Becker and C. Stachniss and M. Teschner and W. Burgard},
  title         = {{Efficient Path Planning for Mobile Robots in Environments with Deformable Objects}},
  booktitle     = icra,
  address       = {Pasadena, CA, USA},
  year          = 2008,
  doi           = {10.1109/ROBOT.2008.4543784},
  url           = {http://ais.informatik.uni-freiburg.de/publications/papers/frank08icra.pdf},
}

@InProceedings{frank2008icraws,
  author        = {B. Frank and M. Becker and C. Stachniss and M. Teschner and W. Burgard},
  title         = {{Learning Cost Functions for Mobile Robot Navigation in Environments with Deformable Objects}},
  booktitle     = {Workshop on Path Planning on Cost Maps at the IEEE Int.~Conf.~on Robotics \& Automation},
  address       = {Pasadena, CA, USA},
  year          = 2008,
  url           = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/frank08icraws.pdf},
}

@InProceedings{steder2008visappws,
  author        = {B. Steder and G. Grisetti and S. Grzonka and C. Stachniss and W. Burgard},
  title         = {{Estimating Consistent Elevation Maps using Down-Looking Cameras and Inertial Sensors}},
  booktitle     = {Workshop on Robotic Perception, International Conference on Computer Vision Theory and Applications},
  year          = 2008,
  address       = {Funchal, Madeira, Portugal},
  url           = {http://ais.informatik.uni-freiburg.de/publications/papers/steder08visapp.pdf},
}

@InBook{burgard2007starbook,
  author        = {W. Burgard and C. Stachniss and D. Haehnel},
  editor        = {Laugier, C. and Chatila, R.},
  title         = {{Autonomous Navigation in Dynamic Environments}},
  chapter       = {Mobile Robot Map Learning from Range Data in Dynamic Environments},
  publisher     = springer,
  year          = 2007,
  volume        = {35},
  series        = springerstaradvanced,
  url           = {http://ais.informatik.uni-freiburg.de/publications/papers/burgard07starbook.pdf},
}

@InProceedings{wurm2007ecmr,
  author        = {K.M. Wurm and C. Stachniss and G. Grisetti and W. Burgard},
  title         = {{Improved Simultaneous Localization and Mapping using a Dual Representation of the Environment}},
  booktitle     = ecmr,
  address       = {Freiburg, Germany},
  year          = 2007,
  url           = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/wurm07ecmr.pdf},
}

@InProceedings{joho2007ams,
  author        = {D. Joho and C. Stachniss and P. Pfaff and W. Burgard},
  title         = {{Autonomous Exploration for 3D Map Learning}},
  booktitle     = ams,
  year          = 2007,
  address       = {Kaiserslautern, Germany},
  url           = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/joho07ams.pdf},
}

@InProceedings{strasdat2007ams,
  author        = {H. Strasdat and C. Stachniss and M. Bennewitz and W. Burgard},
  title         = {{Visual Bearing-Only Simultaneous Localization and Mapping with Improved Feature Matching}},
  booktitle     = ams,
  year          = 2007,
  address       = {Kaiserslautern, Germany},
  url           = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/strasdat07ams.pdf},
}

@Article{stachniss2007it,
  title         = {{Efficiently Learning Metric and Topological Maps with Autonomous Service Robots}},
  author        = {Stachniss, C. and Grisetti, G. and Mart\'{i}nez-Mozos, O. and Burgard, W.},
  journal       = {it -- Information Technology},
  editor        = {Buss, M. and Lawitzki, G.},
  year          = 2007,
  volume        = {49},
  number        = {4},
  pages         = {232--238},
  url           = {http://ais.informatik.uni-freiburg.de/publications/papers/stachniss2007it.pdf},
}

@InProceedings{pfaff2007irosws,
  author        = {P. Pfaff and R. K{\"u}mmerle and D. Joho and C. Stachniss and R. Triebel and W. Burgard},
  title         = {{Navigation in Combined Outdoor and Indoor Environments using M ulti-Level Surface Maps}},
  booktitle     = {Workshop on Safe Navigation in Open and Dynamic Environments a t the IEEE/RSJ Int.~Conf.~on Intelligent Robots and Systems (IROS)},
  year          = 2007,
  address       = {San Diego, CA, USA},
  url           = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/pfaff07irosws.pdf},
}

@InProceedings{stachniss2007iros,
  author        = {C. Stachniss and G. Grisetti and N. Roy and W. Burgard},
  title         = {{Evaluation of Gaussian Proposal Distributions for Mapping with Rao-Blackwellized Particle Filters}},
  booktitle     = iros,
  year          = 2007,
  address       = {San Diego, CA, USA},
  url           = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/stachniss07iros.pdf},
}

@InProceedings{steder2007iros,
  author        = {B. Steder and G. Grisetti and S. Grzonka and C. Stachniss and A. Rottmann and W. Burgard},
  title         = {{Learning Maps in 3D using Attitude and Noisy Vision Sensors}},
  booktitle     = iros,
  year          = 2007,
  address       = {San Diego, CA, USA},
  url           = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/steder07iros.pdf},
}

@InProceedings{steder2007irosws,
  author        = {B. Steder and A. Rottmann and G. Grisetti and C. Stachniss and W. Burgard},
  title         = {{Autonomous Navigation for Small Flying Vehicles}},
  booktitle     = {Workshop on Micro Aerial Vehicles at the IEEE/RSJ Int.~Conf.~on Intelligent Robots and Systems},
  address       = {San Diego, CA, USA},
  year          = 2007,
  url           = {http://ais.informatik.uni-freiburg.de/publications/papers/steder07irosws.pdf},
}

@InProceedings{grisetti2007iros,
  author        = {G. Grisetti and S. Grzonka and C. Stachniss and P. Pfaff and W. Burgard},
  title         = {{Efficient Estimation of Accurate Maximum Likelihood Maps in 3D}},
  booktitle     = iros,
  year          = 2007,
  address       = {San Diego, CA, USA},
  url           = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/grisetti07iros.pdf},
}

@InProceedings{grisetti2007rss,
  author        = {G. Grisetti and C. Stachniss and S. Grzonka and W. Burgard},
  title         = {{A Tree Parameterization for Efficiently Computing Maximum Likelihood Maps using Gradient Descent}},
  booktitle     = rss,
  year          = 2007,
  address       = {Atlanta, GA, USA},
  url           = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/grisetti07rss.pdf},
}

@Article{stachniss2009amai,
  title         = {{Efficient Exploration of Unknown Indoor Environments using a Team of Mobile Robots}},
  author        = {C. Stachniss and O. Martinez Mozos and W. Burgard},
  journal       = {Annals of Mathematics and Artificial Intelligence},
  year          = 2009,
  volume        = 52,
  issue         = 2,
  pages         = {205ff},
  url           = {http://ais.informatik.uni-freiburg.de/publications/papers/stachniss09amai.pdf},
}

@InProceedings{pfaff2007smart,
  author        = {P. Pfaff and R. Triebel and C. Stachniss and P. Lamon and W. Burgard and R. Siegwart},
  title         = {{Towards Mapping of Cities}},
  booktitle     = icra,
  year          = 2007,
  address       = {Rome, Italy},
  url           = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/pfaff07icra.pdf},
}

@InBook{martinez2007starbook,
  author        = {O. Mart\'{i}nez-Mozos and C. Stachniss and A. Rottmann and W. Burgard},
  editor        = {Thrun, S. and Brooks, R. and Durrant-Whyte, H.},
  title         = {{Robotics Research}},
  chapter       = {Using AdaBoost for Place Labelling and Topological Map Building},
  publisher     = springer,
  year          = 2007,
  volume        = {28},
  series        = springerstaradvanced,
  isbn          = {978-3-540-48110-2},
  url           = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/martinez07springer.pdf},
}

@Article{grisetti2007tro,
  title         = {{Improved Techniques for Grid Mapping with Rao-Blackwellized Particle Filters}},
  author        = {G. Grisetti and C. Stachniss and W. Burgard},
  journal       = tro,
  year          = 2007,
  pages         = {34--46},
  volume        = {23},
  number        = {1},
  url           = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/grisetti07tro.pdf},
}

@Article{grisetti2007jras,
  title         = {{Fast and Accurate SLAM with Rao-Blackwellized Particle Filters}},
  author        = {G. Grisetti and G.D. Tipaldi and C. Stachniss and W. Burgard and D. Nardi},
  journal       = jras,
  year          = {2007},
  pages         = {30--38},
  volume        = {55},
  number        = {1},
  url           = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/grisetti07jras.pdf},
}

@InProceedings{meier2006sensor,
  author        = {D. Meier and C. Stachniss and W. Burgard},
  title         = {{Cooperative Exploration With Multiple Robots Using Low Bandwidth Communication}},
  booktitle     = {{Informationsfusion in der Mess- und Sensortechnik}},
  year          = 2006,
  pages         = {145--157},
  editor        = {Beyerer, J. and Puente Le\'{o}n, F. and Sommer, K.-D.},
  isbn          = {3-86644-053-7},
  url           = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/meier06sensor.pdf},
}

@InProceedings{lamon2006irosws,
  author        = {P. Lamon and C. Stachniss and R. Triebel and P. Pfaff and C. Plagemann and G. Grisetti and S. Kolski and W. Burgard and R. Siegwart},
  title         = {{Mapping with an Autonomous Car}},
  booktitle     = {Workshop on Safe Navigation in Open and Dynamic Environments at the IEEE/RSJ Int.~Conf.~on Intelligent Robots and Systems},
  year          = 2006,
  address       = {Beijing, China},
  url           = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/lamon06iros.pdf},
}

@InProceedings{kolski2006irosws,
  author        = {S. Kolski and D. Furgeson and C. Stachniss and R. Siegwart},
  title         = {{Autonomous Driving in Dynamic Environments}},
  booktitle     = {Workshop on Safe Navigation in Open and Dynamic Environments at the IEEE/RSJ Int.~Conf.~on Intelligent Robots and Systems},
  year          = 2006,
  address       = {Beijing, China},
}

@InProceedings{gil2006iros,
  author        = {A. Gil and O. Reinoso and O. Mart\'{i}nez-Mozos and C. Stachniss and W. Burgard},
  title         = {{Improving Data Association in Vision-based SLAM}},
  booktitle     = iros,
  year          = 2006,
  address       = {Beijing, China},
  url           = {https://pdfs.semanticscholar.org/ea1d/829217e1059bb77aae5b5e2605c981a58063.pdf},
}

@Article{sonntag2006austendodj,
  title         = {Determination of Root Canal Curvatures before and after Canal Preparation (Part {II}): A Method based on Numeric Calculus},
  author        = {D. Sonntag and S. Stachniss-Carp and C. Stachniss and V. Stachniss},
  journal       = {Aust Endod J},
  volume        = {32},
  pages         = {16--25},
  year          = 2006,
  url           = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/sonntag06endod.pdf},
}

@PhDThesis{stachniss2006phd,
  author        = {C. Stachniss},
  title         = {{Exploration and Mapping with Mobile Robots}},
  school        = {University of Freiburg, Department of Computer Science},
  year          = 2006,
  url           = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/stachniss06phd.pdf},
}

@InProceedings{stachniss2006icra,
  author        = {Stachniss, C. and Mart\'{i}nez-Mozos, O. and Burgard, W.},
  title         = {{Speeding-Up Multi-Robot Exploration by Considering Semantic Place Information}},
  booktitle     = icra,
  year          = 2006,
  address       = {Orlando, FL, USA},
  url           = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/stachniss06icra.pdf},
  pages         = {1692--1697},
}

@InProceedings{grisetti2006icra,
  author        = {G. Grisetti and G.D. Tipaldi and C. Stachniss and W. Burgard and D. Nardi},
  title         = {Speeding-Up Rao-Blackwellized {SLAM}},
  booktitle     = icra,
  year          = 2006,
  address       = {Orlando, FL, USA},
  url           = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/grisetti06icra.pdf},
  pages         = {442--447},
}

@InProceedings{plagemann2006euros,
  author        = {C. Plagemann and C. Stachniss and W. Burgard},
  title         = {{Efficient Failure Detection for Mobile Robots using Mixed-Abstraction Particle Filters}},
  editor        = {H.I. Christiensen},
  booktitle     = {European Robotics Symposium 2006},
  publisher     = springer,
  year          = 2006,
  volume        = {22},
  series        = springerstaradvanced,
  pages         = {93--107},
  isbn          = {3-540-32688-X},
  url           = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/plagemann06euros.pdf},
}

@InProceedings{bennewitz2006euros,
  author        = {M. Bennewitz and C. Stachniss and W. Burgard and S. Behnke},
  editor        = {H.I. Christiensen},
  booktitle     = {European Robotics Symposium 2006},
  title         = {{Metric Localization with Scale-Invariant Visual Features using a Single Perspective Camera}},
  publisher     = springer,
  year          = 2006,
  volume        = {22},
  series        = springerstaradvanced,
  pages         = {143--157},
  isbn          = {3-540-32688-X},
  url           = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/bennewitz06euros.pdf},
  keywords      = {Localization},
}

@Article{stachniss2005advancedrobotics,
  author        = {C. Stachniss and D. H\"{a}hnel and W. Burgard and G. Grisetti},
  title         = {{On Actively Closing Loops in Grid-based FastSLAM}},
  journal       = advancedrobotics,
  year          = 2005,
  volume        = {19},
  number        = {10},
  pages         = {1059--1080},
  url           = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/stachniss05ar.pdf},
}

@InProceedings{stachniss2005isrr,
  author        = {Stachniss, C. and Mart\'{i}nez-Mozos, O. and Rottmann, A. and Burgard, W.},
  title         = {{Semantic Labeling of Places}},
  booktitle     = isrr,
  year          = 2005,
  address       = {San Francisco, CA, USA},
  url           = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/stachniss05isrr.pdf},
}

@InProceedings{stachniss2005rss,
  title         = {{Information Gain-based Exploration Using Rao-Blackwellized Particle Filters}},
  author        = {C. Stachniss and G. Grisetti and W. Burgard},
  booktitle     = rss,
  year          = 2005,
  address       = {Cambridge, MA, USA},
  pages         = {65--72},
  url           = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/stachniss05rss.pdf},
}

@InProceedings{meier2005ecmr,
  author        = {D. Meier and C. Stachniss and W. Burgard},
  title         = {{Coordinating Multiple Robots During Exploration Under Communication With Limited Bandwidth}},
  booktitle     = ecmr,
  year          = 2005,
  pages         = {26--31},
  address       = {Ancona, Italy},
  url           = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/meier05ecmr.pdf},
}

@InProceedings{stachniss2005aaai,
  title         = {{Mobile Robot Mapping and Localization in Non-Static Environments}},
  author        = {C. Stachniss and W. Burgard},
  booktitle     = aaaiold,
  year          = 2005,
  address       = {Pittsburgh, PA, USA},
  pages         = {1324--1329},
  url           = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/stachniss05aaai.pdf},
}

@InProceedings{rottmann2005aaai,
  title         = {{Place Classification of Indoor Environments with Mobile Robots using Boosting}},
  author        = {A. Rottmann and O. Mart\'{i}nez-Mozos and C. Stachniss and W. Burgard},
  booktitle     = aaaiold,
  year          = 2005,
  address       = {Pittsburgh, PA, USA},
  pages         = {1306--1311},
  url           = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/rottmann05aaai.pdf},
}

@Article{trahanias2005ram,
  author        = {P. Trahanias and W. Burgard and A. Argyros and D. H\"{a}hnel and H. Baltzakis and P. Pfaff and C. Stachniss},
  title         = {{TOURBOT and WebFAIR: Web-Operated Mobile Robots for Tele-Presence in Populated Exhibitions}},
  journal       = ram,
  year          = 2005,
  volume        = {12},
  number        = {2},
  pages         = {77--89},
  url           = {http://www2.informatik.uni-freiburg.de/~stachnis/pdf/trahanias05webfair-ramprint.pdf},
  abstract      = {This paper presents a number of techniques that are needed for realizing Web-operated mobile robots. These techniques include effective map-building capabilities, a method for obstacle avoidance based on a combination of range and visual information, and advanced Web and onboard robot interfaces. In addition to video streams, the system provides high-resolution virtual reality visualizations that also include the people in the vicinity of the robot. This increases the flexibility of the interface and simultaneously allows a user to understand the navigation actions of the robot. The techniques described in this article have been successfully deployed within the EU-funded projects TOURBOT and WebFAIR, which aimed to develop interactive tour-guided robots able to serve Web as well as on-site visitors. Technical developments in the framework of these projects have resulted in robust and reliable systems that have been demonstrated and validated in real-world conditions. Equally important, the system setup time has been drastically reduced, facilitating its porting to new environments.},
}

@Article{burgard2005tro,
  author        = {W. Burgard and M. Moors and C. Stachniss and F. Schneider},
  title         = {{Coordinated Multi-Robot Exploration}},
  journal       = tro,
  year          = 2005,
  volume        = {21},
  number        = {3},
  pages         = {376--378},
  url           = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/burgard05tro.pdf},
}

@InProceedings{burgard2005snowbird,
  title         = {{Information Gain-based Exploration Using Rao-Blackwellized Particle Filters}},
  author        = {W. Burgard and C. Stachniss and G. Grisetti},
  booktitle     = {Proc. of the Learning Workshop (Snowbird)},
  year          = 2005,
  address       = {Snowbird, UT, USA},
  url           = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/burgard05snowbird.pdf},
}

@InProceedings{stachniss2005icra,
  title         = {Recovering Particle Diversity in a Rao-Blackwellized Particle Filter for {SLAM} after Actively Closing Loops},
  author        = {C. Stachniss and G. Grisetti and W. Burgard},
  booktitle     = icra,
  year          = 2005,
  pages         = {667--672},
  address       = {Barcelona, Spain},
  url           = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/stachniss05icra.pdf},
}

@InProceedings{martinez2005icra,
  title         = {{Supervised Learning of Places from Range Data using Adaboost}},
  author        = {O. Mart\'{i}nez-Mozos and C. Stachniss and W. Burgard},
  booktitle     = icra,
  year          = 2005,
  pages         = {1742--1747},
  address       = {Barcelona, Spain},
  url           = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/martinez05icra.pdf},
}

@InProceedings{grisetti2005icra,
  title         = {{Improving Grid-based SLAM with Rao-Blackwellized Particle Filters by Adaptive Proposals and Selective Resampling}},
  author        = {G. Grisetti and C. Stachniss and W. Burgard},
  booktitle     = icra,
  year          = 2005,
  pages         = {2443--2448},
  address       = {Barcelona, Spain},
  url           = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/grisetti05icra.pdf},
}

@InProceedings{stachniss2004soave,
  author        = {C. Stachniss and G. Grisetti and D. H\"{a}hnel and W. Burgard},
  title         = {{Improved Rao-Blackwellized Mapping by Adaptive Sampling and Active Loop-Closure}},
  booktitle     = soave,
  year          = 2004,
  pages         = {1--15},
  address       = {Ilmenau, Germany},
  url           = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/stachniss04soave.pdf},
  note          = {Invited presentation},
}

@InProceedings{stachniss2004iros,
  author        = {C. Stachniss and D. H\"{a}hnel and W. Burgard},
  title         = {{Exploration with Active Loop-Closing for FastSLAM}},
  booktitle     = iros,
  year          = 2004,
  pages         = {1505--1510},
  address       = {Sendai, Japan},
  url           = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/stachniss04iros.pdf},
}

@InProceedings{stachniss2003dagstuhl,
  author        = {C. Stachniss and D. H\"{a}hnel and W. Burgard},
  title         = {{Grid-based FastSLAM and Exploration with Active Loop Closing}},
  booktitle     = {Online Proc.~of the Dagstuhl Seminar on Robot Navigation (Dagstuhl Seminar~03501)},
  year          = 2003,
  address       = {Dagstuhl, Germany},
}

@InProceedings{stachniss2003iros,
  author        = {C. Stachniss and W. Burgard},
  title         = {{Mapping and Exploration with Mobile Robots using Coverage Maps}},
  booktitle     = iros,
  year          = 2003,
  pages         = {476--481},
  address       = {Las Vegas, NV, USA},
  url           = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/stachniss03iros.pdf},
}

@InProceedings{stachniss2003ecmr,
  author        = {C. Stachniss and W. Burgard},
  title         = {{Using Coverage Maps to Represent the Environment of Mobile Robots}},
  booktitle     = ecmr,
  year          = 2003,
  pages         = {59--64},
  address       = {Radziejowice, Poland},
  url           = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/stachniss03ecmr.pdf},
}

@InProceedings{stachniss2003ijcai,
  author        = {C. Stachniss and W. Burgard},
  title         = {{Exploring Unknown Environments with Mobile Robots using Coverage Maps}},
  booktitle     = ijcai,
  year          = 2003,
  pages         = {1127--1132},
  address       = {Acapulco, Mexico},
  url           = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/stachniss03ijcai.pdf},
}

@InProceedings{stachniss2002iros,
  author        = {C. Stachniss and W. Burgard},
  title         = {{An Integrated Approach to Goal-directed Obstacle Avoidance under Dynamic Constraints for Dynamic Environments}},
  booktitle     = iros,
  year          = 2002,
  pages         = {508--513},
  address       = {Lausanne, Switzerland},
  url           = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/stachniss02iros.pdf},
}

@MastersThesis{stachniss2002diplom,
  author        = {C. Stachniss},
  title         = {{Z}ielgerichtete {K}ollisionsvermeidung f{\"u}r mobile {R}oboter in dynamischen {U}mgebungen},
  school        = {University of Freiburg, Department of Computer Science},
  year          = 2002,
  note          = {In German},
  url           = {http://www.informatik.uni-freiburg.de/~stachnis/pdf/stachniss02diplom.pdf},
}

@Article{kalman1960tasme,
  title         = {{A New Approach to Linear Filtering and Prediction Problems}},
  author        = {R.E. Kalman},
  journal       = {Transactions of the ASME -- Journal of Basic Engineering},
  volume        = {82},
  year          = 1960,
  pages         = {35-45},
  abstract      = {The classical filtering and prediction problem is re-examined using the Bode- Shannon representation of random processes and the state transition method of analysis of dynamic systems. New results are: (1) The formulation and methods of solution of the problem apply without modifica- tion to stationary and nonstationary statistics and to growing-memory and infinite-memory filters. (2) A nonlinear difference (or differential) equation is derived for the covariance matrix of the optimal estimation error. From the solution of this equation the coefficients of the difference (or differential) equation of the optimal linear filter are ob- tained without further calculations. (3) The filtering problem is shown to be the dual of the noise-free regulator problem. The new method developed here is applied to two well-known problems, confirming and extending earlier results. The discussion is largely self-contained and proceeds from first principles; basic concepts of the theory of random processes are reviewed in the Appendix.},
  keywords      = {Kalman Filter, State Estimation},
}

@InProceedings{julier2002acc,
  title         = {{The Scaled Unscented Transformation}},
  author        = {S.J. Julier},
  year          = 2002,
  booktitle     = acc,
  keywords      = {Kalman Filter, Non-Linear Estimation, Unscented Filtering, State Estimation},
  abstract      = {This paper describes a generalisation of the unscented transformation (UT) which allows sigma points to be scaled to an arbitrary dimension. The UT is a method for predicting means and covariances in nonlinear systems. A set of samples are deterministically chosen which match the mean and covariance of a (not necessarily Gaussian-distributed) probability distribution. These samples can be scaled by an arbitrary constant. The method guarantees that the mean and covariance second order accuracy in mean and covariance, giving the same performance as a second order truncated filter but without the need to calculate any Jacobians or Hessians. The impacts of scaling issues are illustrated by considering conversions from polarto Cartesian coordinates with large angular uncertainties.},
}

@Article{deng2013fntsp,
  title         = {{Deep Learning: Methods and Applications}},
  author        = {L. Deng and D. Yu},
  year          = 2013,
  volume        = 7,
  issue         = {3-4},
  pages         = {197-387},
  journal       = {Foundations and Trends in Signal Processing},
  keywords      = {Deep Learning},
  abstract      = {This monograph provides an overview of general deep learning methodology and its applications to a variety of signal and information processing tasks. The application areas are chosen with the following three criteria in mind: (1) expertise or knowledge of the authors; (2) the application areas that have already been transformed by the successful use of deep learning technology, such as speech recognition and com- puter vision; and (3) the application areas that have the potential to be impacted significantly by deep learning and that have been experiencing research growth, including natural language and text processing, information retrieval, and multimodal information processing empowered by multi-task deep learning.},
  doi           = {10.1561/2000000039},
}

@PhDThesis{sprunk2015phd,
  author        = {C. Sprunk},
  title         = {{Highly Accurate Mobile Robot Navigation}},
  school        = {Albert-Ludwigs-University of Freiburg, Department of Computer Science},
  year          = 2015,
  url           = {http://ais.informatik.uni-freiburg.de/publications/papers/sprunk15phd.pdf},
}

@PhDThesis{agarwal2015phd,
  author        = {P. Agarwal},
  title         = {{Robust Graph-Based Localization and Mapping}},
  school        = {Albert-Ludwigs-University of Freiburg, Department of Computer Science},
  year          = 2015,
  url           = {http://ais.informatik.uni-freiburg.de/publications/papers/agarwal15phd.pdf},
}

@InProceedings{agarwal2015iros,
  author        = {P. Agarwal and W. Burgard and L. Spinello},
  title         = {{Metric Localization using Google Street View}},
  booktitle     = iros,
  year          = 2015,
  url           = {http://ais.informatik.uni-freiburg.de/publications/papers/agarwal15iros.pdf},
  address       = {Hamburg, Germany},
}

@InProceedings{naseer2015ecmr,
  author        = {T. Naseer and B. Suger and M. Ruhnke and W. Burgard},
  title         = {{Vision-Based Markov Localization Across Large Perceptual Changes}},
  booktitle     = ecmr,
  year          = 2015,
  url           = {http://ais.informatik.uni-freiburg.de/publications/papers/naseer15ecmr.pdf},
  address       = {Lincoln},
}

@InProceedings{mazuran2015icra,
  author        = {M. Mazuran and C. Sprunk and W. Burgard and G.D. Tipaldi},
  title         = {Lex{TOR}: Lexicographic Teach Optimize and Repeat Based on User Preferences},
  booktitle     = icra,
  year          = 2015,
  url           = {http://ais.informatik.uni-freiburg.de/publications/papers/mazuran15icra.pdf},
  address       = {Seattle},
}

@Article{endres2014tro,
  author        = {F. Endres and J. Hess and J. Sturm and D. Cremers and W. Burgard},
  title         = {{3D Mapping with an RGB-D Camera}},
  journal       = tro,
  volume        = {30},
  number        = {1},
  pages         = {177--187},
  year          = 2014,
  doi           = {10.1109/TRO.2013.2279412},
  url           = {http://ais.informatik.uni-freiburg.de/publications/papers/endres14tro.pdf},
}

@Book{heineman2009algorithmsbook,
  author        = {G.T. Heineman and G. Pollice and S. Selkow},
  title         = {{Algorithms in a Nutshell}},
  publisher     = {O{\' }Reiley},
  year          = 2009,
  keywords      = {Book, Programming, Algorithms},
  abstract      = {As authors of this book, we answer the question that has led you here: Can I use algorithm X to solve my problem? If so, how do I implement it?},
}

@Book{lischner2003cppbook,
  author        = {R. Lischner},
  title         = {{C++ in a Nutshell}},
  publisher     = {O{\' }Reiley},
  year          = 2003,
  keywords      = {Book, Programming, CPP},
  abstract      = {C++ in a Nutshell is a reference to the C++ language and library. Being a Nutshell guide, it is not a comprehensive manual, but it is complete enough to cover everything a working professional needs to know. Nonetheless, C++ is such a large and complex language that even this Nutshell guide is a large book. This book covers the C++ standard, the international standard published as ISO/IEC 14882:1998(E), Programming LanguagesC++, plus Technical Corrigendum 1. Many implementations of C++ extend the language and standard library. Except for brief mentions of language and library extensions in the appendixes, this book covers only the standard. The standard library is largeit includes strings, containers, common algorithms, and much morebut it omits much that is commonplace in computing today: concurrency, network protocols, database access, graphics, windows, and so on. See Appendix B for information about nonstandard libraries that provide additional functionality. This book is a reference. It is not a tutorial. Newcomers to C++ might find portions of this book difficult to understand. Although each section contains some advice on idioms and the proper use of certain language constructs, the main focus is on the reference material.},
}

@Book{schildt2003cppbook,
  author        = {H. Schildt},
  title         = {{C++ From the Ground Up}},
  publisher     = {McGraw-Hill/Osborne},
  year          = 2003,
  keywords      = {Book, Programming, CPP},
  abstraxct     = {This book teaches you how to program in C++, the most powerful computer language in use today. No previous programming experience is required. The book starts with the basics, covers the fundamentals, moves on to the core of the language, and concludes with its more advanced features. By the time you finish, you will be an accomplished C++ programmer.},
}

@Book{stroustrup1997cppbook,
  author        = {B. Stroustrup},
  title         = {{The C++ Programming Language}},
  publisher     = {AT\&T},
  keywords      = {Book, Programming, CPP},
  year          = 1997,
  abstract      = {This book introduces standard C++ and the key programming and design techniques supported by C++. Standard C++ is a far more powerful and polished language than the version of C++ intro- duced by the first edition of this book. New language features such as namespaces, exceptions, templates, and run-time type identification allow many techniques to be applied more directly than was possible before, and the standard library allows the programmer to start from a much higher level than the bare language. The primary aim of this book is to help the reader understand how the facilities offered by C++ support key programming techniques. The aim is to take the reader far beyond the point where he or she gets code running primarily by copying examples and emulating programming styles from other languages. Only a good understanding of the ideas behind the language facilities leads to mastery. Supplemented by implementation documentation, the information provided is sufficient for completing significant real-world projects. The hope is that this book will help the reader gain new insights and become a better programmer and designer.},
}

@Book{szeliski2010cvbook,
  author        = {R. Szeliski},
  title         = {{Computer Vision: Algorithms and Applications}},
  publisher     = springer,
  keywords      = {Book, Computer Vision},
  year          = 2010,
  abstract      = {This book also reflects my 20 years experience doing computer vision research in corpo- rate research labs, mostly at Digital Equipment Corporations Cambridge Research Lab and at Microsoft Research. In pursuing my work, I have mostly focused on problems and solution techniques (algorithms) that have practical real-world applications and that work well in practice. Thus, this book has more emphasis on basic techniques that work under real-world conditions and less on more esoteric mathematics that has intrinsic elegance but less practical applicability. This book is suitable for teaching a senior-level undergraduate course in computer vision to students in both computer science and electrical engineering. I prefer students to have either an image processing or a computer graphics course as a prerequisite so that they can spend less time learning general background mathematics and more time studying computer vision techniques. The book is also suitable for teaching graduate-level courses in computer vision (by delving into the more demanding application and algorithmic areas) and as a general reference to fundamental techniques and the recent research literature. To this end, I have attempted wherever possible to at least cite the newest research in each sub-field, even if technical details are too complex to cover in the book itself.},
}

@Book{lavalle2006planningbook,
  author        = {S.M. LaValle},
  title         = {{Planning Algorithms}},
  publisher     = {Cambridge University Press},
  year          = 2006,
  keywords      = {Book, Motion Planning},
  abstract      = {Due to many exciting developments in the fields of robotics, artificial intelligence, and control theory, three topics that were once quite distinct are presently on a collision course. In robotics, motion planning was originally concerned with problems such as how to move a piano from one room to another in a house without hitting anything. The field has grown, however, to include complications such as uncertainties, multiple bodies, and dynamics. In artificial intelligence, planning originally meant a search for a sequence of logical operators or actions that transform an initial world state into a desired goal state. Presently, planning extends beyond this to include many decision-theoretic ideas such as Markov decision processes, imperfect state information, and game-theoretic equilibria. Although control theory has traditionally been concerned with issues such as stability, feedback, and optimality, there has been a growing interest in designing algorithms that find feasible open-loop trajectories for nonlinear systems. In some of this work, the term motion planning has been applied, with a different interpretation of its use in robotics. Thus, even though each originally considered different problems, the fields of robotics, artificial intelligence, and control theory have expanded their scope to share an interesting common ground. In this text, I use the term planning in a broad sense that encompasses this common ground. This does not, however, imply that the term is meant to cover everything important in the fields of robotics, artificial intelligence, and control theory. The presentation focuses on algorithm issues relating to planning. Within robotics, the focus is on designing algorithms that generate useful motions by processing complicated geometric models. Within artificial intelligence, the focus is on designing systems that use decision-theoretic models compute appropriate actions. Within control theory, the focus is on algorithms that compute feasible trajectories for systems, with some additional coverage of feedback and optimality. Analytical techniques, which account for the majority of control theory literature, are not the main focus here. The phrase planning and control is often used to identify complementary issues in developing a system. Planning is often considered as a higher-level process than control. In this text, I make no such distinctions. Ignoring historical connotations that come with the terms, planning or control can be used in terchangeably. Either refers to some kind of decision making in this text, with no associated notion of high or low level. A hierarchical approach can be developed, and either level could be called planning or control without any difference in meaning.},
}

@Book{mackay2004informationbook,
  author        = {D. MacKay},
  title         = {{Information Theory, Inference, and Learning Algorithms}},
  publisher     = {Cambridge University Press},
  year          = 2004,
  keywords      = {Book, Machine Learning, Information Theory},
  abstract      = {This book is aimed at senior undergraduates and graduate students in Engineering, Science, Mathematics, and Computing. It expects familiarity with calculus, probability theory, and linear algebra as taught in a first- or second- year undergraduate course on mathematics for scientists and engineers. Conventional courses on information theory cover not only the beautiful theoretical ideas of Shannon, but also practical solutions to communication problems. This book goes further, bringing in Bayesian data modelling, Monte Carlo methods, variational methods, clustering algorithms, and neural networks. Why unify information theory and machine learning? Because they are two sides of the same coin. In the 1960s, a single field, cybernetics, was populated by information theorists, computer scientists, and neuroscientists, all studying common problems. Information theory and machine learning still belong together. Brains are the ultimate compression and communication systems. And the state-of-the-art algorithms for both data compression and error-correcting codes use the same tools as machine learning.},
}

@Book{alpaydin2010mlbook,
  author        = {E. Alpaydin},
  title         = {{Introdiction to Machine Learning}},
  publisher     = mitpress,
  keywords      = {Book, Machine Learning},
  year          = 2010,
  abstract      = {Machine learning is programming computers to optimize a performance criterion using example data or past experience. We need learning in cases where we cannot directly write a computer program to solve a given problem, but need example data or experience. One case where learning is necessary is when human expertise does not exist, or when humans are unable to explain their expertise. Consider the recognition of spoken speechthat is, converting the acoustic speech signal to an ASCII text; we can do this task seemingly without any difficulty, but we are unable to explain how we do it. Different people utter the same word differently due to differences in age, gender, or accent. In machine learning, the approach is to collect a large collection of sample utterances from different people and learn to map these to words. The book discusses many learning methods that have their bases in different fields: statistics, pattern recognition, neural networks, artificial intelligence, signal processing, control, and data mining. In the past, research in these different communities followed different paths with different emphases. In this book, the aim is to incorporate them together to give a unified treatment of the problems and the proposed solutions to them. This is an introductory textbook, intended for senior undergraduate and graduate-level courses on machine learning, as well as engineers working in the industry who are interested in the application of these methods. The prerequisites are courses on computer programming, probability, calculus, and linear algebra. The aim is to have all learning algorithms sufficiently explained so it will be a small step from the equations given in the book to a computer program. For some cases, pseudocode of algorithms are also included to make this task easier.},
}

@Book{strunk1997stylebook,
  author        = {W. Strunk and E.B. White},
  title         = {{The Elements of Style}},
  publisher     = {Allyn and Bacon},
  keywords      = {Book, Writing},
  year          = 1979,
  abstract      = {No book in shorter space, with fewer words, will help any writer more than this persistent little volume.},
}

@Book{thrun2005probrobbook,
  author        = {S. Thrun and W. Burgard and D. Fox},
  title         = {{Probabilistic Robotics}},
  publisher     = mitpress,
  year          = 2005,
  keywords      = {Book, Probabilistic Robotics},
  abstract      = {This book provides a comprehensive introduction into the emerging field of probabilistic robotics. Probabilistic robotics is a subfield of robotics concerned with perception and control. It relies on statistical techniques for representing information and making decisions. By doing so, it accommodates the uncertainty that arises in most contemporary robotics applications. In recent years, probabilistic techniques have become one of the dominant paradigms for algorithm design in robotics. This monograph provides a first comprehensive introduction into some of the major techniques in this field. This book has a strong focus on algorithms. All algorithms in this book are based on a single overarching mathematical foundation: Bayes rule, and its temporal extension known as Bayes filters. This unifying mathematical framework is the core commonality of probabilistic algorithms.},
}

@Book{rasmussen2006gpbook,
  author        = {C.E. Rasmussen and C.K.I. Williams},
  title         = {{Gaussian Processes for Machine Learning}},
  publisher     = mitpress,
  year          = 2006,
  keywords      = {Book, Machine Learning, Gaussian Process},
  url           = {http://www.gaussianprocess.org/gpml/chapters/RW.pdf},
  abstract      = {Over the last decade there has been an explosion of work in the kernel machines area of machine learning. Probably the best known example of this is work on support vector machines, but during this period there has also been much activity concerning the application of Gaussian process models to machine learning tasks. The goal of this book is to provide a systematic and unified treatment of this area. Gaussian processes provide a principled, practical, probabilistic approach to learning in kernel machines. This gives advantages with respect to the interpretation of model predictions and provides a wellfounded framework for learning and model selection. Theoretical and practical developments of over the last decade have made Gaussian processes a serious competitor for real supervised learning applications.},
}

@Book{mitchell1997mlbook,
  author        = {T. Mitchell},
  title         = {{Machine Learning}},
  publisher     = {McGraw-Hill Science},
  year          = 1997,
  keywords      = {Book, Machine Learning},
  abstract      = {This book covers the field of machine learning, which is the study of algorithms that allow computer programs to automatically improve through experience. The book is intended to support upper level undergraduate and introductory level graduate courses in machine learning},
}

@Book{murphy2012mlbook,
  author        = {K.P. Murphy},
  title         = {{Machine Learning -- A Probabilistic Perspective}},
  publisher     = mitpress,
  year          = 2012,
  keywords      = {Book, Machine Learning},
  abstract      = {With the ever increasing amounts of data in electronic form, the need for automated methods for data analysis continues to grow. The goal of machine learning is to develop methods that can automatically detect patterns in data, and then to use the uncovered patterns to predict future data or other outcomes of interest. Machine learning is thus closely related to the fields of statistics and data mining, but di ers slightly in terms of its emphasis and terminology. This book provides a detailed introduction to the field, and includes worked examples drawn from application domains such as molecular biology, text processing, computer vision, and robotics.},
}

@Article{welch2001acm,
  author        = {G. Welch and G. Bischop},
  title         = {{An Introduction to the Kalman Filter}},
  journal       = {SIGHGRAPH 2001 Tutorial Course},
  publisher     = {ACM},
  year          = 2001,
  keywords      = {Tutorial, Kalman Filter},
  abstract      = {The Kalman filter is a mathematical power tool that is playing an increasingly important role in computer graphics as we include sensing of the real world in our systems. The good news is you do not have to be a mathematical genius to understand and effectively use Kalman filters. This tutorial is designed to provide developers of graphical systems with a basic understanding of this important mathematical tool.While the Kalman filter has been around for about 30 years, it (and related optimal estimators) have recently started popping up in a wide variety of computer graphics applications. These applications span from simulating musical instruments in VR, to head tracking, to extracting lip motion from video sequences of speakers, to fitting spline surfaces over collections of points. The Kalman filter is the best possible (optimal) estimator for a large class of problems and a very effective and useful estimator for an even larger class. With a few conceptual tools, the Kalman filter is actually very easy to use. We will present an intuitive approach to this topic that will enable developers to approach the extensive literature with confidence.},
}

@Book{foerstner2016pcvbook,
  author        = {W. F{\"o}rstner and B. Wrobel},
  title         = {{Photogrammetric Computer Vision -- Statistics, Geometry, Orientation and Reconstruction}},
  publisher     = springer,
  year          = 2016,
  keywords      = {Book, Computer Vision, Photogrammetry, Geometry, State Estimation},
  abstract      = {This textbook on Photogrammetric Computer Vision  Statistics, Geometry, Orientation and Reconstruction provides a statistical treatment of the geometry of multiple view anal- ysis useful for camera calibration, orientation, and geometric scene reconstruction. The book is the first to offer a joint view of photogrammetry and computer vision, two fields that have converged in recent decades. It is motivated by the need for a conceptually consistent theory aiming at generic solutions for orientation and reconstruction problems. Large parts of the book result from teaching bachelors and masters courses for students of geodesy within their education in photogrammetry. Most of these courses were simultaneously offered as subjects in the computer science faculty.The book provides algorithms for various problems in geometric computation and in vision metrology, together with mathematical justification and statistical analysis allowing thorough evaluation. The book aims at enabling researchers, software developers, and practitioners in the photogrammetric and GIS industry to design, write, and test their own algorithms and application software using statistically founded concepts to obtain optimal solutions and to realize self-diagnostics within algorithms. This is essential when applying vision tech- niques in practice. The material of the book can serve as a source for different levels of undergraduate and graduate courses in photogrammetry, computer vision, and computer graphics, and for research and development in statistically based geometric computer vision methods. The sixteen chapters of the book are self-contained, are illustrated with numerous fig- ures, have exercises, and are supported by an appendix and an index. Many of the examples and exercises can be verified or solved using the Matlab routines available on the home page of the book, which also contains solutions to some of the exercises.},
}

@InProceedings{newcombe2011ismar,
  author        = {R. A. Newcombe and S. Izadi and O. Hilliges and D. Molyneaux and D. Kim and A. J. Davison and P. Kohli and J. Shotton and S. Hodges and A. Fitzgibbon},
  title         = {{KinectFusion: Real-Time Dense Surface Mapping and Tracking}},
  booktitle     = ismar,
  year          = 2011,
  pages         = {127--136},
  keywords      = {RGB-D, SLAM, Mapping},
  abstract      = {We present a system for accurate real-time mapping of complex and arbitrary indoor scenes in variable lighting conditions, using only a moving low-cost depth camera and commodity graphics hardware. We fuse all of the depth data streamed from a Kinect sensor into a single global implicit surface model of the observed scene in real-time. The current sensor pose is simultaneously obtained by tracking the live depth frame relative to the global model using a coarse-to-fine iterative closest point (ICP) algorithm, which uses all of the observed depth data available. We demonstrate the advantages of tracking against the growing full surface model compared with frame-to-frame tracking, obtaining tracking and mapping results in constant time within room sized scenes with limited drift and high accuracy. We also show both qualitative and quantitative results relating to various aspects of our tracking and mapping system. Modelling of natural scenes, in real-time with only commodity sensor and GPU hardware, promises an exciting step forward in augmented reality (AR), in particular, it allows dense surfaces to be reconstructed in real-time, with a level of detail and robustness beyond any solution yet presented using passive computer vision.},
  url           = {https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/ismar2011.pdf},
}

@InBook{carrillo2015springer,
  author        = {H. Carrillo and J.A. Castellanos},
  title         = {{Navigation Under Uncertainty Based on Active SLAM Concepts}},
  booktitle     = {Studies in Systems, Decision and Control},
  year          = 2015,
  publisher     = {Springer International Publishing},
  pages         = {209--235},
  isbn          = {978-3-319-26327-4},
  doi           = {10.1007/978-3-319-26327-4_9},
  abstract      = {This chapter addresses the problem of path planning considering uncertainty criteria over the belief space. We propose a path planning algorithm that uses a determinant-based measure of uncertainty and a reduced representation of the environment, in order to obtain the minimum uncertainty path from a roadmap. The determinant-based measure of uncertainty is borrowed from the active SLAM literature. We also present in this chapter an overview of the active SLAM problem. Our path planning proposal does not require a priori knowledge of the environment due to the construction of the roadmap via a graph-based SLAM algorithm. We report experimental results of our proposal in four datasets that show its feasibility to obtain the minimum uncertainty path towards an autonomous navigation framework. We also show an improvement in the computation time with respect to the state of the art.},
}


@Article{ampatzidis2017sustainability,
  author        = {Y. Ampatzidis and L. De Bellis and A. Luvisi},
  title         = {{iPathology: Robotic Applications and Management of Plants and Plant Diseases}},
  journal       = {Sustainability},
  publisher     = {MDPI},
  year          = 2017,
  volume        = 9,
  issue         = 6,
  doi           = {10.3390/su9061010},
  keywords      = {Sustainability, Agriculture Robotics},
  url           = {http://www.mdpi.com/2071-1050/9/6/1010/pdf},
  abstract      = {The rapid development of new technologies and the changing landscape of the online world (e.g., Internet of Things (IoT), Internet of All, cloud-based solutions) provide a unique opportunity for developing automated and robotic systems for urban farming, agriculture, and forestry. Technological advances in machine vision, global positioning systems, laser technologies, actuators, and mechatronics have enabled the development and implementation of robotic systems and intelligent technologies for precision agriculture. Herein, we present and review robotic applications on plant pathology and management, and emerging agricultural technologies for intra-urban agriculture. Greenhouse advanced management systems and technologies have been greatly developed in the last years, integrating IoT and WSN (Wireless Sensor Network). Machine learning, machine vision, and AI (Artificial Intelligence) have been utilized and applied in agriculture for automated and robotic farming. Intelligence technologies, using machine vision/learning, have been developed not only for planting, irrigation, weeding (to some extent), pruning, and harvesting, but also for plant disease detection and identification. However, plant disease detection still represents an intriguing challenge, for both abiotic and biotic stress. Many recognition methods and technologies for identifying plant disease symptoms have been successfully developed; still, the majority of them require a controlled environment for data acquisition to avoid false positives. Machine learning methods (e.g., deep and transfer learning) present promising results for improving image processing and plant symptom identification. Nevertheless, diagnostic specificity is a challenge for microorganism control and should drive the development of mechatronics and robotic solutions for disease management.},
}

@Article{dou2016acmgraphics,
  author        = {M. Dou and S. Khamis and Y. Degtyarev and P. Davidson and S.R. Fanello and A. Kowdle and S.O. Escolano and C. Rhemann and D. Kim and J. Taylor and P. Kohli and V. Tankovich and S. Izadi},
  title         = {{Fusion4D: Real-time Performance Capture of Challenging Scenes}},
  journal       = acmgraphics,
  year          = 2016,
  volume        = 35,
  issue         = 4,
  keywords      = {Kinect, Mapping, Dynamic},
  url           = {http://dl.acm.org/ft_gateway.cfm?id=2925969&ftid=1755905&dwn=1&CFID=967720923&CFTOKEN=61468293},
  abstract      = {We contribute a new pipeline for live multi-view performance capture, generating temporally coherent high-quality reconstructions in real-time. Our algorithm supports both incremental reconstruction, improving the surface estimation over time, as well as parameterizing the nonrigid scene motion. Our approach is highly robust to both large frame-to-frame motion and topology changes, allowing us to reconstruct extremely challenging scenes. We demonstrate advantages over related real-time techniques that either deform an online generated template or continually fuse depth data nonrigidly into a single reference model. Finally, we show geometric reconstruction results on par with offline methods which require orders of magnitude more processing time and many more RGBD cameras.},
}

@InProceedings{hess2016icra,
    author     = {W. Hess and D. Kohler and H. Rapp and D. Andor},
    title      = {{Real-Time Loop Closure in 2D LIDAR SLAM}},
    booktitle  = icra,
    year       = 2016,
    keywords   = {SLAM, Range Sensing},
    abstract   = {Portable laser range-finders, further referred to as LIDAR, and simultaneous localization and mapping (SLAM) are an efficient method of acquiring as-built floor plans. Generating and visualizing floor plans in real-time helps the operator assess the quality and coverage of capture data. Building a portable capture platform necessitates operating under limited computational resources. We present the approach used in our backpack mapping platform which achieves real-time mapping and loop closure at a 5 cm resolution. To achieve realtime loop closure, we use a branch-and-bound approach for computing scan-to-submap matches as constraints. We provide experimental results and comparisons to other well known approaches which show that, in terms of quality, our approach is competitive with established techniques.},
    url        = {proceedings:hess2016icra.pdf},
}

@InProceedings{vongkulbhisal2017cvpr,
    author     = {J. Vongkulbhisal and F. D. l. Torre and J. P. Costeira},
    title      = {{Discriminative Optimization: Theory and Applications to Point Cloud Registration}},
    booktitle  = cvpr,
    year       = 2017,
    abstract   = {Many computer vision problems are formulated as the optimization of a cost function. This approach faces two main challenges: (1) designing a cost function with a local optimum at an acceptable solution, and (2) developing an efficient numerical method to search for one (or multiple) of these local optima. While designing such functions is feasible in the noiseless case, the stability and location of local optima are mostly unknown under noise, occlusion, or missing data. In practice, this can result in undesirable local optima or not having a local optimum in the expected place. On the other hand, numerical optimization algorithms in high-dimensional spaces are typically local and often rely on expensive first or second order information to guide the search. To overcome these limitations, this paper proposes Discriminative Optimization (DO), a method that learns search directions from data without the need of a cost function. Specifically, DO explicitly learns a sequence of updates in the search space that leads to stationary points that correspond to desired solutions. We provide a formal analysis of DO and illustrate its benefits in the problem of 2D and 3D point cloud registration both in synthetic and range-scan data. We show that DO outperforms state-of-the-art algorithms by a large margin in terms of accuracy, robustness to perturbations, and computational efficiency.},
    url        = {proceedings:vongkulbhisal2017cvpr.pdf},
}

@INPROCEEDINGS{serafin2015iros,
  author = {J. Serafin and G. Grisetti},
  title = {{NICP: Dense Normal Based Point Cloud Registration}},
  booktitle = iros,
  year = {2015},
  pages = {742--749},
}

@ARTICLE{serafin2017ras,
 author = {Serafin, J. and Grisetti, G.},
 title = {Using Extended Measurements and Scene Merging for Efficient and Robust Point Cloud Registration},
 journal = jras,
 issue_date = {June 2017},
 volume = {92},
 number = {C},
 month = jun,
 year = {2017},
 issn = {0921-8890},
 pages = {91--106},
 numpages = {16},
 url = {https://doi.org/10.1016/j.robot.2017.03.008},
 doi = {10.1016/j.robot.2017.03.008},
 acmid = {3096992},
 publisher = {North-Holland Publishing Co.},
 address = {Amsterdam, The Netherlands, The Netherlands},
 keywords = {Iterative closest point (ICP), Point cloud registration, Pose tracking},
}

@INPROCEEDINGS{nister2004cvpr,
  author={D. Nister and O. Naroditsky and J. Bergen},
  booktitle=cvpr,
  title={Visual odometry},
  year={2004},
  volume={1},
  pages={I-652-I-659 Vol.1},
  keywords={image matching;motion estimation;navigation;vehicles;video cameras;video signal processing;aerial platforms;automotive platforms;autonomous ground vehicle;camera motion;camera trajectories;feature tracker;geometric hypothesize-and-test architecture;global positioning system;handheld platforms;image trajectories;inertial navigation;motion estimation;pose estimation method;robust estimation;single moving camera;stereo head;video rate;visual odometry;Cameras;Delay estimation;Global Positioning System;Head;Layout;Motion estimation;Navigation;Real time systems;Robustness;Tracking},
  doi={10.1109/CVPR.2004.1315094},
  ISSN={1063-6919},
  month={June},
}

@ARTICLE{forster2017tro,
  author={C. Forster and Z. Zhang and M. Gassner and M. Werlberger and D. Scaramuzza},
  journal=tro,
  title={SVO: Semidirect Visual Odometry for Monocular and Multicamera Systems},
  year={2017},
  volume={33},
  number={2},
  pages={249-265},
  keywords={image texture;sensor fusion;SVO;feature-based methods;high-frequency texture;image intensity gradients;monocular systems;multicamera systems;probabilistic depth estimation algorithm;semidirect visual odometry;Cameras;Feature extraction;Optimization;Real-time systems;Robustness;Tracking;Visualization;Robot vision;simultaneous localization and mapping (SLAM)},
  doi={10.1109/TRO.2016.2623335},
  ISSN={1552-3098},
  month={April},
}

@INPROCEEDINGS{bloesh2015iros,
  author={M. Bloesch and S. Omari and M. Hutter and R. Siegwart},
  booktitle=iros,
  title={Robust visual inertial odometry using a direct EKF-based approach},
  year={2015},
  pages={298-304},
  abstract={In this paper, we present a monocular visual-inertial
                  odometry algorithm which, by directly using pixel
                  intensity errors of image patches, achieves accurate
                  tracking performance while exhibiting a very high
                  level of robustness. After detection, the tracking
                  of the multilevel patch features is closely coupled
                  to the underlying extended Kalman filter (EKF) by
                  directly using the intensity errors as innovation
                  term during the update step. We follow a purely
                  robocentric approach where the location of 3D
                  landmarks are always estimated with respect to the
                  current camera pose. Furthermore, we decompose
                  landmark positions into a bearing vector and a
                  distance parametrization whereby we employ a minimal
                  representation of differences on a corresponding
                  -Algebra in order to achieve better consistency and
                  to improve the computational performance. Due to the
                  robocentric, inverse-distance landmark
                  parametrization, the framework does not require any
                  initialization procedure, leading to a truly
                  power-up-and-go state estimation system. The
                  presented approach is successfully evaluated in a
                  set of highly dynamic hand-held experiments as well
                  as directly employed in the control loop of a
                  multirotor unmanned aerial vehicle (UAV).},
  doi={10.1109/IROS.2015.7353389},
  month={Sept},
}

@ARTICLE{jaimez2015tro,
  author={M. Jaimez and J. Gonzalez-Jimenez},
  journal=tro,
  title={Fast Visual Odometry for 3-D Range Sensors},
  year={2015},
  volume={31},
  number={4},
  pages={809-822},
  keywords={distance measurement;image sensors;iterative methods;public domain software;robot vision;3-D motion;3-D range sensors;GICP;RDVO;angular velocity;fast visual odometry;free-flying range sensor;generalized iterative closest point;linear velocity;open source license;range camera;range flow constraint equation;robotic community;robust dense visual odometry;sensed points;temporal flow;Cameras;Mathematical model;Optical imaging;Optical sensors;Robots;Visualization;Range sensors;real time;visual odometry},
  doi={10.1109/TRO.2015.2428512},
  ISSN={1552-3098},
  month={Aug},
}

@inproceedings{segal2009rss,
  title={Generalized-ICP.},
  author={Segal, A. and Haehnel, D. and Thrun, S.},
  booktitle=rss,
  volume={2},
  number={4},
  year={2009}
}

@ARTICLE{yang2016pami,
  author={J. Yang and H. Li and D. Campbell and Y. Jia},
  journal=pami,
  title={Go-ICP: A Globally Optimal Solution to 3D ICP Point-Set Registration},
  year={2016},
  volume={38},
  number={11},
  pages={2241-2254},
  keywords={Convergence;Iterative closest point algorithm;Optimization;Robustness;Three-dimensional displays;Yttrium;3D point-set registration;SE(3) space search;branch-and-bound;global optimization;iterative closest point},
  doi={10.1109/TPAMI.2015.2513405},
  ISSN={0162-8828},
  month={Nov},
}

@ARTICLE{zhang2016auro,
  author = {J. Zhang and S. Singh},
  title = {{Low-drift and real-time lidar odometry and mapping}},
  journal = {Autonomous Robots},
  year = {2016},
  pages = {1--16}
}

@INPROCEEDINGS{zhang2013rss,
  author = {J. Zhang and S. Singh},
  title = {{LOAM: Lidar Odometry and Mapping in Real-time}},
  booktitle = rss,
  year = {2013}
}

@article{behley17ral,
  author = {J. Behley and C. Stachniss},
  title = {Real-Time Surfel-Based Odometry from 3D Laser Range Data},
  journal = ral,
  note = {Currently under review},
}

@INPROCEEDINGS{keller2013threedv,
  author = {M. Keller and D. Lefloch and M. Lambers and S. Izadi},
  title = {{Real-time 3D Reconstruction in Dynamic Scenes using Point-based
  Fusion}},
  booktitle = threedv,
  year = {2013},
  pages = {1--8}
}

@ARTICLE{weise2011cviu,
  author = {T. Weise and T. Wismer and B. Leibe and L. Van Gool},
  title = {{Online loop closure for real-time interactive 3D scanning}},
  journal = cviu,
  year = {2011},
  volume = {115},
  pages = {635--648}
}

@inproceedings{slavcheva2016eccv,
  title={SDF-2-SDF: Highly Accurate 3D Object Reconstruction},
  author={Slavcheva, M. and Kehl, W. and Navab, N. and Ilic, S.},
  booktitle=eccv,
  pages={680--696},
  year={2016},
  abstract="This paper addresses the problem of 3D object reconstruction
                  using RGB-D sensors. Our main contribution is a
                  novel implicit-to-implicit surface registration
                  scheme between signed distance fields (SDFs),
                  utilized both for the real-time frame-to-frame
                  camera tracking and for the subsequent global
                  optimization. SDF-2-SDF registration circumvents
                  expensive correspondence search and allows for
                  incorporation of multiple geometric constraints
                  without any dependence on texture, yielding highly
                  accurate 3D models. An extensive quantitative
                  evaluation on real and synthetic data demonstrates
                  improved tracking and higher fidelity
                  reconstructions than a variety of state-of-the-art
                  methods. We make our data publicly available,
                  creating the first object reconstruction dataset to
                  include ground-truth CAD models and RGB-D sequences
                  from sensors of various quality.",
  organization={Springer International Publishing}
}

@inproceedings{engel2014eccv,
  title={LSD-SLAM: Large-scale direct monocular SLAM},
  author={Engel, J. and Sch{\"o}ps, T. and Cremers, D.},
  booktitle=eccv,
  pages={834--849},
  year={2014},
  organization={Springer, Cham}
}

@inproceedings{kerl2013icra,
  title={Robust odometry estimation for RGB-D cameras},
  author={Kerl, C. and Sturm, J. and Cremers, D.},
  booktitle= icra,
  pages={3748--3754},
  year={2013},
  abstract = {The goal of our work is to provide a fast and accurate
                  method to estimate the camera motion from RGB-D
                  images. Our approach registers two consecutive RGB-D
                  frames directly upon each other by minimizing the
                  photometric error. We estimate the camera motion
                  using non-linear minimization in combination with a
                  coarse-to-fine scheme},
  organization={IEEE}
}

@inproceedings{kerl2013iros,
  title={Dense visual SLAM for RGB-D cameras},
  author={Kerl, C. and Sturm, J. and Cremers, D.},
  booktitle=iros,
  pages={2100--2106},
  year={2013},
  organization={IEEE}
}


@inproceedings{kerl2015iccv,
  title={Dense continuous-time tracking and mapping with rolling shutter RGB-D cameras},
  author={Kerl, C. and Stuckler, J. and Cremers, D.},
  booktitle= iccv,
  pages={2264--2272},
  abstract = {We propose a dense continuous-time tracking and map-
                  ping method for RGB-D cameras. We parametrize the
                  camera trajectory using continuous B-splines and
                  optimize the trajectory through dense, direct image
                  alignment. Our method also directly models rolling
                  shutter in both RGB and depth images within the
                  optimization, which improves track- ing and
                  reconstruction quality for low-cost CMOS sensors},
  year={2015}
}

@INPROCEEDINGS{sturm2012iros,
  author = {J. Sturm and N. Engelhard and F. Endres and W. Burgard and D. Cremers},
  title = "A Benchmark for the Evaluation of RGB-D SLAM Systems",
  booktitle = iros,
  year = "2012",
}

@incollection{rovina2016aich,
  author = {Serafin, J. and Di Cicco, M. and Bonanni, T. M. and Grisetti, G. and Iocchi, L. and Nardi, D. and Stachniss, C. and Ziparo, V. A.},
  booktitle = {Artificial Intelligence for Cultural Heritage},
  chapter = {5},
  pages = {121-140},
  title = {Robots for Exploration, Digital Preservation and Visualization of Archeological Sites},
  year = {2016},
}

@INPROCEEDINGS{geiger2012cvpr,
  author = {A. Geiger and P. Lenz and R. Urtasun},
  title = {Are we ready for Autonomous Driving? The KITTI Vision Benchmark Suite},
  booktitle = cvpr,
  year = {2012}
}


@InProceedings{rosten2006eccv,
  author={E. Rosten and T. Drummond},
  title={Machine Learning for High-Speed Corner Detection},
  booktitle=eccv,
  year={2006},
  pages={430-443},
abstract={Where feature points are used in real-time frame-rate applications, a high-speed feature detector is necessary. Feature detectors such as SIFT (DoG), Harris and SUSAN are good methods which yield high quality features, however they are too computationally intensive for use in real-time applications of any complexity. Here we show that machine learning can be used to derive a feature detector which can fully process live PAL video using less than 7{\%} of the available processing time. By comparison neither the Harris detector (120{\%}) nor the detection stage of SIFT (300{\%}) can operate at full frame rate.}
}

@ARTICLE{grompone2010pami,
author={R. Grompone von Gioi and J. Jakubowicz and J. M. Morel and G. Randall},
journal=pami,
title={LSD: A Fast Line Segment Detector with a False Detection Control},
year={2010},
volume={32},
number={4},
pages={722-732},
keywords={image sensors;object detection;false detection control;linear-time line segment detector;natural images;state-of-the-art algorithms;Helmholtz principle;Line segment detection;NFA;a contrario detection.;Algorithms;Humans;Image Processing, Computer-Assisted;Linear Models;Pattern Recognition, Automated;Reproducibility of Results;Visual Perception},
doi={10.1109/TPAMI.2008.300},
ISSN={0162-8828},
month={April},}

@InProceedings{calonder2010eccv,
author={M. Calonder and V. Lepetit and C. Strecha and P. Fua},
title={BRIEF: Binary Robust Independent Elementary Features},
booktitle=eccv,
year={2010},
pages={778-792},
abstract={We propose to use binary strings as an efficient feature point descriptor, which we call BRIEF.We show that it is highly discriminative even when using relatively few bits and can be computed using simple intensity difference tests. Furthermore, the descriptor similarity can be evaluated using the Hamming distance, which is very efficient to compute, instead of the L                2 norm as is usually done.}
}

@article{zang2013vcir,
 author = {L. Zhang and R. Koch},
 title = {An Efficient and Robust Line Segment Matching Approach Based on LBD Descriptor and Pairwise Geometric Consistency},
 journal = {Visual Communication and Image Representation},
 year = {2013},
 pages = {794-805}
}

@article{hosseinzadeh2018,
  title={Sparse Point-Plane SLAM},
  author={M. Hosseinzadeha and Y. Latif and I. Reid}
}

@INPROCEEDINGS{taguchi2013icra,
author={Y. Taguchi and Y. D. Jian and S. Ramalingam and C. Feng},
booktitle=icra,
title={Point-plane SLAM for hand-held 3D sensors},
year={2013},
volume={},
number={},
pages={5182-5189},
month={May},}


@INPROCEEDINGS{lemaire2007icra,
author={T. Lemaire and S. Lacroix},
booktitle=icra,
title={Monocular-vision based SLAM using Line Segments},
year={2007},
volume={},
number={},
pages={2791-2796},
keywords={Gaussian processes;Kalman filters;SLAM (robots);edge detection;feature extraction;image representation;robot vision;3D line segments;Gaussian sum;Plucker coordinates;Plucker representation;SLAM;camera;extended Kalman filter;feature initial state approximation;landmark initialization;landmarks state estimation;monocular vision;Cameras;Delay estimation;Image segmentation;Robot kinematics;Robot sensing systems;Robot vision systems;Robotics and automation;Sampling methods;Simultaneous localization and mapping;State estimation},
doi={10.1109/ROBOT.2007.363894},
ISSN={1050-4729},
month={April},}

@InProceedings{gee2006avc,
author={A.P. Gee and W. Mayol-Cuevas},
title={Real-Time Model-Based SLAM Using Line Segments},
booktitle={Advances in Visual Computing},
year={2006},
pages={354-363},
abstract={Existing monocular vision-based SLAM systems favour interest point features as landmarks, but these are easily occluded and can only be reliably matched over a narrow range of viewpoints. Line segments offer an interesting alternative, as line matching is more stable with respect to viewpoint changes and lines are robust to partial occlusion. In this paper we present a model-based SLAM system that uses 3D line segments as landmarks. Unscented Kalman filters are used to initialise new line segments and generate a 3D wireframe model of the scene that can be tracked with a robust model-based tracking algorithm. Uncertainties in the camera position are fed into the initialisation of new model edges. Results show the system operating in real-time with resilience to partial occlusion. The maps of line segments generated during the SLAM process are physically meaningful and their structure is measured against the true 3D structure of the scene.},
}


@article{magnusson2007jfr,
  title={Scan registration for autonomous mining vehicles using 3D-NDT},
  author={Magnusson, Martin and Lilienthal, Achim and Duckett, Tom},
  journal=jfr,
  volume={24},
  number={10},
  pages={803--827},
  year={2007},
  publisher={Wiley Online Library}
}

@inproceedings{trevor2012icra,
  title={Planar surface SLAM with 3D and 2D sensors},
  author={A. JB Trevor and J. G Rogers and H. I Christensen},
  booktitle=icra,
  pages={3041--3048},
  year={2012},
  organization={IEEE}
}

@inproceedings{whelan2015rss,
title={ElasticFusion: Dense SLAM without a pose graph},
author={Whelan, Thomas and Leutenegger, Stefan and Salas-Moreno, R and Glocker, Ben and Davison, Andrew},
booktitle=rss,
year={2015}
}

@article{castellanos1999tra,
  title={The SPmap: A probabilistic framework for simultaneous localization and map building},
  author={Castellanos, Jose A and Montiel, JMM and Neira, Jos{\'e} and Tard{\'o}s, Juan D},
  journal=tra,
  year={1999},
  publisher={IEEE}
}

@article{delapuente2015ras,
  title={Feature based graph SLAM with high level representation using rectangles},
  author={de la Puente, Paloma and Rodriguez-Losada, Diego},
  journal=ras,
  volume={63},
  pages={80--88},
  year={2015},
  publisher={Elsevier}
}

@article{besl1992pami,
  title={A method for registration of 3-D shapes},
  author={Besl, PJ and McKay, Neil D},
  journal=pami,
  volume={14},
  number={2},
  pages={239--256},
  year={1992},
  publisher={IEEE}
}

@inproceedings{biber2003iros,
  title={The normal distributions transform: A new approach to laser scan matching},
  author={Biber, Peter and Stra{\ss}er, Wolfgang},
  booktitle=iros,
  volume={3},
  pages={2743--2748},
  year={2003},
  organization={IEEE}
}

@inproceedings{chen1991icra,
  title={Object modeling by registration of multiple range images},
  author={Chen, Y and Medioni, G},
  booktitle=icra,
  pages={2724--2729},
  year={1991},
  organization={IEEE}
}

@inproceedings{feng2014icra,
  title={Fast plane extraction in organized point clouds using agglomerative hierarchical clustering},
  author={C. Feng and Y. Taguchi and V. R Kamat},
  booktitle=icra,
  pages={6218--6225},
  year={2014},
  organization={IEEE}
}

@article{trevor2013spme,
  title={Efficient organized point cloud segmentation with connected components},
  author={A. JB Trevor and S. Gedikli and R. B Rusu and H. I Christensen},
  journal={Semantic Perception Mapping and Exploration},
  year={2013}
}

@inproceedings{choi2013iros,
  title={RGB-D edge detection and edge-based registration},
  author={C. Choi and A. JB Trevor and H. I Christensen},
  booktitle=iros,
  pages={1568--1575},
  year={2013},
  organization={IEEE}
}

@inproceedings{lu2015iccv,
  title={Robust rgb-d odometry using point and line features},
  author={Y. Lu and D. Song},
  booktitle=iccv,
  pages={3934--3942},
  year={2015}
}

@InProceedings{handa2014icra,
author = {A. Handa and T. Whelan and J.B. McDonald and A.J. Davison},
title = {A Benchmark for {RGB-D} Visual Odometry, {3D} Reconstruction and {SLAM}},
booktitle = icra,
month = {May},
year = {2014}
}

@inproceedings{wasenmueller2016wcacv,
title={{CoRBS}: Comprehensive RGB-D Benchmark for SLAM using Kinect v2},
author={O. Wasenm\"{u}ller and M. Meyer and D. Stricker},
booktitle={IEEE Winter Conference on Applications of Computer Vision},
month={March},
year={2016},
}

@article{smith2006bmvc,
  title={Real-time monocular SLAM with straight lines},
  author={P. Smith and I. D Reid and A. J Davison},
  year={2006},
  journal=bmvc
}

@inproceedings{klein2008eccv,
  title={Improving the agility of keyframe-based SLAM},
  author={G. Klein and D. Murray},
  booktitle=eccv,
  pages={802--815},
  year={2008},
  organization={Springer}
}

@article{eade2009ivc,
  title={Edge landmarks in monocular SLAM},
  author={E. Eade and T. Drummond},
  journal=ivc,
  volume={27},
  number={5},
  pages={588--596},
  year={2009},
  publisher={Elsevier}
}

@article{henry2012ijrr,
  title={RGB-D mapping: Using Kinect-style depth cameras for dense 3D modeling of indoor environments},
  author={P. Henry and M. Krainin and E. Herbst and X. Ren and D. Fox},
  journal=ijrr,
  volume={31},
  number={5},
  pages={647--663},
  year={2012},
  publisher={SAGE Publications Sage UK: London, England}
}

@inproceedings{olsson2006cvpr,
  title={The registration problem revisited: Optimal solutions from points, lines and planes},
  author={C. Olsson and F. Kahl and M. Oskarsson},
  booktitle=cvpr,
  volume={1},
  pages={1206--1213},
  year={2006},
  organization={IEEE}
}


@article{bentley1975acm,
  author    = {J.L. Bentley},
  title     = {Multidimensional Binary Search Trees Used for Associative Searching},
  journal   = {Commun. {ACM}},
  volume    = {18},
  number    = {9},
  pages     = {509--517},
  year      = {1975},
  url       = {http://doi.acm.org/10.1145/361002.361007},
  doi       = {10.1145/361002.361007},
  timestamp = {Tue, 07 Jun 2011 16:51:01 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/cacm/Bentley75},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{wulf2004icra,
	title={2D mapping of cluttered indoor environments by means of 3D perception},
	author={Wulf, Oliver and Arras, Kai Oliver and Christensen, Henrik I and Wagner, Bernardo},
	booktitle=icra,
	volume={4},
	pages={4204--4209},
	year={2004},
	organization={IEEE; 1999}
}
